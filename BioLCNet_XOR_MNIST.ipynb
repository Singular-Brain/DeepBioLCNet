{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/DeepBioLCNet/blob/main/BioLCNet_XOR_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTSvrK3T_GA"
      },
      "source": [
        "#Notebook setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lXtgP_iEPE0G"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/Singular-Brain/DeepBioLCNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OW7m3ugEHZP_"
      },
      "outputs": [],
      "source": [
        "# !wget https://data.deepai.org/mnist.zip\n",
        "# !mkdir -p ../data/MNIST/TorchvisionDatasetWrapper/raw\n",
        "# !unzip mnist.zip -d ../data/MNIST/TorchvisionDatasetWrapper/raw/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXcXvvsXcOlv",
        "outputId": "87d7599f-1605-4e22-8e20-780f8f6fc5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepBioLCNet' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Singular-Brain/DeepBioLCNet\n",
        "# import bindsent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K4l3AVRbGS4Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.image import AxesImage\n",
        "from torch.nn.modules.utils import _pair\n",
        "from matplotlib.collections import PathCollection\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from typing import Tuple, List, Optional, Sized, Dict, Union\n",
        "import math\n",
        "import random\n",
        "# from ..utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "BFGNAecpT-Lj"
      },
      "outputs": [],
      "source": [
        "from bindsnet.network.nodes import Nodes\n",
        "import os\n",
        "### import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, Tuple, Optional, Sequence\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "from bindsnet.datasets import MNIST\n",
        "from bindsnet.encoding import PoissonEncoder, Encoder, NullEncoder\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes, IFNodes\n",
        "from bindsnet.network.topology import Connection, MaxPool2dLocalConnection\n",
        "from bindsnet.network.topology import LocalConnection, LocalConnectionOrig\n",
        "from bindsnet.network.monitors import Monitor, AbstractMonitor, TensorBoardMonitor\n",
        "from bindsnet.learning import PostPre, MSTDP, MSTDPET, WeightDependentPostPre, Hebbian\n",
        "from bindsnet.learning.reward import DynamicDopamineInjection, DopaminergicRPE\n",
        "from bindsnet.analysis.plotting import plot_locally_connected_weights,plot_locally_connected_weights_meh,plot_spikes,\\\n",
        "plot_LC_timepoint_spikes,plot_locally_connected_weights_meh2,plot_convergence_and_histogram,plot_locally_connected_weights_meh3\n",
        "from bindsnet.analysis.visualization import plot_weights_movie, plot_spike_trains_for_example,summary, plot_voltage\n",
        "from bindsnet.utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGGHW43UksI"
      },
      "source": [
        "## Sets up Gpu use and manual seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiUmFrpcUfmR",
        "outputId": "e8979010-a0e8-4c21-c5a8-d78ffeceb9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Device =  cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device =  torch.device(\"cuda\")\n",
        "    gpu = True\n",
        "else:\n",
        "    device =  torch.device(\"cpu\")\n",
        "    gpu = False\n",
        "\n",
        "def manual_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SEED = 2045 # The Singularity is Near!\n",
        "manual_seed(SEED)\n",
        "WANDB = False\n",
        "\n",
        "torch.set_num_threads(os.cpu_count() - 1)\n",
        "print(\"Running on Device = \", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKedMpIleMr"
      },
      "source": [
        "# Custom Monitors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqpsr2a1WV"
      },
      "source": [
        "## Reward Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "M44GJ65GleMs"
      },
      "outputs": [],
      "source": [
        "class RewardMonitor(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: None,\n",
        "        batch_size: int = 1,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.time = time\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # if time is not specified the monitor variable accumulate the logs\n",
        "        if self.time is None:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        self.recording = []\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if \"reward\" in kwargs:\n",
        "            self.recording.append(kwargs[\"reward\"])\n",
        "        # remove the oldest element (first in the list)\n",
        "        # if self.time is not None:\n",
        "        #     self.recording.pop(0)\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clxN_npa1WY"
      },
      "source": [
        "## Plot Eligibility trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SshGlRwpa1WZ"
      },
      "outputs": [],
      "source": [
        "class PlotET(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        i,\n",
        "        j,\n",
        "        source,\n",
        "        target,\n",
        "        connection,\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.connection = connection\n",
        "\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if hasattr(self.connection.update_rule, 'p_plus'):\n",
        "            self.recording['spikes_i'].append(self.source.s.ravel()[self.i].item())\n",
        "            self.recording['spikes_j'].append(self.target.s.ravel()[self.j].item())\n",
        "            self.recording['p_plus'].append(self.connection.update_rule.p_plus[self.i].item())\n",
        "            self.recording['p_minus'].append(self.connection.update_rule.p_minus[self.j].item())\n",
        "            self.recording['eligibility'].append(self.connection.update_rule.eligibility[self.i,self.j].item())\n",
        "            self.recording['eligibility_trace'].append(self.connection.update_rule.eligibility_trace[self.i,self.j].item())\n",
        "            self.recording['w'].append(self.connection.w[self.i,self.j].item())\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        fig, axs  = plt.subplots(7)\n",
        "        fig.set_size_inches(10, 20)\n",
        "        for i, (name, p) in enumerate(self.recording.items()):\n",
        "            axs[i].plot(p[-250:])\n",
        "            axs[i].set_title(name)\n",
        "    \n",
        "        fig.show()\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = {\n",
        "        'spikes_i': [],\n",
        "        'spikes_j': [],\n",
        "        'p_plus':[],\n",
        "        'p_minus':[],\n",
        "        'eligibility':[],\n",
        "        'eligibility_trace':[],\n",
        "        'w': [],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YGE1XjvIkZ"
      },
      "source": [
        "## Kernel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-4hp2V46vOUv"
      },
      "outputs": [],
      "source": [
        "class AbstractKernel(ABC):\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Base class for generating image filter kernels such as Gabor, DoG, etc. Each subclass should override :attr:`__call__` function.\n",
        "        Instantiates a ``Filter Kernel`` object.\n",
        "        :param window_size : The size of the kernel (int)\n",
        "        \"\"\"\n",
        "        self.window_size = kernel_size\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PL2L6_ABwBH4"
      },
      "outputs": [],
      "source": [
        "class DoGKernel(AbstractKernel):\n",
        "    def __init__(self, kernel_size: Union[int, Tuple[int, int]], sigma1 : float, sigma2 : float):\n",
        "        \"\"\"\n",
        "        Generates DoG filter kernels.\n",
        "        :param kernel_size: Horizontal and vertical size of DOG kernels.(If pass int, we consider it as a square filter) \n",
        "        :param sigma1 : The sigma parameter for the first Gaussian function.\n",
        "        :param sigma2 : The sigma parameter for the second Gaussian function.\n",
        "        \"\"\"\n",
        "        super(DoGKernel, self).__init__(kernel_size)\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        \n",
        "    def __call__(self):\n",
        "        k = self.window_size//2\n",
        "        x, y = np.mgrid[-k:k+1:1, -k:k+1:1]\n",
        "        a = 1.0 / (2 * math.pi)\n",
        "        prod = x*x + y*y\n",
        "        f1 = (1/(self.sigma1*self.sigma1)) * np.exp(-0.5 * (1/(self.sigma1*self.sigma1)) * (prod))\n",
        "        f2 = (1/(self.sigma2*self.sigma2)) * np.exp(-0.5 * (1/(self.sigma2*self.sigma2)) * (prod))\n",
        "        dog = a * (f1-f2)\n",
        "        dog_mean = np.mean(dog)\n",
        "        dog = dog - dog_mean\n",
        "        dog_max = np.max(dog)\n",
        "        dog = dog / dog_max\n",
        "        dog_tensor = torch.from_numpy(dog)\n",
        "        # returns a 2d tensor corresponding to the requested DoG filter\n",
        "        return dog_tensor.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zBUT0IUZDXxW"
      },
      "outputs": [],
      "source": [
        "class Filter():\n",
        "    \"\"\"\n",
        "    Applies a filter transform. Each filter contains a sequence of :attr:`FilterKernel` objects.\n",
        "    The result of each filter kernel will be passed through a given threshold (if not :attr:`None`).\n",
        "    Args:\n",
        "        filter_kernels (sequence of FilterKernels): The sequence of filter kernels.\n",
        "        padding (int, optional): The size of the padding for the convolution of filter kernels. Default: 0\n",
        "        thresholds (sequence of floats, optional): The threshold for each filter kernel. Default: None\n",
        "        use_abs (boolean, optional): To compute the absolute value of the outputs or not. Default: False\n",
        "    .. note::\n",
        "        The size of the compund filter kernel tensor (stack of individual filter kernels) will be equal to the \n",
        "        greatest window size among kernels. All other smaller kernels will be zero-padded with an appropriate \n",
        "        amount.\n",
        "    \"\"\"\n",
        "    # filter_kernels must be a list of filter kernels\n",
        "    # thresholds must be a list of thresholds for each kernel\n",
        "    def __init__(self, filter_kernels, padding=0, thresholds=None, use_abs=False):\n",
        "        tensor_list = []\n",
        "        self.max_window_size = 0\n",
        "        for kernel in filter_kernels:\n",
        "            if isinstance(kernel, torch.Tensor):\n",
        "                tensor_list.append(kernel)\n",
        "                self.max_window_size = max(self.max_window_size, kernel.size(-1))\n",
        "            else:\n",
        "                tensor_list.append(kernel().unsqueeze(0))\n",
        "                self.max_window_size = max(self.max_window_size, kernel.window_size)\n",
        "        for i in range(len(tensor_list)):\n",
        "            p = (self.max_window_size - filter_kernels[i].window_size)//2\n",
        "            tensor_list[i] = fn.pad(tensor_list[i], (p,p,p,p))\n",
        "\n",
        "        self.kernels = torch.stack(tensor_list)\n",
        "        self.number_of_kernels = len(filter_kernels)\n",
        "        self.padding = padding\n",
        "        if isinstance(thresholds, list):\n",
        "            self.thresholds = thresholds.clone().detach()\n",
        "            self.thresholds.unsqueeze_(0).unsqueeze_(2).unsqueeze_(3)\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "        self.use_abs = use_abs\n",
        "\n",
        "    # returns a 4d tensor containing the flitered versions of the input image\n",
        "    # input is a 4d tensor. dim: (minibatch=1, filter_kernels, height, width)\n",
        "    def __call__(self, input):\n",
        "\n",
        "        # if input.dim() == 3:\n",
        "        #     input2 = torch.unsqueeze(input, 0)\n",
        "        input.unsqueeze_(0)\n",
        "        output = fn.conv2d(input, self.kernels, padding = self.padding).float()\n",
        "        if not(self.thresholds is None):\n",
        "            output = torch.where(output < self.thresholds, torch.tensor(0.0, device=output.device), output)\n",
        "        if self.use_abs:\n",
        "            torch.abs_(output)\n",
        "        return output.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywXyWP0I83Au"
      },
      "source": [
        "# Design network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcU9FSsVi4Bw",
        "outputId": "f049bf0a-efd6-4a1e-b96e-2fb4ed45834d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ],
      "source": [
        "WANDB = True\n",
        "if WANDB:\n",
        "    !pip install -q wandb\n",
        "    !wandb login\n",
        "    import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8bZpJmlrJDa9"
      },
      "outputs": [],
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "def convergence(c):\n",
        "    if c.norm is None:\n",
        "        return 1-torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2\n",
        "    else:\n",
        "        mean_norm_factor = c.norm / c.w.shape[-1]\n",
        "        return  1-(torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2)\n",
        "\n",
        "        \n",
        "class LCNet(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes: int,\n",
        "        neuron_per_class: int,\n",
        "        in_channels : int,\n",
        "        n_channels1: int,\n",
        "        n_channels2: int,\n",
        "        filter_size1: int,\n",
        "        filter_size2: int,\n",
        "        stride1: int,\n",
        "        stride2: int,\n",
        "        maxPool1: bool,\n",
        "        maxPool2: bool,\n",
        "        online: bool,\n",
        "        deep: bool,\n",
        "        time: int,\n",
        "        reward_fn,\n",
        "        n_neurons: int,\n",
        "        pre_observation: bool,\n",
        "        has_decision_period: bool,\n",
        "        local_rewarding: bool,\n",
        "        nu_LC: Union[float, Tuple[float, float]],\n",
        "        nu_LC2: Union[float, Tuple[float, float]],\n",
        "        nu_Output: float,\n",
        "        dt: float,\n",
        "        crop_size:int ,\n",
        "        nu_inh_LC: float,\n",
        "        nu_inh: float,\n",
        "        inh_type,\n",
        "        inh_LC: bool,\n",
        "        inh_LC2: bool,\n",
        "        inh_factor_LC: float,\n",
        "        inh_factor_LC2: float,\n",
        "        inh_factor:float,\n",
        "        single_output_layer:bool,\n",
        "        NodesType_LC,\n",
        "        NodesType_Output, \n",
        "        update_rule_LC,\n",
        "        update_rule_LC2,\n",
        "        update_rule_Output,\n",
        "        update_rule_inh,\n",
        "        update_rule_inh_LC,\n",
        "        wmin: float,\n",
        "        wmax: float ,\n",
        "        soft_bound,\n",
        "        theta_plus: float,\n",
        "        tc_theta_decay: float,\n",
        "        tc_trace:int,\n",
        "        normal_init:bool,\n",
        "        mu: float,\n",
        "        std:float,\n",
        "        norm_factor_inh_LC: bool,\n",
        "        norm_factor_LC,\n",
        "        norm_factor_LC2,\n",
        "        norm_factor_out,\n",
        "        norm_factor_inh,\n",
        "        trace_additive,\n",
        "        load_path,\n",
        "        save_path,\n",
        "        LC_weights_path,\n",
        "        LC2_weights_path,\n",
        "        confusion_matrix,\n",
        "        lc_weights_vis,\n",
        "        out_weights_vis,\n",
        "        lc_convergence_vis,\n",
        "        out_convergence_vis,\n",
        "        thresh_LC,\n",
        "        thresh_FC,\n",
        "        wandb_active = False,\n",
        "        batch_size=1,\n",
        "        epocsh=1,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructor for class ``BioLCNet``.\n",
        "\n",
        "        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n",
        "        :param n_neurons: Number of excitatory, inhibitory neurons.\n",
        "        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n",
        "        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n",
        "        :param dt: Simulation time step.\n",
        "        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n",
        "            respectively.\n",
        "        :param reduction: Method for reducing parameter updates along the minibatch\n",
        "            dimension.\n",
        "        :param wmin: Minimum allowed weight on input to excitatory synapses.\n",
        "        :param wmax: Maximum allowed weight on input to excitatory synapses.\n",
        "        :param norm: Input to excitatory layer connection weights normalization\n",
        "            constant.\n",
        "        :param theta_plus: On-spike increment of ``(adaptive)LIFNodes`` membrane\n",
        "            threshold potential.\n",
        "        :param tc_theta_decay: Time constant of ``(adaptive)LIFNodes`` threshold\n",
        "            potential decay.\n",
        "        :param inpt_shape: The dimensionality of the input layer.\n",
        "        \"\"\"\n",
        "        manual_seed(SEED)\n",
        "        super().__init__(dt=dt, reward_fn = None, online=online)\n",
        "        kwargs['single_output_layer'] = single_output_layer\n",
        "        kwargs['dt'] = dt\n",
        "        kwargs['n_labels'] = n_classes\n",
        "        kwargs['neuron_per_class'] = neuron_per_class\n",
        "        \n",
        "        self.dt = dt\n",
        "        self.reward_fn = reward_fn(**kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_fn.network = self\n",
        "        self.reward_fn.dt = self.dt\n",
        "        self.n_classes = n_classes\n",
        "        self.neuron_per_class = neuron_per_class\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "        self.deep = deep\n",
        "        self.maxPool1 = maxPool1\n",
        "        self.maxPool2 = maxPool2\n",
        "        self.time = time\n",
        "        self.crop_size = crop_size\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.clamp_intensity = kwargs.get('clamp_intensity',None)\n",
        "        self.single_output_layer = single_output_layer\n",
        "        self.pre_observation = pre_observation\n",
        "        self.has_decision_period = has_decision_period\n",
        "        self.local_rewarding = local_rewarding\n",
        "        self.soft_bound = soft_bound\n",
        "        self.confusion_matrix = confusion_matrix\n",
        "        self.lc_weights_vis = lc_weights_vis\n",
        "        self.out_weights_vis = out_weights_vis\n",
        "        self.lc_convergence_vis = lc_convergence_vis\n",
        "        self.out_convergence_vis = out_convergence_vis\n",
        "        self.in_channels = in_channels\n",
        "        self.n_channels1 = n_channels1\n",
        "        self.n_channels2 = n_channels2\n",
        "        self.convergences = {}\n",
        "        self.norm_factor_LC = norm_factor_LC\n",
        "        self.norm_factor_LC2 = norm_factor_LC2\n",
        "        self.norm_factor_out = norm_factor_out\n",
        "        self.wmin = wmin \n",
        "        self.wmax = wmax\n",
        "        self.wandb_active = wandb_active\n",
        "        self.epochs_trained = 0\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.time_analysis = kwargs.get('time_analysis', False)\n",
        "        print(self.time_analysis)\n",
        "        if kwargs['variant'] == 'scalar':\n",
        "            assert self.has_decision_period == True, ''\n",
        "\n",
        "        if self.online == False:\n",
        "            assert self.has_decision_period == True, ''\n",
        "        \n",
        "        if self.has_decision_period == True:\n",
        "            assert self.online == False, \"Decision period is not compatible with online learning.\"\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.decision_period = kwargs['decision_period']\n",
        "            assert self.decision_period > 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period - self.decision_period\n",
        "\n",
        "        elif self.pre_observation == True:\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period\n",
        "            self.decision_period = self.time - self.observation_period\n",
        "\n",
        "        else:\n",
        "            self.observation_period = 0\n",
        "            self.decision_period = self.time\n",
        "            self.learning_period = self.time\n",
        "\n",
        "        ### nodes\n",
        "        inp = Input(shape= [in_channels,crop_size,crop_size], traces=True, tc_trace=tc_trace,traces_additive = trace_additive)\n",
        "        self.add_layer(inp, name=\"input\")\n",
        "\n",
        "        ## First hidden layer\n",
        "        conv_size1 = compute_size(crop_size, filter_size1, stride1)\n",
        "        main1 = NodesType_LC(shape= [n_channels1, conv_size1, conv_size1], thresh = thresh_LC, traces=True, tc_trace=tc_trace,\n",
        "                             traces_additive = trace_additive,tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "        \n",
        "        self.add_layer(main1, name=\"main1\")\n",
        "\n",
        "        ### connections \n",
        "        LC1 = LocalConnectionOrig(inp, main1, filter_size1, stride1, n_channels1,\\\n",
        "                              nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, norm = norm_factor_LC)\n",
        "\n",
        "        # LC1 = LocalConnection(inp, main1, filter_size1, stride1, in_channels, n_channels1,input_shape=(crop_size,crop_size),\\\n",
        "        #                      nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC)\n",
        "\n",
        "\n",
        "        if LC_weights_path:\n",
        "            a = torch.load(LC_weights_path)\n",
        "            LC1.w.data = a['state_dict']['input_to_main1.w']\n",
        "            print(\"Weights loaded ...\")\n",
        "        \n",
        "        elif normal_init:\n",
        "            w_lc_init = torch.normal(mu,std, size = (in_channels, n_channels1 * compute_size(crop_size, filter_size1, stride1)**2, filter_size1**2))\n",
        "            LC1.w.data = w_lc_init\n",
        "       \n",
        "        self.add_connection(LC1, \"input\", \"main1\")\n",
        "        self.convergences['lc1'] = []\n",
        "\n",
        "        if inh_LC:\n",
        "            main_width = compute_size(crop_size, filter_size1, stride1)\n",
        "            w_inh_LC = torch.zeros(n_channels1,main_width,main_width,n_channels1,main_width,main_width)\n",
        "            for c in range(n_channels1):\n",
        "                for w1 in range(main_width):\n",
        "                    for w2 in range(main_width):\n",
        "                        w_inh_LC[c,w1,w2,:,w1,w2] = - inh_factor_LC\n",
        "                        w_inh_LC[c,w1,w2,c,w1,w2] = 0\n",
        "        \n",
        "            w_inh_LC = w_inh_LC.reshape(main1.n,main1.n)\n",
        "                                                             \n",
        "            LC_recurrent_inhibition = Connection(\n",
        "                source=main1,\n",
        "                target=main1,\n",
        "                w=w_inh_LC,\n",
        "            )\n",
        "            self.add_connection(LC_recurrent_inhibition, \"main1\", \"main1\")\n",
        "        \n",
        "        \n",
        "        self.final_connection_source_name = 'main1'\n",
        "        self.final_connection_source = main1\n",
        "\n",
        "        self.hidden2 = main1\n",
        "        self.hidden2_name = 'main1'\n",
        "        if maxPool1:\n",
        "            maxPool_kernel = 2\n",
        "            maxPool_stride = 2\n",
        "            \n",
        "            conv_size1 =compute_size(conv_size1, maxPool_kernel, maxPool_stride)\n",
        "            self.final_connection_source_name = 'maxpool1'\n",
        "            \n",
        "            maxpool1 = LIFNodes(shape= [self.n_channels1, conv_size1, conv_size1], refrac = 0)\n",
        "            self.add_layer(maxpool1, name=\"maxpool1\")\n",
        "            self.final_connection_source = maxpool1\n",
        "            \n",
        "            maxPoolConnection = MaxPool2dLocalConnection(main1, maxpool1, maxPool_kernel, maxPool_stride)\n",
        "            self.add_connection(maxPoolConnection, \"main1\", 'maxpool1')\n",
        "            \n",
        "            self.hidden2 = maxpool1\n",
        "            self.hidden2_name = 'maxpool1'\n",
        "\n",
        "        if deep:\n",
        "            # # Second hidden layer\n",
        "            conv_size2 = compute_size(conv_size1, filter_size2, stride2)\n",
        "\n",
        "            main2 = NodesType_LC(shape= [n_channels2, conv_size2, conv_size2],traces=True, tc_trace=tc_trace,traces_additive = trace_additive,\n",
        "                                            tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "            \n",
        "            self.add_layer(main2, name=\"main2\")\n",
        "\n",
        "            ### connections \n",
        "            lc2_input_shape = (conv_size1,conv_size1)\n",
        "            LC2 = LocalConnection(self.hidden2, main2, filter_size2, stride2, n_channels1, n_channels2, input_shape= lc2_input_shape,\n",
        "            nu = _pair(nu_LC2), update_rule = update_rule_LC2, wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC2)\n",
        "\n",
        "            self.add_connection(LC2,  self.hidden2_name, \"main2\")\n",
        "            self.convergences['lc2'] = []\n",
        "            if LC2_weights_path:\n",
        "                a = torch.load(LC2_weights_path)\n",
        "                LC2.w.data = a['state_dict']['main1_to_main2.w']\n",
        "                print(\"Weights loaded ...\")\n",
        "            \n",
        "            elif normal_init:\n",
        "                w_lc_init = torch.normal(mu,std, size = (n_channels1, n_channels2 * compute_size(conv_size1, filter_size2, stride2)**2, filter_size2**2))\n",
        "                LC2.w.data = w_lc_init\n",
        "\n",
        "            self.final_connection_source_name = 'main2'\n",
        "            self.final_connection_source = main2\n",
        "\n",
        "            if inh_LC2:\n",
        "                main_width = conv_size2\n",
        "                w_inh_LC2 = torch.zeros(n_channels2,main_width,main_width,n_channels2,main_width,main_width)\n",
        "                for c in range(n_channels2):\n",
        "                    for w1 in range(main_width):\n",
        "                        for w2 in range(main_width):\n",
        "                            w_inh_LC2[c,w1,w2,:,w1,w2] = - inh_factor_LC2\n",
        "                            w_inh_LC2[c,w1,w2,c,w1,w2] = 0\n",
        "            \n",
        "                w_inh_LC2 = w_inh_LC2.reshape(main2.n,main2.n)\n",
        "                                                                \n",
        "                LC_recurrent_inhibition2 = Connection(\n",
        "                    source=main2,\n",
        "                    target=main2,\n",
        "                    w=w_inh_LC2,\n",
        "                )\n",
        "                self.add_connection(LC_recurrent_inhibition2, \"main2\", \"main2\")\n",
        "\n",
        "\n",
        "            if maxPool2:\n",
        "                maxPool_kernel = 2\n",
        "                maxPool_stride = 2\n",
        "                conv_size2 =compute_size(conv_size2, maxPool_kernel, maxPool_stride)\n",
        "                self.final_connection_source_name = 'maxpool2'\n",
        "                maxpool2 = LIFNodes(shape= [self.n_channels2, conv_size2, conv_size2], refrac = 0)\n",
        "                self.final_connection_source = maxpool2\n",
        "                maxPoolConnection2 = MaxPool2dLocalConnection(main2, maxpool2, maxPool_kernel, maxPool_stride)\n",
        "\n",
        "                self.add_layer(maxpool2, name=\"maxpool2\")\n",
        "                self.add_connection(maxPoolConnection2, \"main2\", 'maxpool2')\n",
        "\n",
        "\n",
        "        ### main2 to output\n",
        "        out = NodesType_Output(n= n_neurons, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "        self.add_layer(out, \"output\")\n",
        "\n",
        "        last_main_out = Connection(self.final_connection_source, out, nu = nu_Output, update_rule = update_rule_Output, wmin = wmin, wmax= wmax, norm = norm_factor_out)\n",
        "\n",
        "        if normal_init:\n",
        "            w_last_main_init = torch.normal(mu,std,size = (self.final_connection_source.n,out.n)) \n",
        "            last_main_out.w.data = w_last_main_init\n",
        "        print(torch.mean(last_main_out.w),torch.std(last_main_out.w))    \n",
        "        self.add_connection(last_main_out, self.final_connection_source_name, \"output\")\n",
        "        self.convergences['last_main_out'] = []\n",
        "        ### Inhibitory:\n",
        "        if inh_type == 'between_layers':\n",
        "            w = -inh_factor * torch.ones(out.n, out.n)\n",
        "            for c in range(n_classes):\n",
        "                ind = slice(c*neuron_per_class,(c+1)*neuron_per_class)\n",
        "                w[ind, ind] = 0\n",
        "\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        elif inh_type == 'one_2_all':\n",
        "            w = -inh_factor * (torch.ones(out.n, out.n) - torch.eye(out.n, out.n))\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        # Diehl and Cook\n",
        "        elif inh_type == 'DC':\n",
        "            raise NotImplementedError('Diehl and cook not implemented yet fo r 10 classes')\n",
        "\n",
        "        # Directs network to GPU\n",
        "        if gpu:\n",
        "            self.to(\"cuda\")\n",
        "\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        dataloader,\n",
        "        val_loader,\n",
        "        reward_hparams,\n",
        "        label = None,\n",
        "        hparams = None,\n",
        "        online_validate = True,\n",
        "        n_train = 2000,\n",
        "        n_test = 250,\n",
        "        n_val = 250,\n",
        "        val_interval = 250,\n",
        "        running_window_length = 250,\n",
        "        verbose = True,\n",
        "    ):\n",
        "        manual_seed(SEED)\n",
        "        if self.wandb_active:\n",
        "            wandb.watch(self)\n",
        "        self.verbose = verbose\n",
        "        self.label = label\n",
        "        # add Monitors\n",
        "        #main_monitor = Monitor(self.layers[\"main\"], [\"v\"], time=None, device=device)\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "        #Plot_et = PlotET(i = 0, j = 0, source = self.layers[\"main\"], target = self.layers[\"output\"], connection = self.connections[(\"main\",\"output\")])\n",
        "        #tensorboard = TensorBoardMonitor(self, time = self.time)\n",
        "        #self.add_monitor(main_monitor, name=\"main\")\n",
        "        self.add_monitor(reward_monitor, name=\"reward\")\n",
        "        #self.add_monitor(Plot_et, name=\"Plot_et\")\n",
        "        #self.add_monitor(tensorboard, name=\"tensorboard\")\n",
        "\n",
        "            \n",
        "        acc_hist = collections.deque([], running_window_length)\n",
        "\n",
        "        #if self.single_output_layer:\n",
        "        self.spikes = {}\n",
        "        for layer in set(self.layers):\n",
        "            self.spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=None)\n",
        "            self.add_monitor(self.spikes[layer], name=\"%s_spikes\" % layer)\n",
        "            self.dopaminergic_layers = self.layers[\"output\"]\n",
        "        # else:\n",
        "        #     output_layers = set([layer for layer in self.layers if layer.startswith('output')])\n",
        "        #     self.output_spikes = {}\n",
        "        #     for layer in output_layers:\n",
        "        #         self.output_spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=self.time)\n",
        "        #         self.add_monitor(self.output_spikes[layer], name=\"%s_spikes\" % layer)\n",
        "        #         self.dopaminergic_layers = {name: layer for name, layer in self.layers.items() if name.startswith('output')}\n",
        "\n",
        "        val_acc = 0.0\n",
        "        acc = 0.0\n",
        "\n",
        "        reward_history = []\n",
        "        if self.load_path:\n",
        "            # try:\n",
        "            self.model_params = torch.load(self.load_path)\n",
        "            self.load_state_dict(torch.load(self.load_path)['state_dict'])\n",
        "            iteration =  self.model_params['iteration']\n",
        "            hparams = self.model_params['hparams']\n",
        "            train_accs = self.model_params['train_accs']\n",
        "            val_accs = self.model_params['val_accs']\n",
        "            acc_rewards = self.model_params['acc_rewards']\n",
        "            print(f'Previous model loaded! Resuming training from iteration {iteration}..., last running training accuracy: {train_accs[-1]}, last validation accuracy: {val_accs[-1]}\\n') if self.verbose else None\n",
        "        else:\n",
        "            print(f'Previous model not found! Training from the beginning...\\n') if self.verbose else None\n",
        "            val_accs = []\n",
        "            train_accs = []\n",
        "            acc_rewards = []\n",
        "            # except:\n",
        "            #     pass\n",
        "        pbar = tqdm(total=n_train)\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        if self.time_analysis:\n",
        "            self.sample_spikes = {'input': [], 'main1': [], 'output': []}\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            for (i, data) in enumerate(dataloader):\n",
        "                if self.load_path:\n",
        "                    #try:\n",
        "                    if i*self.batch_size <= iteration:\n",
        "                        n_train += self.batch_size\n",
        "                        continue\n",
        "                    # except:\n",
        "                    #     pass\n",
        "                if i*self.batch_size > n_train:\n",
        "                    break\n",
        "\n",
        "                image = data[\"encoded_image\"]\n",
        "                if self.label is None : label = data[\"label\"]\n",
        "\n",
        "                # Run the network on the input.\n",
        "                if gpu:\n",
        "                    inputs = {\"input\": torch.transpose(image.cuda(),0,1)}\n",
        "                else:\n",
        "                    inputs = {\"input\": torch.transpose(image,0,1)}\n",
        "                #print(self.spikes['output'].get('s'))\n",
        "\n",
        "                ### Spike clamping (baseline activity)\n",
        "\n",
        "                clamp = {}\n",
        "                if self.clamp_intensity is not None:\n",
        "                    encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "                    clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "                self.run(inputs=inputs, \n",
        "                        time=self.time, \n",
        "                        **reward_hparams,\n",
        "                        one_step=True,\n",
        "                        true_label = label,\n",
        "                        dopaminergic_layers= self.dopaminergic_layers,\n",
        "                        clamp = clamp\n",
        "                        )\n",
        "\n",
        "                # Get voltage recording.\n",
        "                #main_voltage = main_monitor.get(\"v\")\n",
        "                reward =reward_monitor.get()\n",
        "                reward_history.append(reward)\n",
        "                #tensorboard.update(step= i)\n",
        "\n",
        "                # Add to spikes recording.\n",
        "                #if self.single_output_layer:\n",
        "                lc_spikes1 = self.spikes['main1'].get('s')\n",
        "                #lc_spikes2 = self.spikes['main2'].get('s')\n",
        "                out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, n_classes, neuron_per_class)\n",
        "                sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "                predicted_label = torch.argmax(sum_spikes, dim=1)\n",
        "                \n",
        "\n",
        "                if self.time_analysis:\n",
        "                    self.sample_spikes['input'].append(self.spikes['input'].get('s'))\n",
        "                    self.sample_spikes['main1'].append(self.spikes['main1'].get('s'))\n",
        "                    self.sample_spikes['output'].append(self.spikes['output'].get('s').view(self.time, self.batch_size,n_classes, neuron_per_class))\n",
        "\n",
        "                # else:\n",
        "                #     spikes_record = torch.zeros(self.n_classes, self.time, self.neuron_per_class)\n",
        "                #     for c in range(self.n_classes):\n",
        "                #         spikes_record[c] = self.output_spikes[f\"output_{c}\"].get(\"s\").squeeze(1)\n",
        "                #     sum_spikes = spikes_record.sum(1).sum(1)\n",
        "                #     predicted_label = torch.argmax(sum_spikes)    \n",
        "\n",
        "                acc_hist.append((predicted_label == label).sum())\n",
        "    \n",
        "                w_lc1 = self.connections[('input', 'main1')].w\n",
        "                w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "                #w_lc2 = self.connections[('main1', 'main2')].w\n",
        "                #w_inh = self.connections[('output','output')].w\n",
        "                convg_lc1=  1 -  torch.mean((w_lc1 - self.wmin)*(self.wmax-w_lc1))\n",
        "                convg_out= 1 -  torch.mean((w_last_main_out - self.wmin)*(self.wmax-w_last_main_out))\n",
        "                if self.norm_factor_LC is not None:\n",
        "                    mean_norm_factor_lc = self.norm_factor_LC / w_lc1.shape[-1]\n",
        "                    convg_lc1=  1 - ( torch.mean((w_lc1 - self.wmin)*(self.wmax-w_lc1)) / ((mean_norm_factor_lc - self.wmin)*(self.wmax - mean_norm_factor_lc)) )\n",
        "                if self.norm_factor_out is not None:    \n",
        "                    mean_norm_factor_out = self.norm_factor_out / w_last_main_out.shape[-2]\n",
        "                    convg_out= 1 - ( torch.mean((w_last_main_out - self.wmin)*(self.wmax-w_last_main_out)) / ((mean_norm_factor_out - self.wmin)*(self.wmax - mean_norm_factor_out)) )\n",
        "            \n",
        "                self.convergences['lc1'].append((convg_lc1 * 10**4).round() / (10**4))\n",
        "                self.convergences['last_main_out'].append((convg_out * 10**4).round() / (10**4)) \n",
        "                #print(sum_spikes.shape)\n",
        "                # print(\"output\", sum_spikes, 'pred_label:',\n",
        "                #     predicted_label, 'GT:', label,\n",
        "                #     ', Acc Rew:', sum(reward_monitor.get()),\n",
        "                #     f\"Pos dps: {self.reward_fn.dps:.5f}, Neg dps: {self.reward_fn.neg_dps:.5f}, Rew base: {self.reward_fn.rew_base:.5f}, Pun base: {self.reward_fn.punish_base:.5f}, RPe: {self.reward_fn.reward_predict_episode:.3f}\",\n",
        "                #     f\"input_mean_fire_freq: {torch.mean(image.float())*1000:.1f},main_mean_fire_freq:{torch.mean(lc_spikes1.float())*1000:.1f}\",#\" main2_mean_fire_freq:{torch.mean(lc_spikes2.float())*1000:.1f}\",\n",
        "                #     f\"output_mean_fire_freq:{torch.mean(out_spikes.float())*1000:.1f}\",\n",
        "                #     f\"mean_lc1_w: {torch.mean(w_lc1):.5f}, mean_fc_w:{torch.mean(w_last_main_out):.5f}\",\n",
        "                #     f\"std_lc1_w: {torch.std(w_lc1):.5f}, std_fc_w:{torch.std(w_last_main_out):.5f}\",\n",
        "                #     f\"convergence_lc1: {convg_lc1:.5f}, convergence_fc: {convg_out:.5f}\",)\n",
        "                    #end = '')           \n",
        "                \n",
        "                acc = 100 * sum(acc_hist)/(len(acc_hist)*self.batch_size)\n",
        "                if self.wandb_active:\n",
        "                    wandb.log({\n",
        "                            **{' to '.join(name) + ' convergence': convergence(c).item() for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                            **{'training accuracy': acc},\n",
        "                            **{\"Reward\": sum(sum(reward))},\n",
        "                            **{' to '.join(name) + ' std': c.w.std().item() for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                            **{name + ' spikes': monitor.get('s').sum().item() for name, monitor in self.spikes.items()},\n",
        "                            **{' to '.join(name) + \" gradients\": wandb.Histogram(c.w.cpu()) for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                        },\n",
        "                        step = self.epochs_trained)\n",
        "\n",
        "\n",
        "                \n",
        "                self.reward_fn.update(accumulated_reward= sum(reward_monitor.get()), ema_window = reward_hparams['ema_window']) \n",
        "\n",
        "                if online_validate and i % val_interval == 0 and i!=0:\n",
        "                    self.reset_state_variables()\n",
        "                    val_acc = self.evaluate(val_loader, n_val, val_interval, running_window_length)\n",
        "                    #tensorboard.writer.add_scalars(\"accuracy\", {\"train\": acc, \"val\" : val_acc}, i)\n",
        "                    train_accs.append(acc)\n",
        "                    val_accs.append(val_acc)\n",
        "                    #acc_rewards.append(sum(reward_monitor.get()))\n",
        "                    if self.save_path is not None:\n",
        "                        model_params = {'state_dict': self.state_dict(), 'hparams': hparams, 'iteration': i, 'val_accs': val_accs, 'train_accs': train_accs, 'acc_rewards': acc_rewards}\n",
        "                        torch.save(model_params, self.save_path)\n",
        "                # else:\n",
        "                #     pass\n",
        "                    #tensorboard.writer.add_scalars(\"accuracy\", {\"train\": acc}, i)\n",
        "                #tensorboard.writer.add_scalar(\"reward\", sum(reward_monitor.get()), i)\n",
        "\n",
        "                # if  i % val_interval == 0 and i!=0:\n",
        "                #     fig = create_plot(self.output_spikes, reward_monitor.get(), label)\n",
        "                #     tensorboard.writer.add_figure('reward', fig, i)\n",
        "                \n",
        "                #Plot_et.plot()    \n",
        "                self.reset_state_variables()  # Reset state variables.\n",
        "                \n",
        "                pbar.set_description_str(\"Epoch: \"+str(epoch)+\", Running accuracy: \" + \"{:.2f}\".format(acc) + \"%, \" + \"Current val accuracy: \" + \"{:.2f}\".format(val_acc) + \"%, \")\n",
        "                pbar.update(len(label))\n",
        "\n",
        "                self.epochs_trained += 1\n",
        "\n",
        "        result_metrics = {'train_acc': acc, 'val_acc': val_acc}\n",
        "        # tensorboard.writer.add_hparams(\n",
        "        #     {k:(v if type(v) in (int, float, bool, str, torch.Tensor) else str(v)) for k,v in {**train_hparams, **data_hparams, **network_hparams, **reward_hparams}.items() },\n",
        "        #     result_metrics\n",
        "        # )\n",
        "    \n",
        "    def predict(self, dataloader, idx,  reward_hparams, label = None):\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "\n",
        "        for i, data in enumerate(dataloader):\n",
        "            if i*self.batch_size == idx:\n",
        "                image = data[\"encoded_image\"]\n",
        "                if self.label is None : label = data[\"label\"]\n",
        "\n",
        "        # Run the network on the input.\n",
        "        if gpu:\n",
        "            inputs = {\"input\": torch.transpose(image.cuda(),0,1)}\n",
        "        else:\n",
        "            inputs = {\"input\": torch.transpose(image,0,1)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step=True,\n",
        "                true_label = label,\n",
        "                dopaminergic_layers= self.dopaminergic_layers,\n",
        "                clamp = clamp\n",
        "                    )\n",
        "        \n",
        "        out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, n_classes, neuron_per_class)\n",
        "        sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "        predicted_label = torch.argmax(sum_spikes, dim=1)\n",
        "\n",
        "        if self.time_analysis:\n",
        "            self.sample_spikes['input'].append(self.spikes['input'].get('s'))\n",
        "            self.sample_spikes['main1'].append(self.spikes['main1'].get('s'))\n",
        "            self.sample_spikes['output'].append(self.spikes['output'].get('s').view(self.time, self.batch_size, n_classes, neuron_per_class))\n",
        "\n",
        "        w_lc1 = self.connections[('input', 'main1')].w\n",
        "        w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "        #w_lc2 = self.connections[('main1', 'main2')].w\n",
        "        #w_inh = self.connections[('output','output')].w\n",
        "        convg_lc1=  1 -  torch.mean((w_lc1 - self.wmin)*(self.wmax-w_lc1))\n",
        "        convg_out= 1 -  torch.mean((w_last_main_out - self.wmin)*(self.wmax-w_last_main_out))\n",
        "        if self.norm_factor_LC is not None:\n",
        "            mean_norm_factor_lc = self.norm_factor_LC / w_lc1.shape[-1]\n",
        "            convg_lc1=  1 - ( torch.mean((w_lc1 - self.wmin)*(self.wmax-w_lc1)) / ((mean_norm_factor_lc - self.wmin)*(self.wmax - mean_norm_factor_lc)) )\n",
        "        if self.norm_factor_out is not None:    \n",
        "            mean_norm_factor_out = self.norm_factor_out / w_last_main_out.shape[-2]\n",
        "            convg_out= 1 - ( torch.mean((w_last_main_out - self.wmin)*(self.wmax-w_last_main_out)) / ((mean_norm_factor_out - self.wmin)*(self.wmax - mean_norm_factor_out)) )\n",
        "        \n",
        "        self.convergences['lc1'].append((convg_lc1 * 10**4).round() / (10**4))\n",
        "        self.convergences['last_main_out'].append((convg_out * 10**4).round() / (10**4)) \n",
        "    \n",
        "        # print(\"\\routput\", sum_spikes, 'pred_label:',\n",
        "        #     predicted_label, 'GT:', label,\n",
        "        #     ', Acc Rew:', round(sum(reward_monitor.get()).item(),4),\n",
        "        #     f\"Pos dps: {self.reward_fn.dps:.5f}, Neg dps: {self.reward_fn.neg_dps:.5f}, Rew base: {self.reward_fn.rew_base:.5f}, Pun base: {self.reward_fn.punish_base:.5f}, RPe: {self.reward_fn.reward_predict_episode:.3f}\",\n",
        "        #     f\"input_mean_fire_freq: {torch.mean(image.float())*1000:.1f},main_mean_fire_freq:{torch.mean(lc_spikes1.float())*1000:.1f}\",#\" main2_mean_fire_freq:{torch.mean(lc_spikes2.float())*1000:.1f}\",\n",
        "        #     f\"output_mean_fire_freq:{torch.mean(out_spikes.float())*1000:.1f}\",\n",
        "        #     f\"mean_lc1_w: {torch.mean(w_lc1):.5f}, mean_fc_w:{torch.mean(w_last_main_out):.5f}\",\n",
        "        #     f\"std_lc1_w: {torch.std(w_lc1):.5f}, std_fc_w:{torch.std(w_last_main_out):.5f}\",\n",
        "        #     f\"convergence_lc1: {convg_lc1:.5f}, convergence_fc: {convg_out:.5f}\",\n",
        "        #     end = '')           \n",
        "        \n",
        "        acc = 100 * sum(acc_hist)/(len(acc_hist)*self.batch_size)\n",
        "        \n",
        "        self.reward_fn.update(accumulated_reward= sum(reward_monitor.get()), ema_window = reward_hparams['ema_window']) \n",
        "        \n",
        "        self.reset_state_variables()  # Reset state variables.\n",
        "\n",
        "        return label, predicted_label, reward_monitor.get() \n",
        "\n",
        "    def evaluate(self, val_loader, n_val, val_interval, running_window_length):\n",
        "        manual_seed(SEED)\n",
        "        acc_hist_val = collections.deque([], running_window_length)\n",
        "\n",
        "        spikes_val = {}\n",
        "\n",
        "        self.train(False)\n",
        "        self.learning = False\n",
        "\n",
        "        GT, y_pred = [], []\n",
        "        for (i, data) in enumerate(val_loader):\n",
        "            if i*self.batch_size > n_val:\n",
        "                break\n",
        "\n",
        "            image = data[\"encoded_image\"]\n",
        "            if self.label is None : \n",
        "              label = data[\"label\"]\n",
        "            else :\n",
        "              label = self.label\n",
        "\n",
        "            # Run the network on the input.\n",
        "            if gpu:\n",
        "                inputs = {\"input\": torch.transpose(image.cuda(),0,1)}\n",
        "            else:\n",
        "                inputs = {\"input\": torch.transpose(image,0,1)}\n",
        "\n",
        "            self.run(inputs=inputs, \n",
        "                    time=self.time, \n",
        "                    **reward_hparams,\n",
        "                    one_step = True,\n",
        "                    true_label = label,\n",
        "                    dopaminergic_layers= self.dopaminergic_layers,\n",
        "                     )\n",
        "            # Add to spikes recording.\n",
        "            #if self.single_output_layer:\n",
        "            out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, n_classes, neuron_per_class)\n",
        "            sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "            predicted_label = torch.argmax(sum_spikes, dim=1)\n",
        "            # else:\n",
        "            #     spikes_record = torch.zeros(self.n_classes, self.time, self.neuron_per_class)\n",
        "            #     for c in range(self.n_classes):\n",
        "            #         spikes_record[c] = self.output_spikes[f\"output_{c}\"].get(\"s\").squeeze(1)\n",
        "            #     sum_spikes = spikes_record.sum(1).sum(1)\n",
        "            #     predicted_label = torch.argmax(sum_spikes)\n",
        "\n",
        "            acc_hist_val.append((predicted_label == label).sum())\n",
        "            \n",
        "            GT.append(label)\n",
        "            y_pred.append(predicted_label)\n",
        "            \n",
        "            # print(\"\\r*validation: output\",sum_spikes,\n",
        "            #     'predicted_label:', predicted_label, 'GT:', label,\n",
        "            #     end = '') if self.verbose else None\n",
        "            \n",
        "            self.reset_state_variables()  # Reset state variables.\n",
        "\n",
        "        self.train(True)\n",
        "        self.learning = True\n",
        "        \n",
        "        if self.confusion_matrix:\n",
        "            self.plot_confusion_matrix(GT, y_pred)\n",
        "\n",
        "        if self.lc_weights_vis:\n",
        "            plot_locally_connected_weights_meh(self.connections[('input','main1')].w,self.n_channels1,self.in_channels,0,self.crop_size,self.filter_size1,self.layers['main1'].shape[1])\n",
        "            plt.show()\n",
        "            if self.deep:\n",
        "                plot_locally_connected_weights_meh(self.connections[(self.hidden2_name,'main1')].w,self.n_channels2,self.n_channels1,0,self.layers[self.hidden2_name].shape[1],self.filter_size2,self.layers['main2'].shape[1])                \n",
        "                plt.show()\n",
        "        \n",
        "        if self.lc_convergence_vis: \n",
        "            plot_convergence_and_histogram(self.connections[('input','main1')].w,self.convergences['lc1'])\n",
        "            plt.show()\n",
        "        if self.out_convergence_vis:\n",
        "            plot_convergence_and_histogram(self.connections[(self.final_connection_source_name,'output')].w,self.convergences['last_main_out'])\n",
        "            plt.show() \n",
        "\n",
        "        if self.out_weights_vis:\n",
        "            plot_locally_connected_weights_meh3(self.connections[('input','main1')].w,self.connections[(self.final_connection_source_name,'output')].w,\\\n",
        "                                                0,0,self.neuron_per_class,self.n_channels1,self.in_channels,0,self.crop_size,self.filter_size1,self.layers['main1'].shape[1]) \n",
        "            plt.show()\n",
        "            plot_locally_connected_weights_meh3(self.connections[('input','main1')].w,self.connections[(self.final_connection_source_name,'output')].w,\\\n",
        "                                                0,1,self.neuron_per_class,self.n_channels1,self.in_channels,0,self.crop_size,self.filter_size1,self.layers['main1'].shape[1])           \n",
        "            plt.show()\n",
        "        val_acc = 100 * sum(acc_hist_val)/(len(acc_hist_val)*self.batch_size)\n",
        "        return val_acc\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_confusion_matrix(GT, y_predicted):\n",
        "        cm = confusion_matrix(GT, y_predicted)\n",
        "        plt.figure(figsize = (10,7))\n",
        "        sn.heatmap(cm, annot=True)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Truth')\n",
        "        plt.show()\n",
        "\n",
        "    \n",
        "    def one_step(self,datum,label = None):\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        image = datum[\"encoded_image\"]\n",
        "        if label is None : label = datum[\"label\"]\n",
        "\n",
        "        if gpu:\n",
        "            inputs = {\"input\": torch.transpose(image.cuda(),0,1)}\n",
        "        else:\n",
        "            inputs = {\"input\": torch.transpose(image,0,1)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step = True,\n",
        "                true_label = label,\n",
        "                dopaminergic_layers= self.dopaminergic_layers,\n",
        "                )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zuUBU9pU3vE"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2DW7dB11jdqi"
      },
      "outputs": [],
      "source": [
        "class ClassSelector(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"Select target classes from the dataset\"\"\"\n",
        "    def __init__(self, target_classes, data_source, mask = None):\n",
        "        if mask is not None:\n",
        "            self.mask = mask\n",
        "        else:\n",
        "            raise ValueError()\n",
        "            self.mask = torch.tensor([1 if data_source[i]['label'] in target_classes else 0 for i in range(len(data_source))])\n",
        "        self.data_source = data_source\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter([i.item() for i in torch.nonzero(self.mask)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H0yDLE3-aHo",
        "outputId": "0988fc08-4d39-4fce-e879-91217496fe73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sPA_h5Ruuric"
      },
      "outputs": [],
      "source": [
        "# !cp /content/drive/MyDrive/train-20220122T160532Z-001.zip ./\n",
        "# !cp /content/drive/MyDrive/test-20220122T154947Z-001.zip ./ \n",
        "\n",
        "# !unzip /content/test-20220122T154947Z-001.zip -d ./test\n",
        "# !unzip /content/train-20220122T160532Z-001.zip -d ./train "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YTEP_hb4GrRU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "from typing import Any, Callable, cast, Dict, List, Optional, Tuple\n",
        "from typing import Union\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "\n",
        "def has_file_allowed_extension(filename: str, extensions: Union[str, Tuple[str, ...]]) -> bool:\n",
        "    \"\"\"Checks if a file is an allowed extension.\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "        extensions (tuple of strings): extensions to consider (lowercase)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the filename ends with one of given extensions\n",
        "    \"\"\"\n",
        "    return filename.lower().endswith(extensions if isinstance(extensions, str) else tuple(extensions))\n",
        "\n",
        "\n",
        "def is_image_file(filename: str) -> bool:\n",
        "    \"\"\"Checks if a file is an allowed image extension.\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the filename ends with a known image extension\n",
        "    \"\"\"\n",
        "    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "    \"\"\"Finds the class folders in a dataset.\n",
        "\n",
        "    See :class:`DatasetFolder` for details.\n",
        "    \"\"\"\n",
        "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
        "    if not classes:\n",
        "        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
        "\n",
        "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "\n",
        "def make_dataset(\n",
        "    directory: str,\n",
        "    class_to_idx: Optional[Dict[str, int]] = None,\n",
        "    extensions: Optional[Union[str, Tuple[str, ...]]] = None,\n",
        "    is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        ") -> List[Tuple[str, int]]:\n",
        "    \"\"\"Generates a list of samples of a form (path_to_sample, class).\n",
        "\n",
        "    See :class:`DatasetFolder` for details.\n",
        "\n",
        "    Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function\n",
        "    by default.\n",
        "    \"\"\"\n",
        "    directory = os.path.expanduser(directory)\n",
        "\n",
        "    if class_to_idx is None:\n",
        "        _, class_to_idx = find_classes(directory)\n",
        "    elif not class_to_idx:\n",
        "        raise ValueError(\"'class_to_index' must have at least one entry to collect any samples.\")\n",
        "\n",
        "    both_none = extensions is None and is_valid_file is None\n",
        "    both_something = extensions is not None and is_valid_file is not None\n",
        "    if both_none or both_something:\n",
        "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
        "\n",
        "    if extensions is not None:\n",
        "\n",
        "        def is_valid_file(x: str) -> bool:\n",
        "            return has_file_allowed_extension(x, extensions)  # type: ignore[arg-type]\n",
        "\n",
        "    is_valid_file = cast(Callable[[str], bool], is_valid_file)\n",
        "\n",
        "    instances = []\n",
        "    available_classes = set()\n",
        "    for target_class in sorted(class_to_idx.keys()):\n",
        "        class_index = class_to_idx[target_class]\n",
        "        target_dir = os.path.join(directory, target_class)\n",
        "        if not os.path.isdir(target_dir):\n",
        "            continue\n",
        "        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                if is_valid_file(path):\n",
        "                    item = path, class_index\n",
        "                    instances.append(item)\n",
        "\n",
        "                    if target_class not in available_classes:\n",
        "                        available_classes.add(target_class)\n",
        "\n",
        "    empty_classes = set(class_to_idx.keys()) - available_classes\n",
        "    if empty_classes:\n",
        "        msg = f\"Found no valid file for the classes {', '.join(sorted(empty_classes))}. \"\n",
        "        if extensions is not None:\n",
        "            msg += f\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\n",
        "        raise FileNotFoundError(msg)\n",
        "\n",
        "    return instances\n",
        "\n",
        "\n",
        "class DatasetFolder(VisionDataset):\n",
        "    \"\"\"A generic data loader.\n",
        "\n",
        "    This default directory structure can be customized by overriding the\n",
        "    :meth:`find_classes` method.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory path.\n",
        "        loader (callable): A function to load a sample given its path.\n",
        "        extensions (tuple[string]): A list of allowed extensions.\n",
        "            both extensions and is_valid_file should not be passed.\n",
        "        transform (callable, optional): A function/transform that takes in\n",
        "            a sample and returns a transformed version.\n",
        "            E.g, ``transforms.RandomCrop`` for images.\n",
        "        target_transform (callable, optional): A function/transform that takes\n",
        "            in the target and transforms it.\n",
        "        is_valid_file (callable, optional): A function that takes path of a file\n",
        "            and check if the file is a valid file (used to check of corrupt files)\n",
        "            both extensions and is_valid_file should not be passed.\n",
        "\n",
        "     Attributes:\n",
        "        classes (list): List of the class names sorted alphabetically.\n",
        "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "        samples (list): List of (sample path, class_index) tuples\n",
        "        targets (list): The class_index value for each image in the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        loader: Callable[[str], Any],\n",
        "        extensions: Optional[Tuple[str, ...]] = None,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
        "        classes, class_to_idx = self.find_classes(self.root)\n",
        "        samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
        "\n",
        "        self.loader = loader\n",
        "        self.extensions = extensions\n",
        "\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.samples = samples\n",
        "        self.targets = [s[1] for s in samples]\n",
        "\n",
        "    @staticmethod\n",
        "    def make_dataset(\n",
        "        directory: str,\n",
        "        class_to_idx: Dict[str, int],\n",
        "        extensions: Optional[Tuple[str, ...]] = None,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "    ) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Generates a list of samples of a form (path_to_sample, class).\n",
        "\n",
        "        This can be overridden to e.g. read files from a compressed zip file instead of from the disk.\n",
        "\n",
        "        Args:\n",
        "            directory (str): root dataset directory, corresponding to ``self.root``.\n",
        "            class_to_idx (Dict[str, int]): Dictionary mapping class name to class index.\n",
        "            extensions (optional): A list of allowed extensions.\n",
        "                Either extensions or is_valid_file should be passed. Defaults to None.\n",
        "            is_valid_file (optional): A function that takes path of a file\n",
        "                and checks if the file is a valid file\n",
        "                (used to check of corrupt files) both extensions and\n",
        "                is_valid_file should not be passed. Defaults to None.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: In case ``class_to_idx`` is empty.\n",
        "            ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.\n",
        "            FileNotFoundError: In case no valid file was found for any class.\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[str, int]]: samples of a form (path_to_sample, class)\n",
        "        \"\"\"\n",
        "        if class_to_idx is None:\n",
        "            # prevent potential bug since make_dataset() would use the class_to_idx logic of the\n",
        "            # find_classes() function, instead of using that of the find_classes() method, which\n",
        "            # is potentially overridden and thus could have a different logic.\n",
        "            raise ValueError(\"The class_to_idx parameter cannot be None.\")\n",
        "        return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)\n",
        "\n",
        "    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "        \"\"\"Find the class folders in a dataset structured as follows::\n",
        "\n",
        "            directory/\n",
        "            ├── class_x\n",
        "            │   ├── xxx.ext\n",
        "            │   ├── xxy.ext\n",
        "            │   └── ...\n",
        "            │       └── xxz.ext\n",
        "            └── class_y\n",
        "                ├── 123.ext\n",
        "                ├── nsdf3.ext\n",
        "                └── ...\n",
        "                └── asd932_.ext\n",
        "\n",
        "        This method can be overridden to only consider\n",
        "        a subset of classes, or to adapt to a different dataset directory structure.\n",
        "\n",
        "        Args:\n",
        "            directory(str): Root directory path, corresponding to ``self.root``\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If ``dir`` has no class folders.\n",
        "\n",
        "        Returns:\n",
        "            (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\n",
        "        \"\"\"\n",
        "        return find_classes(directory)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (sample, target) where target is class_index of the target class.\n",
        "        \"\"\"\n",
        "        path, target = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "IMG_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")\n",
        "\n",
        "\n",
        "def pil_loader(path: str) -> Image.Image:\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, \"rb\") as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert(\"RGB\")\n",
        "\n",
        "\n",
        "# TODO: specify the return type\n",
        "def accimage_loader(path: str) -> Any:\n",
        "    import accimage\n",
        "\n",
        "    try:\n",
        "        return accimage.Image(path)\n",
        "    except OSError:\n",
        "        # Potentially a decoding problem, fall back to PIL.Image\n",
        "        return pil_loader(path)\n",
        "\n",
        "\n",
        "def default_loader(path: str) -> Any:\n",
        "    from torchvision import get_image_backend\n",
        "\n",
        "    if get_image_backend() == \"accimage\":\n",
        "        return accimage_loader(path)\n",
        "    else:\n",
        "        return pil_loader(path)\n",
        "\n",
        "\n",
        "class ImageFolder2(DatasetFolder):\n",
        "    \"\"\"A generic data loader where the images are arranged in this way by default: ::\n",
        "\n",
        "        root/dog/xxx.png\n",
        "        root/dog/xxy.png\n",
        "        root/dog/[...]/xxz.png\n",
        "\n",
        "        root/cat/123.png\n",
        "        root/cat/nsdf3.png\n",
        "        root/cat/[...]/asd932_.png\n",
        "\n",
        "    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n",
        "    the same methods can be overridden to customize the dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory path.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        loader (callable, optional): A function to load an image given its path.\n",
        "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
        "            and check if the file is a valid file (used to check of corrupt files)\n",
        "\n",
        "     Attributes:\n",
        "        classes (list): List of the class names sorted alphabetically.\n",
        "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "        imgs (list): List of (image path, class_index) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        intensity,\n",
        "        image_encoder: Optional[Encoder] = None,\n",
        "        label_encoder: Optional[Encoder] = None,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        loader: Callable[[str], Any] = default_loader,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            root,\n",
        "            loader,\n",
        "            IMG_EXTENSIONS if is_valid_file is None else None,\n",
        "            transform=transform,\n",
        "            target_transform=target_transform,\n",
        "            is_valid_file=is_valid_file,\n",
        "        )\n",
        "        self.imgs = self.samples\n",
        "        self.intensity = intensity \n",
        "\n",
        "        # Allow the passthrough of None, but change to NullEncoder\n",
        "        if image_encoder is None:\n",
        "            image_encoder = NullEncoder()\n",
        "\n",
        "        if label_encoder is None:\n",
        "            label_encoder = NullEncoder()\n",
        "\n",
        "        self.image_encoder = image_encoder\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __getitem__(self, ind: int) -> Dict[str, torch.Tensor]:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Utilizes the ``torchvision.dataset`` parent class to grab the data, then\n",
        "        encodes using the supplied encoders.\n",
        "        :param int ind: Index to grab data at.\n",
        "        :return: The relevant data and encoded data from the requested index.\n",
        "        \"\"\"\n",
        "        image, label = super().__getitem__(ind)\n",
        "\n",
        "        # print(image.shape)\n",
        "        output = {\n",
        "            \"image\": image,\n",
        "            \"label\": label,\n",
        "            \"encoded_image\": self.image_encoder(self.intensity * image.squeeze(0)),\n",
        "            \"encoded_label\": self.label_encoder(label),\n",
        "        }\n",
        "\n",
        "        # print(output['label'], output['encoded_image'].shape)\n",
        "        # plt.imshow(output['image'][0,...])\n",
        "        # plt.show()\n",
        "        \n",
        "        # plt.imshow(torch.sum(output['encoded_image'], axis = 0))\n",
        "        # plt.show()\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DlCXBY0DU3Mc"
      },
      "outputs": [],
      "source": [
        "# kernels = [DoGKernel(7,1,2),\n",
        "# \t\t\tDoGKernel(7,2,1),]\n",
        "# filter = Filter(kernels, padding = 3, thresholds = 50/255)\n",
        "# Load MNIST data.\n",
        "\n",
        "def load_datasets(network_hparams, data_hparams, mask=None, test_mask=None, batch_size=1):\n",
        "    manual_seed(SEED)\n",
        "    # dataset = MNIST(\n",
        "    #     PoissonEncoder(time=network_hparams['time'], dt=network_hparams['dt']),\n",
        "    #     None,\n",
        "    #     root= os.path.join(\"..\", \"..\", \"data\", \"MNIST\"),\n",
        "    #     download=True,\n",
        "    #     transform=transforms.Compose(\n",
        "    #         [transforms.ToTensor(),\n",
        "    #         # filter,\n",
        "    #         transforms.Lambda(lambda x: (\n",
        "    #             x.round() if data_hparams['round_input'] else x\n",
        "    #         ) * data_hparams['intensity']),\n",
        "    #         transforms.CenterCrop(data_hparams['crop_size'])]\n",
        "    #     ),\n",
        "    # )\n",
        "\n",
        "    dataset = ImageFolder2(\n",
        "                        root =  \"/content/drive/MyDrive/Data/train\", \n",
        "                        intensity = data_hparams['intensity'],\n",
        "                        image_encoder = PoissonEncoder(time=network_hparams['time'], dt=network_hparams['dt']),\n",
        "                        transform = transforms.Compose([transforms.Grayscale(), transforms.ToTensor(),\n",
        "                                                        transforms.CenterCrop(data_hparams['crop_size'])])\n",
        "                            # filter,\n",
        "                            # transforms.Lambda(lambda x: (\n",
        "                            #     x.round() if data_hparams['round_input'] else x\n",
        "                            # ) * data_hparams['intensity']),\n",
        "                            # transforms.CenterCrop(data_hparams['crop_size'])]\n",
        "                        )\n",
        "\n",
        "\n",
        "    # Create a dataloader to iterate and batch data\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                            sampler = ClassSelector(\n",
        "                                                    target_classes = target_classes,\n",
        "                                                    data_source = dataset,\n",
        "                                                    mask = mask,\n",
        "                                                    ) if target_classes else None\n",
        "                                            )\n",
        "\n",
        "    # # Load test dataset\n",
        "    # test_dataset = MNIST(   \n",
        "    #     PoissonEncoder(time=network_hparams['time'], dt=network_hparams['dt']),\n",
        "    #     None,\n",
        "    #     root=os.path.join(\"..\", \"..\", \"data\", \"MNIST\"),\n",
        "    #     download=True,\n",
        "    #     train=False,\n",
        "    #     transform=transforms.Compose(\n",
        "    #         [transforms.ToTensor(),\n",
        "    #         # filter,\n",
        "    #         transforms.Lambda(lambda x: (\n",
        "    #             x.round() if data_hparams['round_input'] else x\n",
        "    #         ) * data_hparams['intensity']),\n",
        "    #         transforms.CenterCrop(data_hparams['crop_size'])]\n",
        "    #     ),\n",
        "    # )\n",
        "\n",
        "    test_dataset = ImageFolder2(\n",
        "                        root = \"/content/drive/MyDrive/Data/test\", \n",
        "                        intensity = data_hparams['intensity'],\n",
        "                        image_encoder = PoissonEncoder(time=network_hparams['time'], dt=network_hparams['dt']),\n",
        "                        transform = transforms.Compose([transforms.Grayscale(), transforms.ToTensor(),\n",
        "                                                        transforms.CenterCrop(data_hparams['crop_size'])])\n",
        "                            # filter,\n",
        "                            # transforms.Lambda(lambda x: (\n",
        "                            #     x.round() if data_hparams['round_input'] else x\n",
        "                            # ) * data_hparams['intensity']),\n",
        "                            # transforms.CenterCrop(data_hparams['crop_size'])]\n",
        "                        )\n",
        "        \n",
        "    val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                            sampler = ClassSelector(\n",
        "                                                    target_classes = target_classes,\n",
        "                                                    data_source = test_dataset,\n",
        "                                                    mask = mask_test,\n",
        "                                                    ) if target_classes else None\n",
        "                                            )\n",
        "    \n",
        "\n",
        "    return dataloader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qplEHOW4SMCs"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ldZx5enrSMCt"
      },
      "outputs": [],
      "source": [
        "train_hparams = {\n",
        "    'n_train' : 2000,\n",
        "    'n_test' : 1,\n",
        "    'n_val' : 1,\n",
        "    'val_interval' : 200,\n",
        "    'running_window_length': 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nJr6PpX6SMCt"
      },
      "outputs": [],
      "source": [
        "# Dataset Hyperparameter\n",
        "mask = None\n",
        "mask_test = None\n",
        "n_classes = 2\n",
        "target_classes = None \n",
        "\n",
        "data_hparams = { \n",
        "    'intensity': 128,\n",
        "    'time': 256,\n",
        "    'crop_size': 40,\n",
        "    'round_input': False,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "G0CQb5sQSMCt"
      },
      "outputs": [],
      "source": [
        "n_neurons = 200\n",
        "neuron_per_class = int(n_neurons/n_classes)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': 40,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': 100,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': 32,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': 4,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_classes': n_classes,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': False,\n",
        "    'has_decision_period': False,\n",
        "    'observation_period': 0,\n",
        "    'decision_period': 0,\n",
        "    'online': True,\n",
        "    'local_rewarding': False,\n",
        "\n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "\n",
        "    # Learning\n",
        "    'update_rule_LC': PostPre,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDP,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.0001,0.01),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output':0.1,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': True,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': 'between_layers',\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "\n",
        "    # Normalization\n",
        "    'norm_factor_LC': 0.25*24*24,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamping\n",
        "    'clamp_intensity': None,\n",
        "\n",
        "    # Save\n",
        "    'save_path': '/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f32_s4_inh100_norm25_ch100_xor.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'LC_weights_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25_weights.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': False,\n",
        "    'out_weights_vis': False,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': False,\n",
        "}\n",
        "\n",
        "reward_hparams= {\n",
        "    'n_labels': n_classes,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    \n",
        "    'variant': 'pure_per_spike',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': 1.,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0005,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n",
        "\n",
        "\n",
        "network_hparams.update(\n",
        "    {\n",
        "     'time': 256,\n",
        "     'time_analysis': True,\n",
        "    }\n",
        ")\n",
        "\n",
        "dataloader,val_loader = load_datasets(network_hparams, data_hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZOlT6WZ8SMCt"
      },
      "outputs": [],
      "source": [
        "# for (i, datum) in enumerate(dataloader):\n",
        "#     print(i, datum['encoded_image'].shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_Lm8h5EzSMCt"
      },
      "outputs": [],
      "source": [
        "# dataloader_iterator = iter(dataloader)\n",
        "# for i in range(10):\n",
        "#     datum = next(dataloader_iterator)\n",
        "#     plt.imshow(datum['encoded_image'].squeeze().sum(0))\n",
        "#     plt.show()\n",
        "#     net.one_step(datum)\n",
        "#     spikes = net.spikes[\"main1\"].get(\"s\").sum(0).squeeze().view(30,30)\n",
        "#     print(torch.max(spikes))\n",
        "#     plot_locally_connected_weights_meh2(net.connections[('input', 'main1')].w,spikes,100,1,0,20,12,3)\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx16gkoq87SO"
      },
      "source": [
        "# Feature Extraction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "31ef119669a740b3a7dca675d0844952",
            "60fdbad35f1f488398e3db70662c04fa",
            "64f26fa7724949b4b86c192a54e025e4",
            "6d787bc3c0474fd882b68c4218d047dc",
            "38d27b0ba8f54d04b9cab41ae8e8286f",
            "038a0cd0737c46f490bbce49cdb08f2e",
            "8c984a41973a463fb75757e12e2ca0da",
            "762f117169034b648c91e6ac742b3169",
            "40f94965b86f4350a912656f97026b5f",
            "ee79153562d145308350859c44dc78ae",
            "228a4e5d5c3749e2934a737684e4e425"
          ]
        },
        "id": "un535KZe8-bA",
        "outputId": "84c40e6d-6756-4ff8-dc89-4630c0b199eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/singularbrain/biolcnet/runs/tsga701g\" target=\"_blank\">restful-snow-177</a></strong> to <a href=\"https://wandb.ai/singularbrain/biolcnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "tensor(0.5005) tensor(0.2888)\n",
            "Previous model not found! Training from the beginning...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31ef119669a740b3a7dca675d0844952",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "manual_seed(SEED)\n",
        "hparams = {**reward_hparams, **network_hparams, **train_hparams, **data_hparams}\n",
        "if WANDB:\n",
        "    wandb.init(project='BioLCNet', entity='singularbrain', config=network_hparams)\n",
        "    \n",
        "net = LCNet(**hparams, reward_fn = DynamicDopamineInjection, wandb_active = WANDB)\n",
        "net.fit(dataloader = dataloader, val_loader = val_loader, reward_hparams = reward_hparams, **train_hparams)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "\n",
        "plot_locally_connected_weights(\n",
        "    weights= net.connections[(\"input\",\"main1\")].w,\n",
        "    n_filters= hparams[\"n_channels1\"],\n",
        "    kernel_size= hparams[\"filter_size1\"],\n",
        "    conv_size= compute_size(hparams[\"crop_size\"], hparams[\"filter_size1\"], hparams[\"stride1\"]),\n",
        "    locations= net.connections[(\"input\",\"main1\")].locations,\n",
        "    input_sqrt= hparams[\"crop_size\"],\n",
        "    wmin = 0.0,\n",
        "    wmax = 1.0,\n",
        "    figsize=(20, 20),\n",
        "    cmap = \"hot_r\",)"
      ],
      "metadata": {
        "id": "NFTfcAG5ybpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "of-xyStlyIBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.connections[('input', 'main1')]"
      ],
      "metadata": {
        "id": "28ex-V1bm3kD",
        "outputId": "5faee3d4-7f6a-496b-8624-c344210e7fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LocalConnectionOrig(\n",
              "  (source): Input()\n",
              "  (target): AdaptiveLIFNodes()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_locally_connected_weights_meh(net.connections[('input','main1')].w,net.connections[(net.final_connection_source_name,'output')].w,\\\n",
        "#                                     0,5,net.neuron_per_class,net.n_channels1,net.in_channels,0,net.crop_size,net.filter_size1,net.layers['main1'].shape[1])"
      ],
      "metadata": {
        "id": "3Yh7YqF7pVg6"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqAFucAUDb8"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0Wh_eJIFkO-"
      },
      "outputs": [],
      "source": [
        "train_hparams = {\n",
        "    'n_train' : 60000,\n",
        "    'n_test' : 9999,\n",
        "    'n_val' : 1,\n",
        "    'val_interval' : 250,\n",
        "    'running_window_length': 500,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVOqxcYtFd5T"
      },
      "outputs": [],
      "source": [
        "# Dataset Hyperparameters\n",
        "target_classes = (0,1,2)\n",
        "if target_classes:\n",
        "    npz_file = np.load(f'DeepBioLCNet/mask_{\"_\".join([str(i) for i in target_classes])}.npz')\n",
        "    # npz_file = np.load('bindsnet/mask_0_1.npz') ##### KESAFAT KARI !!!\n",
        "    mask, mask_test = torch.from_numpy(npz_file['arr_0']), torch.from_numpy(npz_file['arr_1'])\n",
        "    n_classes = len(target_classes)\n",
        "    \n",
        "else:\n",
        "    mask = None\n",
        "    mask_test = None\n",
        "    n_classes = 10\n",
        "\n",
        "data_hparams = { \n",
        "    'intensity': 1024*2,\n",
        "    'crop_size': 20,\n",
        "    'round_input': False,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XDRGEn6Sp7jR"
      },
      "outputs": [],
      "source": [
        "# for (i, datum) in enumerate(dataloader):\n",
        "#     print(i, datum['encoded_image'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TerGeJoFdzg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "d9845b02-3c86-4a8b-830b-ad32dde55801"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-8950b1ef89be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     }\n\u001b[1;32m    127\u001b[0m )\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "n_neurons = 3 #100\n",
        "neuron_per_class = int(n_neurons/n_classes)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': 20,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': 25,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': 5,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': 2,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_classes': n_classes,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': True,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': 25,\n",
        "    'decision_period': 25,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': PostPre,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDP,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0,1e-4),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output':0.1,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': True,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': 'between_layers',\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': 0.25*5*5,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': None,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'LC_weights_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25_weights.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': False,\n",
        "    'out_weights_vis': False,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': False,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "reward_hparams= {\n",
        "    'n_labels': n_classes,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': .2,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0005,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n",
        "\n",
        "\n",
        "network_hparams.update(\n",
        "    {\n",
        "     'time': 25+25+25, #320,\n",
        "     'time_analysis': True,\n",
        "    }\n",
        ")\n",
        "dataloader, val_loader = load_datasets(network_hparams, data_hparams, mask, mask_test, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqb0Ph5AYdRv"
      },
      "outputs": [],
      "source": [
        "# dataloader_iterator = iter(dataloader)\n",
        "# for i in range(10):\n",
        "#     datum = next(dataloader_iterator)\n",
        "#     plt.imshow(datum['encoded_image'].squeeze().sum(0))\n",
        "#     plt.show()\n",
        "#     net.one_step(datum)\n",
        "#     spikes = net.spikes[\"main1\"].get(\"s\").sum(0).squeeze().view(30,30)\n",
        "#     print(torch.max(spikes))\n",
        "#     plot_locally_connected_weights_meh2(net.connections[('input', 'main1')].w,spikes,100,1,0,20,12,3)\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokdidkrV2Z5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Venb2KhSYrT_"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "#     drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRruWKq0kHbz"
      },
      "source": [
        "### Hyperparameters 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "3963942cc0d24dc6ac2b8664d5fcef59",
            "3394bed82e554ef39e392d74fd6b838f",
            "df8c3560d72e4bfca0a45684dc7ca0e5",
            "ecd12401662a4d3a90ebcfc0d5c6c915",
            "d7e28a2eac4b4ea1a3a5b216dbcba8bf",
            "12b937dca3ba49bd9588436ea3f149b1",
            "97bb9bba45c44d789d827f797066f0f8",
            "8cd3eb20697f4693bb4f6b85ac2364a5",
            "6c694566c97f410cb143f338f7b488e1",
            "9b08d3b5b13842079cb24a92cbaa1bba",
            "f4b020cb6b6e4282837a8703d65f3aa7",
            "491f7bb440cb4b9b8e7cd71e035cd0ee",
            "c3fb6433903c40dd871cca2247a807e6",
            "7c02649744224b1e9884beb9ce7ffc1f",
            "17e248d5c9a34014bcab95b9e7a968a0",
            "fb8a04e3bac04e6a9b1d365ba2bb7ebf",
            "f16501f36d0e4866b041d376b0d891b0",
            "2b36b5b22fdd4cf290cd2d5373548e6a",
            "9eb25a82fa064b6284b9d40f34435465"
          ]
        },
        "id": "oThYyYvHJzeP",
        "outputId": "4de0a15a-8d17-4508-8cf8-159d98d50345"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:2ss5fnl6) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 733... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3963942cc0d24dc6ac2b8664d5fcef59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Reward</td><td>▂▅▂▅▁▇▅▆▁▆▂▁█▆▅▅▁█▁▃▆▂▁▃▆▅▁▅▂▃▂▅▃▃▅▅▃▂▇▇</td></tr><tr><td>input spikes</td><td>▄▂▃▂▃▁▃▂▃▂▂▅▁▄▂▃▄▄▃▃▄▃▂▄▅▃▁▃▂▃▂▃▃▂▃▃▃▂▂█</td></tr><tr><td>input to main1 convergence</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>input to main1 std</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>main1 spikes</td><td>█▆▅▄▃▃▄▃▃▂▃▃▃▃▂▃▃▃▃▂▃▂▂▃▃▂▁▁▁▁▁▂▁▂▁▁▁▁▁▄</td></tr><tr><td>main1 to output convergence</td><td>▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▃▁▂▂▃▄▃▂▃▅▅▅▆▆▇▇██▇▇▇▇</td></tr><tr><td>main1 to output std</td><td>██▄▆▅▇▇▆▆▆▆▄▇▄▇▅▆▆▆▅▄▄▇▄▁▁▃▇█▇▇▆▆▇█▆▇▅▅█</td></tr><tr><td>output spikes</td><td>█▆▆▅▅▄▅▄▄▃▃▄▃▄▃▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▂▂▁▂▁▁▁▁▁▃</td></tr><tr><td>training accuracy</td><td>█▅▅▆▆████▇▇▇███▇▇▆▆▆▆▅▄▄▃▃▃▃▂▂▁▁▁▁▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Reward</td><td>160.0</td></tr><tr><td>input spikes</td><td>130697</td></tr><tr><td>input to main1 convergence</td><td>0.95436</td></tr><tr><td>input to main1 std</td><td>0.06301</td></tr><tr><td>main1 spikes</td><td>1345</td></tr><tr><td>main1 to output convergence</td><td>0.46937</td></tr><tr><td>main1 to output std</td><td>0.29695</td></tr><tr><td>output spikes</td><td>20</td></tr><tr><td>training accuracy</td><td>31.9125</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">earthy-universe-51</strong>: <a href=\"https://wandb.ai/singularbrain/biolcnet/runs/2ss5fnl6\" target=\"_blank\">https://wandb.ai/singularbrain/biolcnet/runs/2ss5fnl6</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220116_111736-2ss5fnl6/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:2ss5fnl6). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/singularbrain/biolcnet/runs/127m63pv\" target=\"_blank\">rosy-sponge-52</a></strong> to <a href=\"https://wandb.ai/singularbrain/biolcnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "tensor(0.5014) tensor(0.2898)\n",
            "Previous model not found! Training from the beginning...\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c694566c97f410cb143f338f7b488e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/60000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-00508feee02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BioLCNet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'singularbrain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLCNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamicDopamineInjection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_active\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWANDB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_hparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-9712e07a1dac>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataloader, val_loader, reward_hparams, label, hparams, online_validate, n_train, n_test, n_val, val_interval, running_window_length, verbose)\u001b[0m\n\u001b[1;32m    472\u001b[0m                         \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                         \u001b[0mdopaminergic_layers\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdopaminergic_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                         \u001b[0mclamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                         )\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/network.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, time, one_step, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                         self.connections[c].update(\n\u001b[0;32m--> 459\u001b[0;31m                             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m                             )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/topology.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mCompute\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/topology.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlearning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_rule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/learning/learning.py\u001b[0m in \u001b[0;36m_connection_update\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# Compute weight update based on the eligibility value of the past timestep.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meligibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rewarding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (15) must match the size of tensor b (16) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "manual_seed(SEED)\n",
        "hparams = {**reward_hparams, **network_hparams, **train_hparams, **data_hparams}\n",
        "\n",
        "if WANDB:\n",
        "    wandb.init(project='BioLCNet', entity='singularbrain', config=network_hparams)\n",
        "net = LCNet(**hparams, reward_fn = DynamicDopamineInjection, wandb_active = WANDB)\n",
        "net.fit(dataloader = dataloader, val_loader = val_loader, reward_hparams = reward_hparams, **train_hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvfSASGVJ-xH"
      },
      "outputs": [],
      "source": [
        "plot_locally_connected_weights(net.connections[('input', 'main1')].w, 25, 5, 8, net.connections[('input', 'main1')].locations, 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLZrN09RaRwd"
      },
      "outputs": [],
      "source": [
        "plot_LC_weights(net.connections[('input', 'main1')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfTcVmMNCSUo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cwx3uWyIJ9D"
      },
      "outputs": [],
      "source": [
        "spikes_ = plot_LC_timepoint_spikes(net.sample_spikes['main1'][0], 40, 25, 1, 0, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihJVM6yUa_Mc"
      },
      "outputs": [],
      "source": [
        "print(spikes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8U2YG9uTp4X"
      },
      "outputs": [],
      "source": [
        "# sum(net.sample_spikes['main1'][0] == True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyiGB9SZFkP8"
      },
      "outputs": [],
      "source": [
        "net.sample_spikes['main1'][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kesM_O-9StW"
      },
      "outputs": [],
      "source": [
        "net.spikes['main1'].get('s').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfE77ZYtGTED"
      },
      "outputs": [],
      "source": [
        "net.connections[('input', 'main1')].w.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sku9TIEiNavq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXSLZZoGS4z"
      },
      "source": [
        "# Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DIAdG1mGS4z"
      },
      "outputs": [],
      "source": [
        "def plot_LC_timepoint_spikes(spikes: torch.Tensor,\n",
        "    timepoint: int,\n",
        "    n_filters: int,\n",
        "    in_chans: int,\n",
        "    slice_to_plot: int,\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    im: Optional[AxesImage] = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (10, 10),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(n_filters)))\n",
        "    sel_slice = spikes[timepoint].view(in_chans, n_filters, conv_size, conv_size).cpu()\n",
        "    sel_slice = sel_slice[slice_to_plot, ...].view(n_filters, conv_size, conv_size)\n",
        "    spikes_ = np.zeros((n_sqrt*conv_size, n_sqrt*conv_size))\n",
        "    filt_counter = 0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for n1 in range(n_sqrt):\n",
        "        for n2 in range(n_sqrt):\n",
        "            filter_ = sel_slice[filt_counter, :, :].view(conv_size, conv_size)\n",
        "            spikes_[n1 * conv_size : (n1 + 1) * conv_size, n2 * conv_size : (n2 + 1) * conv_size] = filter_\n",
        "            filt_counter += 1\n",
        "            ax.axhline((n1 + 1) * conv_size, color=\"g\", linestyle=\"-\")\n",
        "            ax.axvline((n2 + 1) * conv_size, color=\"g\", linestyle=\"--\")\n",
        "    ax.imshow(spikes_, cmap='Greys')\n",
        "    return spikes_\n",
        "    \n",
        "def plot_FC_response_map(lc: object,\n",
        "    fc: object,\n",
        "    ind_neuron_in_group: int,\n",
        "    label: int,\n",
        "    n_per_class: int,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param fc: FC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input\n",
        "    :param scale_factor: determines intensity of activation map  \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\t\n",
        "    ind_neuron = label * n_per_class + ind_neuron_in_group\n",
        "    w = fc.w[:,ind_neuron].view(reshaped.shape[0]//lc.kernel_size[0],reshaped.shape[1]//lc.kernel_size[1])\n",
        "    w = w.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*w, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_activation_map(lc : object,\n",
        "    spikes: torch.tensor,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot an activation map of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param scale_factor: determines intensity of activation map \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "    spikes = spikes.sum(0).squeeze().view(lc.conv_size[0]*int(np.sqrt(lc.out_channels)),lc.conv_size[1]*int(np.sqrt(lc.out_channels)))\n",
        "    x = scale_factor * spikes / torch.max(spikes)\n",
        "    x = x.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*x, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def reshape_LC_weights(\n",
        "    w: torch.Tensor,\n",
        "    n_filters: int,\n",
        "    kernel_size: Union[int, Tuple[int, int]],\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    input_sqrt: Union[int, Tuple[int, int]],\n",
        ") -> torch.Tensor:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Get the weights from a locally connected layer and reshape them to be two-dimensional and square.\n",
        "    :param w: Weights from a locally connected layer.\n",
        "    :param n_filters: No. of neuron filters.\n",
        "    :param kernel_size: Side length(s) of convolutional kernel.\n",
        "    :param conv_size: Side length(s) of convolution population.\n",
        "    :param input_sqrt: Sides length(s) of input neurons.\n",
        "    :return: Locally connected weights reshaped as a collection of spatially ordered square grids.\n",
        "    \"\"\"\n",
        "    k1, k2 = kernel_size\n",
        "    c1, c2 = conv_size\n",
        "    i1, i2 = input_sqrt\n",
        "    c1sqrt, c2sqrt = int(math.ceil(math.sqrt(c1))), int(math.ceil(math.sqrt(c2)))\n",
        "    fs = int(math.ceil(math.sqrt(n_filters)))\n",
        "\n",
        "    w_ = torch.zeros((n_filters * k1, k2 * c1 * c2))\n",
        "\n",
        "    for n1 in range(c1):\n",
        "        for n2 in range(c2):\n",
        "            for feature in range(n_filters):\n",
        "                n = n1 * c2 + n2\n",
        "                filter_ = w[feature, n1, n2, :, :\n",
        "                ].view(k1, k2)\n",
        "                w_[feature * k1 : (feature + 1) * k1, n * k2 : (n + 1) * k2] = filter_\n",
        "\n",
        "    if c1 == 1 and c2 == 1:\n",
        "        square = torch.zeros((i1 * fs, i2 * fs))\n",
        "\n",
        "        for n in range(n_filters):\n",
        "            square[\n",
        "                (n // fs) * i1 : ((n // fs) + 1) * i2,\n",
        "                (n % fs) * i2 : ((n % fs) + 1) * i2,\n",
        "            ] = w_[n * i1 : (n + 1) * i2]\n",
        "\n",
        "        return square\n",
        "    else:\n",
        "        square = torch.zeros((k1 * fs * c1, k2 * fs * c2))\n",
        "\n",
        "        for n1 in range(c1):\n",
        "            for n2 in range(c2):\n",
        "                for f1 in range(fs):\n",
        "                    for f2 in range(fs):\n",
        "                        if f1 * fs + f2 < n_filters:\n",
        "                            square[\n",
        "                                k1 * (n1 * fs + f1) : k1 * (n1 * fs + f1 + 1),\n",
        "                                k2 * (n2 * fs + f2) : k2 * (n2 * fs + f2 + 1),\n",
        "                            ] = w_[\n",
        "                                (f1 * fs + f2) * k1 : (f1 * fs + f2 + 1) * k1,\n",
        "                                (n1 * c2 + n2) * k2 : (n1 * c2 + n2 + 1) * k2,\n",
        "                            ]\n",
        "\n",
        "        return square\n",
        "\n",
        "def plot_semantic_pooling(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKBau_f5NoWY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz4jl4_2jso-"
      },
      "outputs": [],
      "source": [
        "plot_locally_connected_weights_meh3(net.connections[('input','main1')].w,net.connections[(net.final_connection_source_name,'output')].w,\\\n",
        "                                    0,5,net.neuron_per_class,net.n_channels1,net.in_channels,0,net.crop_size,net.filter_size1,net.layers['main1'].shape[1]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkz7skWaiWj7"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAKGbShj7kOM"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXv13w317oW7"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir '/content/runs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGe49P4-q9zp"
      },
      "source": [
        "## Save/Load Sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ow_r8C5qzwu"
      },
      "source": [
        "Save tensorBoard Session "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdHDClhIqUlW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -a /content/runs/. /content/drive/MyDrive/LCNet/logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NOgZCamq4p-"
      },
      "source": [
        "Read Saved Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlItthjkq4S5"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/LCNet/logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZtdOQm4oAu1"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLDxcrwithj"
      },
      "source": [
        "install and import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGik9_MeQfaI"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsM-Y9CZiv15"
      },
      "source": [
        "Define objective function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj_n_rwWDrdV"
      },
      "outputs": [],
      "source": [
        "STUDY_NAME  = ''\n",
        "DATA_PATH = ''\n",
        "N_TRIALS = ''\n",
        "\n",
        "def objective(trial):\n",
        "    ### Suggest parameters: \n",
        "    num_layers = trial.suggest_int('Number of Layers', 1, 4)\n",
        "    dropout_rate  = trial.suggest_float('Dropout', 0, .99)\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'selu', 'sigmoid', 'elu'])\n",
        "    lr = trial.suggest_float('Learning rate', 1e-6, 1)\n",
        "    network_hparams.update({\n",
        "        \n",
        "    })\n",
        "    ###Define your model\n",
        "    manual_seed(SEED)\n",
        "    hparams = {**reward_hparams, **network_hparams, **train_hparams, **data_hparams}\n",
        "    net = LCNet(**hparams, reward_fn = DynamicDopamineInjection)\n",
        "    va_acc = net.fit(dataloader = dataloader, val_loader = val_loader, reward_hparams = reward_hparams, **train_hparams)\n",
        "    ### Define objective value\n",
        "    objective_value = min(va_acc)\n",
        "\n",
        "    \n",
        "    return objective_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywzhxB84jthL"
      },
      "source": [
        "Run the study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDFttXEkjv4f"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(study_name = STUDY_NAME , storage=f\"sqlite:////content/drive/MyDrive/LCNet/optuna/optuna_study.db\", load_if_exists=True)\n",
        "study.optimize(objective, n_trials=N_TRIALS )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azp25HveO77U"
      },
      "outputs": [],
      "source": [
        "study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epsdhrYujy4f"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYgaOQZBaPJk"
      },
      "outputs": [],
      "source": [
        "plot_parallel_coordinate(study, params=[\"Learning rate\", \"Number of Layers\", \"Dropout\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya3ZmsSUqtd3"
      },
      "source": [
        "## Izhikevich 2007 - Task 2 | Pavlovian Conditioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw9OfurBUytc"
      },
      "outputs": [],
      "source": [
        "train_hparams = {\n",
        "    'n_train' : 4000,\n",
        "    'n_test' : 500,\n",
        "    'n_val' : 50,\n",
        "    'val_interval' : 250,\n",
        "    'running_window_length': 250,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK13AFE7Uytc"
      },
      "outputs": [],
      "source": [
        "# Dataset Hyperparameters\n",
        "target_classes = (8,9)\n",
        "if target_classes:\n",
        "    #npz_file = np.load(f'bindsnet/mask_{\"_\".join([str(i) for i in target_classes])}.npz')\n",
        "    npz_file = np.load('bindsnet/mask_5.npz') ##### KESAFAT KARI !!!\n",
        "    mask, mask_test = torch.from_numpy(npz_file['arr_0']), torch.from_numpy(npz_file['arr_1'])\n",
        "    n_classes = len(target_classes)\n",
        "    \n",
        "else:\n",
        "    mask = None\n",
        "    mask_test = None\n",
        "    n_classes = 10\n",
        "\n",
        "data_hparams = { \n",
        "    'intensity': 128,\n",
        "    'crop_size': 22,\n",
        "    'round_input': False,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqj7BASfUytd"
      },
      "outputs": [],
      "source": [
        "n_neurons = 2 #100\n",
        "neuron_per_class = int(n_neurons/n_classes)\n",
        "single_output_layer = True\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': 22,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': 25,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': 13,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': 3,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_classes': n_classes,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': False,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': 0,\n",
        "    'decision_period': 256,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': None,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDP,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.00,0.0),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output':0.01,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': False,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.05,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': 'between_layers',\n",
        "    'inh_factor': 1,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': 0.25*13*13,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': None,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': None,#'/content/drive/My Drive/LCNet/LCNet_phase2_baseline2_gpu.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/LCNet_phase2_baseline2_gpu.pth',\n",
        "    'LC_weights_path': '/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f13_s3_inh100_norm3.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': False,\n",
        "    'out_weights_vis': True,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': True,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "reward_hparams= {\n",
        "    'n_labels': n_classes,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 2.5,\n",
        "    'punishment_base': 2.5,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0001,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2iFc_v_Uytd"
      },
      "outputs": [],
      "source": [
        "time = 356 #320\n",
        "network_hparams.update(\n",
        "    {\n",
        "     'time': time,\n",
        "    }\n",
        ")\n",
        "dataloader, val_loader = load_datasets(network_hparams, data_hparams, mask, mask_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGsvklk7U7_r"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaBPo8BJU7_s"
      },
      "outputs": [],
      "source": [
        "manual_seed(SEED)\n",
        "hparams = {**reward_hparams, **network_hparams, **train_hparams, **data_hparams}\n",
        "net = LCNet(**hparams, reward_fn = DynamicDopamineInjection)\n",
        "net.fit(dataloader = dataloader, label = torch.tensor([1]),val_loader = val_loader, reward_hparams = reward_hparams, **train_hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy7-iAETvuPH"
      },
      "outputs": [],
      "source": [
        "net.fit(dataloader = dataloader, val_loader = val_loader, reward_hparams = reward_hparams, hparams = hparams, **train_hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrbB8W8g7JbQ"
      },
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "  net.one_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkmj9tDazbm3"
      },
      "source": [
        "## Plotting the feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSdD8hPAzm5P"
      },
      "outputs": [],
      "source": [
        "n_filts = (int((network_hparams['crop_size']-network_hparams['filter_size'])/network_hparams['stride'])+1)\n",
        "n_filts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYAtejQAzoku"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(net.connections[('input', 'main')].w.shape)\n",
        "\n",
        "in_chans = 2\n",
        "n_chans = 100\n",
        "k = 12\n",
        "\n",
        "reshaped_w = net.connections[('input', 'main')].w.view(in_chans, n_chans, n_filts, n_filts, k, k)\n",
        "\n",
        "print(reshaped_w.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut7wA1Emzq4J"
      },
      "outputs": [],
      "source": [
        "chan_idx = 20 # 0 to N_channels \n",
        "fig, axs = plt.subplots(n_filts, n_filts)\n",
        "for i in range(n_filts):\n",
        "    for j in range(n_filts):\n",
        "        axs[i][j].imshow(reshaped_w[0,chan_idx,i,j].view(k, k).cpu(), cmap='Greys')\n",
        "        axs[i][j].axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QZtdOQm4oAu1",
        "ya3ZmsSUqtd3"
      ],
      "machine_shape": "hm",
      "name": "BioLCNet_XOR_MNIST.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "87ae7d1e75b14a98f2d7b99b6b39b40721989d38e6517f9dbec64ca4d8e3011b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12b937dca3ba49bd9588436ea3f149b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17e248d5c9a34014bcab95b9e7a968a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b36b5b22fdd4cf290cd2d5373548e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3394bed82e554ef39e392d74fd6b838f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7e28a2eac4b4ea1a3a5b216dbcba8bf",
            "placeholder": "​",
            "style": "IPY_MODEL_12b937dca3ba49bd9588436ea3f149b1",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
          }
        },
        "3963942cc0d24dc6ac2b8664d5fcef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3394bed82e554ef39e392d74fd6b838f",
              "IPY_MODEL_df8c3560d72e4bfca0a45684dc7ca0e5"
            ],
            "layout": "IPY_MODEL_ecd12401662a4d3a90ebcfc0d5c6c915"
          }
        },
        "491f7bb440cb4b9b8e7cd71e035cd0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b36b5b22fdd4cf290cd2d5373548e6a",
            "placeholder": "​",
            "style": "IPY_MODEL_9eb25a82fa064b6284b9d40f34435465",
            "value": " 18608/60000 [09:58&lt;14:10, 48.65it/s]"
          }
        },
        "6c694566c97f410cb143f338f7b488e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b08d3b5b13842079cb24a92cbaa1bba",
              "IPY_MODEL_f4b020cb6b6e4282837a8703d65f3aa7",
              "IPY_MODEL_491f7bb440cb4b9b8e7cd71e035cd0ee"
            ],
            "layout": "IPY_MODEL_c3fb6433903c40dd871cca2247a807e6"
          }
        },
        "7c02649744224b1e9884beb9ce7ffc1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd3eb20697f4693bb4f6b85ac2364a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97bb9bba45c44d789d827f797066f0f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b08d3b5b13842079cb24a92cbaa1bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c02649744224b1e9884beb9ce7ffc1f",
            "placeholder": "​",
            "style": "IPY_MODEL_17e248d5c9a34014bcab95b9e7a968a0",
            "value": "Epoch: 0, Running accuracy: 11.39%, Current val accuracy: 0.00%, :  31%"
          }
        },
        "9eb25a82fa064b6284b9d40f34435465": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3fb6433903c40dd871cca2247a807e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e28a2eac4b4ea1a3a5b216dbcba8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df8c3560d72e4bfca0a45684dc7ca0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97bb9bba45c44d789d827f797066f0f8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cd3eb20697f4693bb4f6b85ac2364a5",
            "value": 1
          }
        },
        "ecd12401662a4d3a90ebcfc0d5c6c915": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f16501f36d0e4866b041d376b0d891b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4b020cb6b6e4282837a8703d65f3aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb8a04e3bac04e6a9b1d365ba2bb7ebf",
            "max": 60000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f16501f36d0e4866b041d376b0d891b0",
            "value": 18608
          }
        },
        "fb8a04e3bac04e6a9b1d365ba2bb7ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31ef119669a740b3a7dca675d0844952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60fdbad35f1f488398e3db70662c04fa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_64f26fa7724949b4b86c192a54e025e4",
              "IPY_MODEL_6d787bc3c0474fd882b68c4218d047dc",
              "IPY_MODEL_38d27b0ba8f54d04b9cab41ae8e8286f"
            ]
          }
        },
        "60fdbad35f1f488398e3db70662c04fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64f26fa7724949b4b86c192a54e025e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_038a0cd0737c46f490bbce49cdb08f2e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch: 0, Running accuracy: 0.00%, Current val accuracy: 100.00%, :  11%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c984a41973a463fb75757e12e2ca0da"
          }
        },
        "6d787bc3c0474fd882b68c4218d047dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_762f117169034b648c91e6ac742b3169",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40f94965b86f4350a912656f97026b5f"
          }
        },
        "38d27b0ba8f54d04b9cab41ae8e8286f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ee79153562d145308350859c44dc78ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213/2000 [03:42&lt;31:08,  1.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_228a4e5d5c3749e2934a737684e4e425"
          }
        },
        "038a0cd0737c46f490bbce49cdb08f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c984a41973a463fb75757e12e2ca0da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "762f117169034b648c91e6ac742b3169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40f94965b86f4350a912656f97026b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee79153562d145308350859c44dc78ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "228a4e5d5c3749e2934a737684e4e425": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}