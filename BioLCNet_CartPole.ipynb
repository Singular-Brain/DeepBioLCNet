{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/DeepBioLCNet/blob/main/BioLCNet_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTSvrK3T_GA"
      },
      "source": [
        "#Notebook setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lXtgP_iEPE0G"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/Singular-Brain/DeepBioLCNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KXcXvvsXcOlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03d26af-0b0f-407c-d38e-9f00c9c883a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepBioLCNet' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "0QFC6xL2uAaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a69f51-0d19-43f9-a71e-5b534add0e02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f9f07590f50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K4l3AVRbGS4Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.image import AxesImage\n",
        "from torch.nn.modules.utils import _pair\n",
        "from matplotlib.collections import PathCollection\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from typing import Tuple, List, Optional, Sized, Dict, Union\n",
        "import math\n",
        "import random\n",
        "from torchvision.transforms.functional import crop\n",
        "# from ..utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights\n",
        "\n",
        "import gym\n",
        "import tkinter\n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BFGNAecpT-Lj"
      },
      "outputs": [],
      "source": [
        "from bindsnet.network.nodes import Nodes\n",
        "import os\n",
        "### import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, Tuple, Optional, Sequence\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "from bindsnet.datasets import MNIST\n",
        "from bindsnet.encoding import PoissonEncoder\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes, IFNodes\n",
        "from bindsnet.network.topology import Connection, MaxPool2dLocalConnection\n",
        "from bindsnet.network.topology import LocalConnection, LocalConnectionOrig\n",
        "from bindsnet.network.monitors import Monitor, AbstractMonitor, TensorBoardMonitor\n",
        "from bindsnet.learning import PostPre, MSTDP, MSTDPET, WeightDependentPostPre, Hebbian\n",
        "from bindsnet.learning.reward import DynamicDopamineInjection, DopaminergicRPE, AbstractReward\n",
        "from bindsnet.analysis.plotting import plot_locally_connected_weights,plot_locally_connected_weights_meh,plot_spikes,\\\n",
        "plot_LC_timepoint_spikes,plot_locally_connected_weights_meh2,plot_convergence_and_histogram,plot_locally_connected_weights_meh3\n",
        "from bindsnet.analysis.visualization import plot_weights_movie, plot_spike_trains_for_example,summary, plot_voltage\n",
        "from bindsnet.utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RLTasks(AbstractReward):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Computes the reward for a given RL task in the current state\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_id,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if env_id == \"CartPole-v0\":\n",
        "            self.compute = self._cartPole_compute\n",
        "        elif env_id == \"MountainCar-v0-v0\":\n",
        "            self.compute = self._mountainCar_compute\n",
        "        elif env_id == \"BreakoutDeterministic-v4\":\n",
        "            self.compute = self._breakout_compute\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"This rl environment is not currently supported.\"\n",
        "            )\n",
        "    \n",
        "    def _cartPole_compute(self, **kwargs):\n",
        "        success = kwargs['success']\n",
        "        failure = kwargs['failure']\n",
        "        if kwargs['classic_reward']:\n",
        "            if failure:\n",
        "                return torch.tensor([0.])\n",
        "            else:\n",
        "                return torch.tensor([1.])\n",
        "\n",
        "        env = kwargs['env']\n",
        "        state = env.state\n",
        "        success = kwargs['success']\n",
        "        failure = kwargs['failure']\n",
        "        x, x_dot, theta, theta_dot = state\n",
        "        # r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "        # r2 = ((env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5)#*2\n",
        "        #reward = r1 + r2\n",
        "        reward = ((env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians) - 0.5*abs(theta_dot)\n",
        "        if success:\n",
        "            reward += 5\n",
        "        elif failure:\n",
        "            reward -= 5\n",
        "        reward = torch.tensor([reward])\n",
        "        return reward\n",
        "\n",
        "    def _mountainCar_compute(self):\n",
        "        pass\n",
        "\n",
        "    def _breakout_compute(self):\n",
        "        pass\n",
        "        \n",
        "    def update(self):\n",
        "        pass\n",
        "\n",
        "    def online_compute(self,):\n",
        "        pass"
      ],
      "metadata": {
        "id": "WqFD8yro6WCb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGGHW43UksI"
      },
      "source": [
        "## Sets up Gpu use and manual seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LiUmFrpcUfmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3191d753-9e9f-4b46-8693-84785a85ab8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Device =  cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device =  torch.device(\"cuda\")\n",
        "    gpu = True\n",
        "else:\n",
        "    device =  torch.device(\"cpu\")\n",
        "    gpu = False\n",
        "\n",
        "def manual_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SEED = 2045 # The Singularity is Near!\n",
        "manual_seed(SEED)\n",
        "WANDB = False\n",
        "\n",
        "torch.set_num_threads(os.cpu_count() - 1)\n",
        "print(\"Running on Device = \", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKedMpIleMr"
      },
      "source": [
        "# Custom Monitors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqpsr2a1WV"
      },
      "source": [
        "## Reward Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M44GJ65GleMs"
      },
      "outputs": [],
      "source": [
        "class RewardMonitor(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: None,\n",
        "        batch_size: int = 1,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.time = time\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # if time is not specified the monitor variable accumulate the logs\n",
        "        if self.time is None:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        self.recording = []\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if \"reward\" in kwargs:\n",
        "            self.recording.append(kwargs[\"reward\"])\n",
        "        # remove the oldest element (first in the list)\n",
        "        # if self.time is not None:\n",
        "        #     self.recording.pop(0)\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clxN_npa1WY"
      },
      "source": [
        "## Plot Eligibility trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SshGlRwpa1WZ"
      },
      "outputs": [],
      "source": [
        "class PlotET(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        i,\n",
        "        j,\n",
        "        source,\n",
        "        target,\n",
        "        connection,\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.connection = connection\n",
        "\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if hasattr(self.connection.update_rule, 'p_plus'):\n",
        "            self.recording['spikes_i'].append(self.source.s.ravel()[self.i].item())\n",
        "            self.recording['spikes_j'].append(self.target.s.ravel()[self.j].item())\n",
        "            self.recording['p_plus'].append(self.connection.update_rule.p_plus[self.i].item())\n",
        "            self.recording['p_minus'].append(self.connection.update_rule.p_minus[self.j].item())\n",
        "            self.recording['eligibility'].append(self.connection.update_rule.eligibility[self.i,self.j].item())\n",
        "            self.recording['eligibility_trace'].append(self.connection.update_rule.eligibility_trace[self.i,self.j].item())\n",
        "            self.recording['w'].append(self.connection.w[self.i,self.j].item())\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        fig, axs  = plt.subplots(7)\n",
        "        fig.set_size_inches(10, 20)\n",
        "        for i, (name, p) in enumerate(self.recording.items()):\n",
        "            axs[i].plot(p[-250:])\n",
        "            axs[i].set_title(name)\n",
        "    \n",
        "        fig.show()\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = {\n",
        "        'spikes_i': [],\n",
        "        'spikes_j': [],\n",
        "        'p_plus':[],\n",
        "        'p_minus':[],\n",
        "        'eligibility':[],\n",
        "        'eligibility_trace':[],\n",
        "        'w': [],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YGE1XjvIkZ"
      },
      "source": [
        "## Kernel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-4hp2V46vOUv"
      },
      "outputs": [],
      "source": [
        "class AbstractKernel(ABC):\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Base class for generating image filter kernels such as Gabor, DoG, etc. Each subclass should override :attr:`__call__` function.\n",
        "        Instantiates a ``Filter Kernel`` object.\n",
        "        :param window_size : The size of the kernel (int)\n",
        "        \"\"\"\n",
        "        self.window_size = kernel_size\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PL2L6_ABwBH4"
      },
      "outputs": [],
      "source": [
        "class DoGKernel(AbstractKernel):\n",
        "    def __init__(self, kernel_size: Union[int, Tuple[int, int]], sigma1 : float, sigma2 : float):\n",
        "        \"\"\"\n",
        "        Generates DoG filter kernels.\n",
        "        :param kernel_size: Horizontal and vertical size of DOG kernels.(If pass int, we consider it as a square filter) \n",
        "        :param sigma1 : The sigma parameter for the first Gaussian function.\n",
        "        :param sigma2 : The sigma parameter for the second Gaussian function.\n",
        "        \"\"\"\n",
        "        super(DoGKernel, self).__init__(kernel_size)\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        \n",
        "    def __call__(self):\n",
        "        k = self.window_size//2\n",
        "        x, y = np.mgrid[-k:k+1:1, -k:k+1:1]\n",
        "        a = 1.0 / (2 * math.pi)\n",
        "        prod = x*x + y*y\n",
        "        f1 = (1/(self.sigma1*self.sigma1)) * np.exp(-0.5 * (1/(self.sigma1*self.sigma1)) * (prod))\n",
        "        f2 = (1/(self.sigma2*self.sigma2)) * np.exp(-0.5 * (1/(self.sigma2*self.sigma2)) * (prod))\n",
        "        dog = a * (f1-f2)\n",
        "        dog_mean = np.mean(dog)\n",
        "        dog = dog - dog_mean\n",
        "        dog_max = np.max(dog)\n",
        "        dog = dog / dog_max\n",
        "        dog_tensor = torch.from_numpy(dog)\n",
        "        # returns a 2d tensor corresponding to the requested DoG filter\n",
        "        return dog_tensor.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zBUT0IUZDXxW"
      },
      "outputs": [],
      "source": [
        "class Filter():\n",
        "    \"\"\"\n",
        "    Applies a filter transform. Each filter contains a sequence of :attr:`FilterKernel` objects.\n",
        "    The result of each filter kernel will be passed through a given threshold (if not :attr:`None`).\n",
        "    Args:\n",
        "        filter_kernels (sequence of FilterKernels): The sequence of filter kernels.\n",
        "        padding (int, optional): The size of the padding for the convolution of filter kernels. Default: 0\n",
        "        thresholds (sequence of floats, optional): The threshold for each filter kernel. Default: None\n",
        "        use_abs (boolean, optional): To compute the absolute value of the outputs or not. Default: False\n",
        "    .. note::\n",
        "        The size of the compund filter kernel tensor (stack of individual filter kernels) will be equal to the \n",
        "        greatest window size among kernels. All other smaller kernels will be zero-padded with an appropriate \n",
        "        amount.\n",
        "    \"\"\"\n",
        "    # filter_kernels must be a list of filter kernels\n",
        "    # thresholds must be a list of thresholds for each kernel\n",
        "    def __init__(self, filter_kernels, padding=0, thresholds=None, use_abs=False):\n",
        "        tensor_list = []\n",
        "        self.max_window_size = 0\n",
        "        for kernel in filter_kernels:\n",
        "            if isinstance(kernel, torch.Tensor):\n",
        "                tensor_list.append(kernel)\n",
        "                self.max_window_size = max(self.max_window_size, kernel.size(-1))\n",
        "            else:\n",
        "                tensor_list.append(kernel().unsqueeze(0))\n",
        "                self.max_window_size = max(self.max_window_size, kernel.window_size)\n",
        "        for i in range(len(tensor_list)):\n",
        "            p = (self.max_window_size - filter_kernels[i].window_size)//2\n",
        "            tensor_list[i] = fn.pad(tensor_list[i], (p,p,p,p))\n",
        "\n",
        "        self.kernels = torch.stack(tensor_list)\n",
        "        self.number_of_kernels = len(filter_kernels)\n",
        "        self.padding = padding\n",
        "        if isinstance(thresholds, list):\n",
        "            self.thresholds = thresholds.clone().detach()\n",
        "            self.thresholds.unsqueeze_(0).unsqueeze_(2).unsqueeze_(3)\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "        self.use_abs = use_abs\n",
        "\n",
        "    # returns a 4d tensor containing the flitered versions of the input image\n",
        "    # input is a 4d tensor. dim: (minibatch=1, filter_kernels, height, width)\n",
        "    def __call__(self, input):\n",
        "\n",
        "        # if input.dim() == 3:\n",
        "        #     input2 = torch.unsqueeze(input, 0)\n",
        "        input.unsqueeze_(0)\n",
        "        output = fn.conv2d(input, self.kernels, padding = self.padding).float()\n",
        "        if not(self.thresholds is None):\n",
        "            output = torch.where(output < self.thresholds, torch.tensor(0.0, device=output.device), output)\n",
        "        if self.use_abs:\n",
        "            torch.abs_(output)\n",
        "        return output.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXSLZZoGS4z"
      },
      "source": [
        "# Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3DIAdG1mGS4z"
      },
      "outputs": [],
      "source": [
        "def plot_LC_timepoint_spikes(spikes: torch.Tensor,\n",
        "    timepoint: int,\n",
        "    n_filters: int,\n",
        "    in_chans: int,\n",
        "    slice_to_plot: int,\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    im: Optional[AxesImage] = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (10, 10),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(n_filters)))\n",
        "    sel_slice = spikes[timepoint].view(in_chans, n_filters, conv_size, conv_size).cpu()\n",
        "    sel_slice = sel_slice[slice_to_plot, ...].view(n_filters, conv_size, conv_size)\n",
        "    spikes_ = np.zeros((n_sqrt*conv_size, n_sqrt*conv_size))\n",
        "    filt_counter = 0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for n1 in range(n_sqrt):\n",
        "        for n2 in range(n_sqrt):\n",
        "            filter_ = sel_slice[filt_counter, :, :].view(conv_size, conv_size)\n",
        "            spikes_[n1 * conv_size : (n1 + 1) * conv_size, n2 * conv_size : (n2 + 1) * conv_size] = filter_\n",
        "            filt_counter += 1\n",
        "            ax.axhline((n1 + 1) * conv_size, color=\"g\", linestyle=\"-\")\n",
        "            ax.axvline((n2 + 1) * conv_size, color=\"g\", linestyle=\"--\")\n",
        "    ax.imshow(spikes_, cmap='Greys')\n",
        "    return spikes_\n",
        "    \n",
        "def plot_FC_response_map(lc: object,\n",
        "    fc: object,\n",
        "    ind_neuron_in_group: int,\n",
        "    label: int,\n",
        "    n_per_action: int,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param fc: FC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input\n",
        "    :param scale_factor: determines intensity of activation map  \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\t\n",
        "    ind_neuron = label * n_per_action + ind_neuron_in_group\n",
        "    w = fc.w[:,ind_neuron].view(reshaped.shape[0]//lc.kernel_size[0],reshaped.shape[1]//lc.kernel_size[1])\n",
        "    w = w.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*w, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_activation_map(lc : object,\n",
        "    spikes: torch.tensor,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot an activation map of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param scale_factor: determines intensity of activation map \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "    spikes = spikes.sum(0).squeeze().view(lc.conv_size[0]*int(np.sqrt(lc.out_channels)),lc.conv_size[1]*int(np.sqrt(lc.out_channels)))\n",
        "    x = scale_factor * spikes / torch.max(spikes)\n",
        "    x = x.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*x, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def reshape_LC_weights(\n",
        "    w: torch.Tensor,\n",
        "    n_filters: int,\n",
        "    kernel_size: Union[int, Tuple[int, int]],\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    input_sqrt: Union[int, Tuple[int, int]],\n",
        ") -> torch.Tensor:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Get the weights from a locally connected layer and reshape them to be two-dimensional and square.\n",
        "    :param w: Weights from a locally connected layer.\n",
        "    :param n_filters: No. of neuron filters.\n",
        "    :param kernel_size: Side length(s) of convolutional kernel.\n",
        "    :param conv_size: Side length(s) of convolution population.\n",
        "    :param input_sqrt: Sides length(s) of input neurons.\n",
        "    :return: Locally connected weights reshaped as a collection of spatially ordered square grids.\n",
        "    \"\"\"\n",
        "    k1, k2 = kernel_size\n",
        "    c1, c2 = conv_size\n",
        "    i1, i2 = input_sqrt\n",
        "    c1sqrt, c2sqrt = int(math.ceil(math.sqrt(c1))), int(math.ceil(math.sqrt(c2)))\n",
        "    fs = int(math.ceil(math.sqrt(n_filters)))\n",
        "\n",
        "    w_ = torch.zeros((n_filters * k1, k2 * c1 * c2))\n",
        "\n",
        "    for n1 in range(c1):\n",
        "        for n2 in range(c2):\n",
        "            for feature in range(n_filters):\n",
        "                n = n1 * c2 + n2\n",
        "                filter_ = w[feature, n1, n2, :, :\n",
        "                ].view(k1, k2)\n",
        "                w_[feature * k1 : (feature + 1) * k1, n * k2 : (n + 1) * k2] = filter_\n",
        "\n",
        "    if c1 == 1 and c2 == 1:\n",
        "        square = torch.zeros((i1 * fs, i2 * fs))\n",
        "\n",
        "        for n in range(n_filters):\n",
        "            square[\n",
        "                (n // fs) * i1 : ((n // fs) + 1) * i2,\n",
        "                (n % fs) * i2 : ((n % fs) + 1) * i2,\n",
        "            ] = w_[n * i1 : (n + 1) * i2]\n",
        "\n",
        "        return square\n",
        "    else:\n",
        "        square = torch.zeros((k1 * fs * c1, k2 * fs * c2))\n",
        "\n",
        "        for n1 in range(c1):\n",
        "            for n2 in range(c2):\n",
        "                for f1 in range(fs):\n",
        "                    for f2 in range(fs):\n",
        "                        if f1 * fs + f2 < n_filters:\n",
        "                            square[\n",
        "                                k1 * (n1 * fs + f1) : k1 * (n1 * fs + f1 + 1),\n",
        "                                k2 * (n2 * fs + f2) : k2 * (n2 * fs + f2 + 1),\n",
        "                            ] = w_[\n",
        "                                (f1 * fs + f2) * k1 : (f1 * fs + f2 + 1) * k1,\n",
        "                                (n1 * c2 + n2) * k2 : (n1 * c2 + n2 + 1) * k2,\n",
        "                            ]\n",
        "\n",
        "        return square\n",
        "\n",
        "def plot_semantic_pooling(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_weights(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        #print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            lc.kernel_size[0],#n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt*lc.conv_size[0] * lc.kernel_size[0],#n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            lc.kernel_size[0],#,n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt*lc.conv_size[1] * lc.kernel_size[1],#n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color=color, linestyle=\"--\")\n",
        "            \n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywXyWP0I83Au"
      },
      "source": [
        "# Design network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PcU9FSsVi4Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853dca0b-1329-4c09-ecf0-b027a5f4c9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ],
      "source": [
        "WANDB = True\n",
        "if WANDB:\n",
        "    !pip install -q wandb\n",
        "    !wandb login\n",
        "    import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8bZpJmlrJDa9"
      },
      "outputs": [],
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "def convergence(c):\n",
        "    if c.norm is None:\n",
        "        return 1-torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2\n",
        "    else:\n",
        "        mean_norm_factor = c.norm / c.w.shape[-1]\n",
        "        return  1-(torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2)\n",
        "\n",
        "        \n",
        "class LCNet(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: int,\n",
        "        n_actions: int,\n",
        "        neuron_per_action: int,\n",
        "        in_channels : int,\n",
        "        n_channels1: int,\n",
        "        n_channels2: int,\n",
        "        filter_size1: int,\n",
        "        filter_size2: int,\n",
        "        stride1: int,\n",
        "        stride2: int,\n",
        "        maxPool1: bool,\n",
        "        maxPool2: bool,\n",
        "        online: bool,\n",
        "        deep: bool,\n",
        "        reward_fn,\n",
        "        n_neurons: int,\n",
        "        pre_observation: bool,\n",
        "        has_decision_period: bool,\n",
        "        local_rewarding: bool,\n",
        "        nu_LC: Union[float, Tuple[float, float]],\n",
        "        nu_LC2: Union[float, Tuple[float, float]],\n",
        "        nu_Output: float,\n",
        "        dt: float,\n",
        "        crop_size:int ,\n",
        "        nu_inh_LC: float,\n",
        "        nu_inh: float,\n",
        "        inh_type,\n",
        "        inh_LC: bool,\n",
        "        inh_LC2: bool,\n",
        "        inh_factor_LC: float,\n",
        "        inh_factor_LC2: float,\n",
        "        inh_factor:float,\n",
        "        single_output_layer:bool,\n",
        "        NodesType_LC,\n",
        "        NodesType_Output, \n",
        "        update_rule_LC,\n",
        "        update_rule_LC2,\n",
        "        update_rule_Output,\n",
        "        update_rule_inh,\n",
        "        update_rule_inh_LC,\n",
        "        wmin: float,\n",
        "        wmax: float ,\n",
        "        soft_bound,\n",
        "        theta_plus: float,\n",
        "        tc_theta_decay: float,\n",
        "        tc_trace:int,\n",
        "        normal_init:bool,\n",
        "        mu: float,\n",
        "        std:float,\n",
        "        norm_factor_fc,\n",
        "        norm_factor_inh_LC: bool,\n",
        "        norm_factor_LC,\n",
        "        norm_factor_LC2,\n",
        "        norm_factor_out,\n",
        "        norm_factor_inh,\n",
        "        trace_additive,\n",
        "        load_path,\n",
        "        save_path,\n",
        "        LC_weights_path,\n",
        "        LC2_weights_path,\n",
        "        confusion_matrix,\n",
        "        lc_weights_vis,\n",
        "        out_weights_vis,\n",
        "        lc_convergence_vis,\n",
        "        out_convergence_vis,\n",
        "        thresh_LC,\n",
        "        thresh_FC,\n",
        "        num_episodes,\n",
        "        max_steps,\n",
        "        NodeType_FC,\n",
        "        add_layer_fc,\n",
        "        num_neurons_in_fc,\n",
        "        nu_fc,\n",
        "        update_rule_fc,\n",
        "        inh_fc,\n",
        "        inh_factor_fc,\n",
        "        classic_reward,\n",
        "        wandb_active = False,\n",
        "        batch_size=1,\n",
        "        save_interval = 100,\n",
        "\n",
        "\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructor for class ``BioLCNet``.\n",
        "\n",
        "        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n",
        "        :param n_neurons: Number of excitatory, inhibitory neurons.\n",
        "        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n",
        "        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n",
        "        :param dt: Simulation time step.\n",
        "        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n",
        "            respectively.\n",
        "        :param reduction: Method for reducing parameter updates along the minibatch\n",
        "            dimension.\n",
        "        :param wmin: Minimum allowed weight on input to excitatory synapses.\n",
        "        :param wmax: Maximum allowed weight on input to excitatory synapses.\n",
        "        :param norm: Input to excitatory layer connection weights normalization\n",
        "            constant.\n",
        "        :param theta_plus: On-spike increment of ``(adaptive)LIFNodes`` membrane\n",
        "            threshold potential.\n",
        "        :param tc_theta_decay: Time constant of ``(adaptive)LIFNodes`` threshold\n",
        "            potential decay.\n",
        "        :param inpt_shape: The dimensionality of the input layer.\n",
        "        \"\"\"\n",
        "        manual_seed(SEED)\n",
        "        super().__init__(dt=dt, reward_fn = None, online=online)\n",
        "        kwargs['single_output_layer'] = single_output_layer\n",
        "        kwargs['dt'] = dt\n",
        "        kwargs['n_labels'] = n_actions\n",
        "        kwargs['neuron_per_actionn'] = neuron_per_action\n",
        "        self.true_label = 0\n",
        "        self.dt = dt\n",
        "        self.intensity = kwargs['intensity']\n",
        "        self.reward_fn = reward_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_fn.network = self\n",
        "        self.reward_fn.dt = self.dt\n",
        "        self.n_actions = n_actions\n",
        "        self.neuron_per_action = neuron_per_action\n",
        "        self.n_classes = n_actions\n",
        "        self.classic_reward = classic_reward\n",
        "        self.add_layer_fc = add_layer_fc\n",
        "        self.num_neurons_in_fc = num_neurons_in_fc\n",
        "        self.neuron_per_class = neuron_per_action\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "        self.save_interval = save_interval\n",
        "        self.deep = deep\n",
        "        self.maxPool1 = maxPool1\n",
        "        self.maxPool2 = maxPool2\n",
        "        self.time = time\n",
        "        self.crop_size = crop_size\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.clamp_intensity = kwargs.get('clamp_intensity',None)\n",
        "        self.single_output_layer = single_output_layer\n",
        "        self.pre_observation = pre_observation\n",
        "        self.has_decision_period = has_decision_period\n",
        "        self.local_rewarding = local_rewarding\n",
        "        self.soft_bound = soft_bound\n",
        "        self.confusion_matrix = confusion_matrix\n",
        "        self.lc_weights_vis = lc_weights_vis\n",
        "        self.out_weights_vis = out_weights_vis\n",
        "        self.lc_convergence_vis = lc_convergence_vis\n",
        "        self.out_convergence_vis = out_convergence_vis\n",
        "        self.frame_analysis = frame_analysis\n",
        "        self.in_channels = in_channels\n",
        "        self.n_channels1 = n_channels1\n",
        "        self.n_channels2 = n_channels2\n",
        "        self.stride1 = stride1\n",
        "        self.stride2 = stride2\n",
        "        self.convergences = {}\n",
        "        self.norm_factor_LC = norm_factor_LC\n",
        "        self.norm_factor_LC2 = norm_factor_LC2\n",
        "        self.norm_factor_out = norm_factor_out\n",
        "        self.wmin = wmin \n",
        "        self.wmax = wmax\n",
        "        self.wandb_active = wandb_active\n",
        "        self.epochs_trained = 0\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_steps= max_steps\n",
        "        self.rew = 0.0\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.env.reset()\n",
        "\n",
        "        self.time_analysis = kwargs.get('time_analysis', False)\n",
        "        if kwargs['variant'] == 'scalar':\n",
        "            assert self.has_decision_period == True, ''\n",
        "\n",
        "        if self.online == False:\n",
        "            assert self.has_decision_period == True, ''\n",
        "        \n",
        "        if self.has_decision_period == True:\n",
        "            assert self.online == False, \"Decision period is not compatible with online learning.\"\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.decision_period = kwargs['decision_period']\n",
        "            assert self.decision_period > 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period - self.decision_period\n",
        "\n",
        "        elif self.pre_observation == True:\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period\n",
        "            self.decision_period = self.time - self.observation_period\n",
        "\n",
        "        else:\n",
        "            self.observation_period = 0\n",
        "            self.decision_period = self.time\n",
        "            self.learning_period = self.time\n",
        "\n",
        "        ### nodes\n",
        "        inp = Input(shape= [in_channels,crop_size,crop_size], traces=True, tc_trace=tc_trace,traces_additive = trace_additive)\n",
        "        self.add_layer(inp, name=\"input\")\n",
        "\n",
        "        ## First hidden layer\n",
        "        conv_size1 = compute_size(crop_size, filter_size1, stride1)\n",
        "        main1 = NodesType_LC(shape= [n_channels1, conv_size1, conv_size1], thresh = thresh_LC, traces=True, tc_trace=tc_trace,\n",
        "                             traces_additive = trace_additive,tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "        \n",
        "        self.add_layer(main1, name=\"main1\")\n",
        "\n",
        "        ### connections \n",
        "        LC1 = LocalConnectionOrig(inp, main1, filter_size1, stride1, n_channels1,\\\n",
        "                              nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, norm = norm_factor_LC)\n",
        "\n",
        "        # LC1 = LocalConnection(inp, main1, filter_size1, stride1, in_channels, n_channels1,input_shape=(crop_size,crop_size),\\\n",
        "        #                      nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC)\n",
        "\n",
        "\n",
        "        if LC_weights_path:\n",
        "            a = torch.load(LC_weights_path)\n",
        "            LC1.w.data = a['state_dict']['input_to_main1.w']\n",
        "            print(\"Weights loaded ...\")\n",
        "        \n",
        "        elif normal_init:\n",
        "            w_lc_init = torch.normal(mu,std, size = (in_channels, n_channels1 * compute_size(crop_size, filter_size1, stride1)**2, filter_size1**2))\n",
        "            LC1.w.data = w_lc_init\n",
        "       \n",
        "        self.add_connection(LC1, \"input\", \"main1\")\n",
        "        self.convergences['lc1'] = []\n",
        "\n",
        "        if inh_LC:\n",
        "            main_width = compute_size(crop_size, filter_size1, stride1)\n",
        "            w_inh_LC = torch.zeros(n_channels1,main_width,main_width,n_channels1,main_width,main_width)\n",
        "            for c in range(n_channels1):\n",
        "                for w1 in range(main_width):\n",
        "                    for w2 in range(main_width):\n",
        "                        w_inh_LC[c,w1,w2,:,w1,w2] = - inh_factor_LC\n",
        "                        w_inh_LC[c,w1,w2,c,w1,w2] = 0\n",
        "        \n",
        "            w_inh_LC = w_inh_LC.reshape(main1.n,main1.n)\n",
        "                                                             \n",
        "            LC_recurrent_inhibition = Connection(\n",
        "                source=main1,\n",
        "                target=main1,\n",
        "                w=w_inh_LC,\n",
        "            )\n",
        "            self.add_connection(LC_recurrent_inhibition, \"main1\", \"main1\")\n",
        "        \n",
        "        self.final_connection_source_name = 'main1'\n",
        "        self.final_connection_source = main1\n",
        "        \n",
        "        if self.add_layer_fc:\n",
        "            FC1 = NodeType_FC(n= self.num_neurons_in_fc, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "            self.add_layer(FC1, \"fc1\")\n",
        "\n",
        "            main_fc = Connection(main1, FC1, nu = nu_fc, update_rule = update_rule_fc, wmin = wmin, wmax= wmax, norm = norm_factor_fc)\n",
        "            \n",
        "            self.add_connection(main_fc, \"main1\", \"fc1\")\n",
        "            self.final_connection_source_name = 'fc1'\n",
        "            self.final_connection_source = FC1\n",
        "\n",
        "            if inh_fc:\n",
        "                w = -inh_factor_fc * (torch.ones(FC1.n, FC1.n) - torch.eye(FC1.n, FC1.n))\n",
        "                fc_recurrent_inhibition = Connection(\n",
        "                    source=FC1,\n",
        "                    target=FC1,\n",
        "                    w=w,\n",
        "                    update_rule = None,\n",
        "                    wmin=-inh_factor_fc,\n",
        "                    wmax=0,\n",
        "                )\n",
        "                self.add_connection(fc_recurrent_inhibition, \"fc1\", \"fc1\")\n",
        "\n",
        "        self.hidden2 = main1\n",
        "        self.hidden2_name = 'main1'\n",
        "        if maxPool1:\n",
        "            maxPool_kernel = 2\n",
        "            maxPool_stride = 2\n",
        "            \n",
        "            conv_size1 =compute_size(conv_size1, maxPool_kernel, maxPool_stride)\n",
        "            self.final_connection_source_name = 'maxpool1'\n",
        "            \n",
        "            maxpool1 = LIFNodes(shape= [self.n_channels1, conv_size1, conv_size1], refrac = 0)\n",
        "            self.add_layer(maxpool1, name=\"maxpool1\")\n",
        "            self.final_connection_source = maxpool1\n",
        "            \n",
        "            maxPoolConnection = MaxPool2dLocalConnection(main1, maxpool1, maxPool_kernel, maxPool_stride)\n",
        "            self.add_connection(maxPoolConnection, \"main1\", 'maxpool1')\n",
        "            \n",
        "            self.hidden2 = maxpool1\n",
        "            self.hidden2_name = 'maxpool1'\n",
        "\n",
        "        if deep:\n",
        "            # # Second hidden layer\n",
        "            conv_size2 = compute_size(conv_size1, filter_size2, stride2)\n",
        "\n",
        "            main2 = NodesType_LC(shape= [n_channels2, conv_size2, conv_size2],traces=True, tc_trace=tc_trace,traces_additive = trace_additive,\n",
        "                                            tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "            \n",
        "            self.add_layer(main2, name=\"main2\")\n",
        "\n",
        "            ### connections \n",
        "            lc2_input_shape = (conv_size1,conv_size1)\n",
        "            LC2 = LocalConnection(self.hidden2, main2, filter_size2, stride2, n_channels1, n_channels2, input_shape= lc2_input_shape,\n",
        "            nu = _pair(nu_LC2), update_rule = update_rule_LC2, wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC2)\n",
        "\n",
        "            self.add_connection(LC2,  self.hidden2_name, \"main2\")\n",
        "            self.convergences['lc2'] = []\n",
        "            if LC2_weights_path:\n",
        "                a = torch.load(LC2_weights_path)\n",
        "                LC2.w.data = a['state_dict']['main1_to_main2.w']\n",
        "                print(\"Weights loaded ...\")\n",
        "                \n",
        "            \n",
        "            elif normal_init:\n",
        "                w_lc_init = torch.normal(mu,std, size = (n_channels1, n_channels2 * compute_size(conv_size1, filter_size2, stride2)**2, filter_size2**2))\n",
        "                LC2.w.data = w_lc_init\n",
        "\n",
        "            self.final_connection_source_name = 'main2'\n",
        "            self.final_connection_source = main2\n",
        "\n",
        "            if inh_LC2:\n",
        "                main_width = conv_size2\n",
        "                w_inh_LC2 = torch.zeros(n_channels2,main_width,main_width,n_channels2,main_width,main_width)\n",
        "                for c in range(n_channels2):\n",
        "                    for w1 in range(main_width):\n",
        "                        for w2 in range(main_width):\n",
        "                            w_inh_LC2[c,w1,w2,:,w1,w2] = - inh_factor_LC2\n",
        "                            w_inh_LC2[c,w1,w2,c,w1,w2] = 0\n",
        "            \n",
        "                w_inh_LC2 = w_inh_LC2.reshape(main2.n,main2.n)\n",
        "                                                                \n",
        "                LC_recurrent_inhibition2 = Connection(\n",
        "                    source=main2,\n",
        "                    target=main2,\n",
        "                    w=w_inh_LC2,\n",
        "                )\n",
        "                self.add_connection(LC_recurrent_inhibition2, \"main2\", \"main2\")\n",
        "\n",
        "\n",
        "            if maxPool2:\n",
        "                maxPool_kernel = 2\n",
        "                maxPool_stride = 2\n",
        "                conv_size2 =compute_size(conv_size2, maxPool_kernel, maxPool_stride)\n",
        "                self.final_connection_source_name = 'maxpool2'\n",
        "                maxpool2 = LIFNodes(shape= [self.n_channels2, conv_size2, conv_size2], refrac = 0)\n",
        "                self.final_connection_source = maxpool2\n",
        "                maxPoolConnection2 = MaxPool2dLocalConnection(main2, maxpool2, maxPool_kernel, maxPool_stride)\n",
        "\n",
        "                self.add_layer(maxpool2, name=\"maxpool2\")\n",
        "                self.add_connection(maxPoolConnection2, \"main2\", 'maxpool2')\n",
        "\n",
        "\n",
        "        ### main2 to output\n",
        "        out = NodesType_Output(n= n_neurons, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "        self.add_layer(out, \"output\")\n",
        "\n",
        "        last_main_out = Connection(self.final_connection_source, out, nu = nu_Output, update_rule = update_rule_Output, wmin = wmin, wmax= wmax, norm = norm_factor_out)\n",
        "\n",
        "        if normal_init:\n",
        "            w_last_main_init = torch.normal(mu,std,size = (self.final_connection_source.n,out.n)) \n",
        "            last_main_out.w.data = w_last_main_init\n",
        "\n",
        "        self.add_connection(last_main_out, self.final_connection_source_name, \"output\")\n",
        "        self.convergences['last_main_out'] = []\n",
        "        ### Inhibitory:\n",
        "        if inh_type == 'between_layers':\n",
        "            w = -inh_factor * torch.ones(out.n, out.n)\n",
        "            for c in range(n_actions):\n",
        "                ind = slice(c*neuron_per_action,(c+1)*neuron_per_action)\n",
        "                w[ind, ind] = 0\n",
        "\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        elif inh_type == 'one_2_all':\n",
        "            w = -inh_factor * (torch.ones(out.n, out.n) - torch.eye(out.n, out.n))\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        # Diehl and Cook\n",
        "        elif inh_type == 'DC':\n",
        "            raise NotImplementedError('Diehl and cook not implemented yet fo r 10 classes')\n",
        "        elif inh_type == None:\n",
        "            pass\n",
        "        # Directs network to GPU\n",
        "        if gpu:\n",
        "            self.to(\"cuda\")\n",
        "\n",
        "    def frame_process(self, x):\n",
        "        x[x<1.0] = 2.0\n",
        "        x[x==1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "\n",
        "\n",
        "    def get_state_spiking(self):\n",
        "        intensity = self.intensity\n",
        "        screen = self.env.render(mode='rgb_array')\n",
        "        screen = screen.transpose((2, 0, 1))\n",
        "        _, screen_height, screen_width = screen.shape\n",
        "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "        view_width = int(screen_width * 0.6)\n",
        "        world_width = self.env.x_threshold * 2\n",
        "        scale = screen_width / world_width\n",
        "        cart_location = int(self.env.state[0] * scale + screen_width / 2.0)\n",
        "        if cart_location < view_width // 2:\n",
        "            slice_range = slice(view_width)\n",
        "        elif cart_location > (screen_width - view_width // 2):\n",
        "            slice_range = slice(-view_width, None)\n",
        "        else:\n",
        "            slice_range = slice(cart_location - view_width // 2,\n",
        "                                cart_location + view_width // 2)\n",
        "            \n",
        "        # Strip off the edges, so that we have a square image centered on a cart\n",
        "        screen = screen[:, :, slice_range]\n",
        "\n",
        "        # Convert to float, rescale, convert to torch tensor\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        h = screen.shape[1]\n",
        "        w = screen.shape[2]\n",
        "        resize = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize([80, 180]),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "                    # transforms.Resize([80, 80]),\n",
        "                    #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "                    # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "                    #transforms.CenterCrop((crop_size, crop_size)),\n",
        "                    transforms.Grayscale(),\n",
        "                    transforms.ToTensor(),\n",
        "                    #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "                    transforms.Lambda(self.frame_process),\n",
        "                    #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "                    transforms.Lambda(lambda x: x * intensity),\n",
        "                    transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "        screen = resize(screen)\n",
        "        if self.frame_analysis:\n",
        "            f = screen.sum(axis=0)\n",
        "            f = f.to(device)\n",
        "            plt.imshow(f.cpu().numpy().squeeze(), cmap='gray')\n",
        "            plt.show()\n",
        "        return screen\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        hparams = None,\n",
        "        online_validate = True,\n",
        "        running_window_length = 250,\n",
        "        verbose = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        manual_seed(SEED)\n",
        "        if self.wandb_active:\n",
        "            wandb.watch(self)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "\n",
        "        self.add_monitor(reward_monitor, name=\"reward\")\n",
        "\n",
        "        reward_hist = []\n",
        "\n",
        "        self.spikes = {}\n",
        "        for layer in set(self.layers):\n",
        "            self.spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=None)\n",
        "            self.add_monitor(self.spikes[layer], name=\"%s_spikes\" % layer)\n",
        "            self.dopaminergic_layers = self.layers[\"output\"]\n",
        "       \n",
        "        self.episode = 0\n",
        "        rew = 0.0\n",
        "        tot_rew = 0.0\n",
        "\n",
        "\n",
        "        if self.load_path:\n",
        "            \n",
        "            self.model_params = torch.load(self.load_path)\n",
        "            self.load_state_dict(torch.load(self.load_path)['state_dict'])\n",
        "            self.episode =  self.model_params['episode']\n",
        "            hparams = self.model_params['hparams']\n",
        "            reward_hist = self.model_params['reward_hist']\n",
        "            print(f'Previous model loaded! Resuming training from episode {self.episode}...\\n')\n",
        "        else:\n",
        "            self.env = gym.make('CartPole-v0')\n",
        "            self.env.reset()\n",
        "            print(f'Previous model not found! Training from the beginning...\\n')\n",
        "\n",
        "        pbar = tqdm(total=self.num_episodes)\n",
        "        \n",
        "        if self.time_analysis:\n",
        "            self.sample_spikes = {'input': [], 'main1': [], 'output': []}\n",
        "\n",
        "        for ep in range(self.num_episodes):\n",
        "            # print(f\"episode: {ep+1}\")\n",
        "            self.reset_state_variables()\n",
        "            self.env.reset()\n",
        "            done = False\n",
        "            tot_rew = 0.0\n",
        "            success = False\n",
        "            failure = False\n",
        "            num_steps = 0\n",
        "            for t in count():\n",
        "                \n",
        "                if t != 0:\n",
        "                    print(\"state (x, x_dot, theta (rad), theta_dot):\", round(x,3), round(x_dot*0.02, 3), round(theta, 3), round(theta_dot*0.02,3), \"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', round(rew,3))\n",
        "                    if self.time_analysis:\n",
        "                        self.sample_spikes['input'].append(self.spikes['input'].get('s'))\n",
        "                        self.sample_spikes['main1'].append(self.spikes['main1'].get('s'))\n",
        "                        self.sample_spikes['output'].append(self.spikes['output'].get('s').view(self.time, self.batch_size, n_actions, neuron_per_action))\n",
        "\n",
        "                        # w_lc1 = self.connections[('input', 'main1')].w\n",
        "                        # w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "                \n",
        "                image = self.get_state_spiking()\n",
        "                if gpu:\n",
        "                    inputs = {\"input\": image.cuda().view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "                else:\n",
        "                    inputs = {\"input\": image.view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "\n",
        "                clamp = {}\n",
        "                if self.clamp_intensity is not None:\n",
        "                    encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "                    clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "                # if done:\n",
        "                #     failure = True\n",
        "\n",
        "                if t >= self.max_steps:\n",
        "                    success = True\n",
        "                    done = True\n",
        "                elif done:\n",
        "                    failure = True\n",
        "\n",
        "\n",
        "                self.run(inputs=inputs, \n",
        "                        time = self.time,\n",
        "                        one_step=False,\n",
        "                        clamp = clamp,\n",
        "                        env = self.env,\n",
        "                        success = success,\n",
        "                        failure = failure,\n",
        "                        **kwargs,\n",
        "                        )\n",
        "                \n",
        "                rew = float(reward_monitor.get()[0])\n",
        "                tot_rew += rew\n",
        "                \n",
        "                lc_spikes1 = self.spikes['main1'].get('s')\n",
        "                #lc_spikes2 = self.spikes['main2'].get('s')\n",
        "                out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, self.n_actions, self.neuron_per_action)\n",
        "                sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "                selected_action = torch.argmax(sum_spikes, dim=1)\n",
        "\n",
        "                self.spikes['main1'].reset_state_variables()\n",
        "                self.spikes[\"output\"].reset_state_variables()\n",
        "                reward_monitor.reset_state_variables()\n",
        "                #self.reset_state_variables()\n",
        "\n",
        "                if done:\n",
        "                    if success == True:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\Successful episode!')\n",
        "                        num_steps = t+1\n",
        "                    else:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\nEpisode not successful!')\n",
        "                        num_steps = t+1\n",
        "                    break\n",
        "\n",
        "                obs, reward, done, _ = self.env.step(int(selected_action[0]))     \n",
        "                x = obs[0]\n",
        "                x_dot = obs[1]\n",
        "                theta = obs[2]\n",
        "                theta_dot = obs[3]\n",
        "\n",
        "                # Get voltage recording.\n",
        "                #main_voltage = main_monitor.get(\"v\")\n",
        "\n",
        "                #tensorboard.update(step= i)\n",
        "\n",
        "\n",
        "            if self.lc_weights_vis:\n",
        "                plot_locally_connected_weights(self.connections[('input','main1')].w, self.n_channels1, self.filter_size1,\n",
        "                                                compute_size(self.crop_size, self.filter_size1, self.stride1), self.connections[('input','main1')].locations,\n",
        "                                                self.crop_size ** 2)\n",
        "                plt.show()\n",
        "\n",
        "            if self.wandb_active:\n",
        "                wandb.log({\n",
        "                        **{'reward': tot_rew},\n",
        "                        **{' to '.join(name) + ' std': c.w.std().item() for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                        #**{name + ' spikes': monitor.get('s').sum().item() for name, monitor in self.spikes.items()},\n",
        "                        **{' to '.join(name) + \" gradients\": wandb.Histogram(c.w.cpu()) for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                    },\n",
        "                    step = self.episode)\n",
        "\n",
        "\n",
        "            #self.reward_fn.update() \n",
        "            #Plot_et.plot()    \n",
        "            # self.reset_state_variables()  # Reset state variables.\n",
        "            \n",
        "            self.episode += 1\n",
        "            reward_hist.append(tot_rew)\n",
        "            print(f'\\nEpisode {self.episode} lasted for {num_steps} time steps with total reward of {tot_rew}\\n')\n",
        "            if self.episode % self.save_interval == 0 and self.save_path:\n",
        "                model_params = {'state_dict': self.state_dict(), 'hparams': network_hparams, 'episode': self.episode, 'reward_hist': reward_hist}\n",
        "                torch.save(model_params, self.save_path)\n",
        "                print(\"\\n Model saved!\\n\")\n",
        "\n",
        "            pbar.set_description_str(\"Episode: \"+str(self.episode)+ \", Number of steps: \" + str(num_steps) +\", Episode Total Reward: \" + \"{:.2f}\".format(tot_rew))\n",
        "            pbar.update()\n",
        "\n",
        "    \n",
        "    def single_trial(self):\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        image = self.get_state_spiking()\n",
        "\n",
        "        if gpu:\n",
        "            inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "        else:\n",
        "            inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step = False,\n",
        "                mode = 'RL',\n",
        "                env = 'CartPole'\n",
        "                )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqAFucAUDb8"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3TerGeJoFdzg"
      },
      "outputs": [],
      "source": [
        "n_neurons = 2 #100\n",
        "n_actions = 2\n",
        "neuron_per_action = int(n_neurons/n_actions)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "crop_size = 40\n",
        "intensity = 200\n",
        "frame_analysis = False\n",
        "lc_weights_vis = False\n",
        "\n",
        "obs = 100\n",
        "dec = 200\n",
        "learn = 100\n",
        "time = obs+dec+learn\n",
        "\n",
        "max_steps = 100\n",
        "num_episodes = 10000\n",
        "\n",
        "filter_size1 = 24\n",
        "stride1 = 8\n",
        "n_channels1 = 100\n",
        "\n",
        "save_interval = 100\n",
        "\n",
        "learning_rate = 0.1\n",
        "norm_factor = 0.25\n",
        "\n",
        "add_layer_fc = False\n",
        "num_neurons_in_fc = 10\n",
        "NodeTypeFC = LIFNodes\n",
        "\n",
        "nu_fc = (0.0001,0.01)\n",
        "update_rule_fc = PostPre\n",
        "\n",
        "classic_reward = False\n",
        "inh_type = 'between_layers'\n",
        "\n",
        "clamp_intensity = 40\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': crop_size,\n",
        "    'intensity': intensity,\n",
        "    'round_input': False,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': n_channels1,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': filter_size1,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': stride1,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_actions': n_actions,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'max_steps': max_steps,\n",
        "    'num_episodes': num_episodes,\n",
        "    'save_interval': save_interval,\n",
        "    'add_layer_fc': add_layer_fc,\n",
        "    'num_neurons_in_fc': num_neurons_in_fc,\n",
        "    'classic_reward': classic_reward,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': True,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': obs,\n",
        "    'decision_period': dec,\n",
        "    'time_analysis': False,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'NodeType_FC': NodeTypeFC,\n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': PostPre,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDPET,\n",
        "    'update_rule_fc': update_rule_fc,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.0001,0.01),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output': learning_rate,\n",
        "    'nu_fc': nu_fc,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': True,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': inh_type,\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    'inh_fc': False,\n",
        "    'inh_factor_fc': 100,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': norm_factor*filter_size1*filter_size1,\n",
        "    'norm_factor_fc' : None,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': clamp_intensity,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': '/content/drive/My Drive/LCNet/BioLCNet_cartpole_100tr.pth',\n",
        "    'load_path': '/content/drive/My Drive/LCNet/BioLCNet_cartpole_100tr.pth',#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'LC_weights_path': '/content/drive/My Drive/LCNet/BioLCNet_cartpole1.pth',#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25_weights.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': lc_weights_vis,\n",
        "    'out_weights_vis': False,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': False,\n",
        "    'frame_analysis': False,\n",
        "\n",
        "    ## reward\n",
        "    'n_labels': n_actions,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': 1.,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0005,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokdidkrV2Z5"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Venb2KhSYrT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d42ebf8-cced-4658-b27a-77012d019644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oThYyYvHJzeP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a2022e92497c45bb9f467e121924b024",
            "43012881fd414e41942d734f7721a223",
            "ac49951d9e8c4a4db5fbb502e386a507",
            "53574f384f734bdba6a8cdbdd0959fa9",
            "15151c976c404a379f20e679bf70f15e",
            "c2cbeac26956419cb6cffad8959eca0a",
            "487cd8c6a5894f85adfffea8c7392892",
            "b378ef99e49a4894b364c1e589158afa",
            "02c60c9b619c47fdbf4c6ba4f0c20f92",
            "51cdd6210ab24a6e98bb54f4c11164f9",
            "283723e2216443f782894fffefd2684e"
          ]
        },
        "outputId": "3616c398-3c02-49a7-9103-8bc8a98c8373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/singularbrain/biolcnet/runs/1u6akl6s\" target=\"_blank\">fluent-water-275</a></strong> to <a href=\"https://wandb.ai/singularbrain/biolcnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights loaded ...\n",
            "Previous model loaded! Resuming training from episode 300...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2022e92497c45bb9f467e121924b024",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.0 0.029 0.001 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.004 0.03 0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.845\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.008 0.036 0.013 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.691\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.004 0.049 0.007 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.508\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.0 0.056 0.002 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.587\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.004 0.058 0.008 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.691\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.008 0.066 0.014 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.529\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.004 0.08 0.009 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.336\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.008 0.088 0.015 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.405\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.004 0.103 0.01 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.008 0.113 0.016 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.267\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.004 0.129 0.011 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.059\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.0 0.14 0.006 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.11\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 0.003 0.146 0.001 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.182\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 0.007 0.147 -0.004 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.276\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 0.003 0.143 0.003 output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.204\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 0.007 0.146 -0.002 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.243\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 0.003 0.144 0.005 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.253\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 0.007 0.149 -0.0 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.192\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.003 0.149 0.007 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.286\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.001 0.155 0.013 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.124\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.003 0.169 0.009 output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.007 0.177 0.004 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.019\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.003 0.181 0.011 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.058\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.001 0.192 0.018 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.131\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.003 0.209 0.013 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.354\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.001 0.222 0.02 output tensor([[13,  2]]) selected_action: tensor([0]) rew: -0.324\n",
            "output tensor([[4, 3]]) selected_action: tensor([0]) rew: -5.561825244960643\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 612 lasted for 31 time steps with total reward of 3.8921635267200347\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.005 0.041 -0.006 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.798\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.001 0.035 0.0 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.659\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.005 0.036 -0.005 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.823\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.001 0.03 0.001 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.697\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.003 0.031 0.007 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.835\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.007 0.038 0.013 output tensor([[9, 5]]) selected_action: tensor([0]) rew: 0.68\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.011 0.051 0.019 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.496\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.015 0.07 0.025 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.283\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.019 0.095 0.031 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.038\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.023 0.126 0.038 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.239\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.019 0.164 0.033 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.549\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.023 0.197 0.04 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.604\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.149 -0.027 0.237 0.047 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.93\n",
            "output tensor([[21,  0]]) selected_action: tensor([0]) rew: -6.2926735063820125\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 613 lasted for 14 time steps with total reward of -3.30554966149192\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.003 0.045 0.006 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.785\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.001 0.051 0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.631\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.003 0.052 0.007 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.741\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.001 0.058 0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.583\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.003 0.06 0.008 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.689\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.001 0.067 0.002 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.527\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.004 0.069 -0.003 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.0 0.066 0.003 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.004 0.069 -0.003 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.613\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.008 0.066 -0.008 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.012 0.058 -0.013 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.486\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.008 0.045 -0.007 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.388\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.004 0.038 -0.001 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.607\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.0 0.037 0.005 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.794\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 -0.004 0.042 0.011 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.697\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 -0.007 0.053 0.017 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.52\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.011 0.07 0.023 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.314\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.015 0.094 0.03 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.077\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.011 0.124 0.025 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.192\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.015 0.148 0.031 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.203\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.011 0.179 0.026 output tensor([[3, 5]]) selected_action: tensor([1]) rew: -0.484\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.008 0.205 0.022 output tensor([[1, 4]]) selected_action: tensor([1]) rew: -0.511\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.012 0.227 0.029 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.52\n",
            "output tensor([[6, 8]]) selected_action: tensor([1]) rew: -5.798180206396403\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 614 lasted for 24 time steps with total reward of 2.5639544263705263\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.005 0.005 -0.006 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.974\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.001 -0.0 0.0 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.005 -0.0 -0.006 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.996\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.009 -0.006 -0.012 output tensor([[ 4, 14]]) selected_action: tensor([1]) rew: 0.854\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.013 -0.018 -0.018 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.68\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.009 -0.035 -0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.005 -0.047 -0.006 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.537\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.001 -0.053 -0.001 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.003 -0.054 0.005 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 -0.007 -0.049 0.01 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.011 -0.039 0.016 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.506\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.015 -0.023 0.022 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.011 -0.001 0.016 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.353\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.007 0.014 0.01 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.606\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.003 0.024 0.004 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.689\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.001 0.028 -0.002 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.787\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.005 0.026 -0.007 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.822\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.001 0.019 -0.001 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.688\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.005 0.017 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.874\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.009 0.01 -0.013 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.738\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.013 -0.003 -0.019 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.629\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.017 -0.021 -0.025 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.519\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.013 -0.046 -0.019 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.283\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.017 -0.065 -0.025 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.309\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.021 -0.09 -0.031 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.065\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.025 -0.121 -0.038 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.21\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 0.021 -0.159 -0.033 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.519\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.151 0.017 -0.191 -0.028 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.572\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.168 0.013 -0.219 -0.023 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.608\n",
            "output tensor([[18,  1]]) selected_action: tensor([0]) rew: -5.626898783165493\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 615 lasted for 30 time steps with total reward of 8.069168810642225\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.003 0.025 0.007 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.862\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.001 0.032 0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.708\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.003 0.033 0.007 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.007 0.04 0.013 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.661\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.003 0.054 0.008 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.475\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 0.001 0.061 0.002 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.551\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 0.005 0.064 -0.003 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.652\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.001 0.06 0.003 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.615\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.003 0.063 0.009 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.637\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 0.001 0.073 0.004 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.468\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 0.005 0.076 -0.002 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.56\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.001 0.075 0.005 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.595\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.005 0.079 -0.001 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.526\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.001 0.079 0.006 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.604\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.003 0.084 0.012 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.007 0.096 0.018 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.297\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.011 0.115 0.025 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.081\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.007 0.139 0.02 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.167\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.011 0.159 0.026 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.158\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.015 0.186 0.033 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.419\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.019 0.219 0.04 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.713\n",
            "output tensor([[5, 6]]) selected_action: tensor([1]) rew: -6.043580561955641\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 616 lasted for 22 time steps with total reward of 2.0922417456154134\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.005 0.045 0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.77\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.009 0.05 0.012 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.648\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.005 0.062 0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.468\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.001 0.068 0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.55\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.005 0.069 0.007 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.657\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.009 0.076 0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.497\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.005 0.089 0.008 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.307\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.009 0.097 0.014 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.378\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.005 0.111 0.009 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.18\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.009 0.12 0.016 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.242\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.005 0.136 0.011 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.036\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.009 0.146 0.017 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.088\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.005 0.164 0.012 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.128\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.001 0.176 0.008 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.088\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 0.003 0.183 0.003 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.029\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.001 0.186 0.01 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.051\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 0.003 0.196 0.005 output tensor([[1, 3]]) selected_action: tensor([1]) rew: -0.135\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.001 0.201 0.012 output tensor([[8, 8]]) selected_action: tensor([0]) rew: -0.067\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.002 0.214 0.008 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.266\n",
            "output tensor([[5, 8]]) selected_action: tensor([1]) rew: -5.212573299603745\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 617 lasted for 20 time steps with total reward of -1.051959295173912\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.004 0.004 -0.006 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 0.976\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.008 -0.001 -0.011 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.838\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.004 -0.013 -0.006 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.706\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.008 -0.018 -0.012 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.797\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.011 -0.03 -0.018 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.015 -0.048 -0.024 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.418\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.019 -0.071 -0.03 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.183\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.023 -0.101 -0.036 output tensor([[1, 8]]) selected_action: tensor([1]) rew: -0.084\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 0.027 -0.137 -0.042 output tensor([[1, 8]]) selected_action: tensor([1]) rew: -0.383\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.148 0.023 -0.179 -0.038 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.716\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.171 0.027 -0.217 -0.044 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.795\n",
            "output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -6.145122165480504\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 618 lasted for 12 time steps with total reward of -3.581770515882662\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.004 0.008 0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.949\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.0 0.014 -0.0 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.823\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.004 0.013 0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.927\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.008 0.019 0.012 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.794\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.012 0.031 0.018 output tensor([[22,  0]]) selected_action: tensor([0]) rew: 0.619\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.015 0.048 0.024 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.414\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.019 0.072 0.03 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.179\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.023 0.102 0.036 output tensor([[8, 8]]) selected_action: tensor([0]) rew: -0.087\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.019 0.138 0.031 output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.386\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.023 0.169 0.037 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.429\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -0.027 0.206 0.044 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.742\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.168 -0.031 0.25 0.051 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -1.091\n",
            "output tensor([[ 3, 19]]) selected_action: tensor([1]) rew: -6.476820536842281\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 619 lasted for 13 time steps with total reward of -4.506335775885884\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.004 -0.031 -0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.824\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 0.0 -0.036 0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.727\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.004 -0.035 -0.005 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.0 -0.041 0.0 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.695\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.004 -0.04 -0.006 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.803\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.008 -0.046 -0.012 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.658\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.012 -0.058 -0.018 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.016 -0.077 -0.024 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.266\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.012 -0.101 -0.019 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.024\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.008 -0.12 -0.014 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.041\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.004 -0.134 -0.009 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.079\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.008 -0.143 -0.015 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.139\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.012 -0.158 -0.022 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.069\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.008 -0.18 -0.017 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.309\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.012 -0.198 -0.024 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.295\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.016 -0.222 -0.031 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.55\n",
            "output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: -5.839528599304344\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 620 lasted for 17 time steps with total reward of -1.515503073010569\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.003 -0.043 0.005 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.788\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.007 -0.038 0.011 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.66\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.003 -0.027 0.005 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.007 -0.022 0.01 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.75\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.011 -0.012 0.016 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.631\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.007 0.004 0.01 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.539\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.003 0.015 0.004 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.721\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.001 0.019 -0.001 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.004 0.018 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.876\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 0.001 0.011 -0.001 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.739\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.003 0.01 0.005 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.921\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.007 0.015 0.011 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.832\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.003 0.025 0.005 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.661\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.007 0.03 0.011 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.753\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.003 0.041 0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.579\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.007 0.047 0.011 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.668\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.011 0.058 0.018 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.489\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.015 0.076 0.024 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.281\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.019 0.1 0.03 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.042\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.023 0.13 0.037 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.23\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.019 0.166 0.032 output tensor([[3, 8]]) selected_action: tensor([1]) rew: -0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -0.015 0.198 0.027 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.179 -0.011 0.225 0.022 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.617\n",
            "output tensor([[3, 6]]) selected_action: tensor([1]) rew: -5.632772314116053\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 621 lasted for 24 time steps with total reward of 4.6966641273931575\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.003 0.004 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.958\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.007 0.009 0.011 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.854\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.003 0.02 0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.682\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.001 0.025 -0.001 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.775\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.005 0.025 -0.006 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.866\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.001 0.018 -0.0 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.726\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.003 0.018 0.006 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.906\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.007 0.024 0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.77\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.003 0.036 0.006 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.593\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.001 0.042 0.0 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.68\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.003 0.042 0.006 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.007 0.048 0.013 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.637\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.003 0.061 0.007 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.453\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.007 0.068 0.013 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.532\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.011 0.081 0.02 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.342\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.015 0.101 0.026 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.122\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.011 0.127 0.021 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.13\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.015 0.148 0.027 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.123\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.019 0.175 0.034 output tensor([[9, 1]]) selected_action: tensor([0]) rew: -0.387\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.023 0.209 0.041 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.685\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.129 -0.027 0.25 0.048 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -1.018\n",
            "output tensor([[2, 8]]) selected_action: tensor([1]) rew: -6.387842965007096\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 622 lasted for 22 time steps with total reward of 1.957136738373043\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.004 0.005 0.006 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.966\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.008 0.012 0.012 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.012 0.024 0.018 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.639\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.015 0.042 0.024 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.433\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.019 0.066 0.03 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.196\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.023 0.096 0.036 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.072\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.019 0.133 0.031 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.373\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.016 0.164 0.026 output tensor([[1, 3]]) selected_action: tensor([1]) rew: -0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.019 0.19 0.033 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.442\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.119 -0.016 0.224 0.029 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.737\n",
            "output tensor([[8, 4]]) selected_action: tensor([0]) rew: -5.780826427813446\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 623 lasted for 11 time steps with total reward of -4.770648121534723\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.004 0.019 -0.007 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.882\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.008 0.012 -0.012 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.744\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.004 -0.0 -0.006 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.632\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.001 -0.007 -0.001 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.838\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 -0.003 -0.007 0.005 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.953\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.007 -0.002 0.011 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 0.835\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 -0.003 0.009 0.005 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.007 0.014 0.011 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.011 0.025 0.017 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.007 0.042 0.011 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.455\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.011 0.053 0.017 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.516\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.007 0.071 0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.309\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.003 0.083 0.007 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.364\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.0 0.089 0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.442\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.003 0.09 0.008 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.544\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.007 0.098 0.014 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.378\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.004 0.112 0.009 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.182\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.007 0.121 0.015 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.245\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.004 0.136 0.01 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.04\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.008 0.146 0.017 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.093\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.011 0.163 0.024 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.122\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.015 0.187 0.03 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.37\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.019 0.217 0.037 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.652\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: -5.969576989452422\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 624 lasted for 24 time steps with total reward of 3.5391026595762174\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.003 0.033 0.006 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.837\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.001 0.039 0.001 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.684\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.005 0.04 -0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.795\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.001 0.035 0.001 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.687\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.003 0.036 0.007 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 0.802\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.007 0.044 0.013 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.011 0.057 0.019 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.458\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.007 0.076 0.014 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.241\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.011 0.09 0.02 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.285\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.007 0.111 0.015 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.061\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.003 0.126 0.01 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.095\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.007 0.136 0.017 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.151\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.011 0.152 0.023 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.061\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.015 0.175 0.03 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.306\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.019 0.205 0.037 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.015 0.242 0.032 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.898\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: -5.962222827475684\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 625 lasted for 17 time steps with total reward of -2.068241199121433\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.003 -0.018 0.005 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.907\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.007 -0.012 0.011 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.781\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.003 -0.001 0.005 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.663\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.0 0.004 -0.001 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.865\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 0.004 0.003 -0.007 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.964\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.008 -0.003 -0.012 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.822\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.004 -0.016 -0.007 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.674\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.0 -0.022 -0.001 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.761\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.003 -0.023 0.005 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.874\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.007 -0.018 0.011 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.766\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.011 -0.008 0.016 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.647\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.015 0.009 0.022 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.555\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.019 0.031 0.028 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.404\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.023 0.059 0.034 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.15\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.027 0.093 0.04 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.135\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.023 0.134 0.035 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.453\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 -0.027 0.169 0.042 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.515\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.165 -0.023 0.21 0.037 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.848\n",
            "output tensor([[2, 6]]) selected_action: tensor([1]) rew: -5.9288459504237006\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 626 lasted for 19 time steps with total reward of 1.9530973134296463\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.003 0.021 0.006 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.884\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.007 0.027 0.011 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.761\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.011 0.038 0.018 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.015 0.056 0.024 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.38\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.011 0.079 0.018 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.144\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.015 0.097 0.024 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.169\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.019 0.122 0.031 output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.076\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.015 0.153 0.026 output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.353\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.019 0.178 0.033 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.374\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.015 0.211 0.028 output tensor([[2, 8]]) selected_action: tensor([1]) rew: -0.666\n",
            "output tensor([[2, 2]]) selected_action: tensor([0]) rew: -5.704921636989619\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 627 lasted for 11 time steps with total reward of -4.248538837392191\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.004 -0.024 -0.005 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.866\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.001 -0.029 0.0 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.754\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.004 -0.029 -0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.852\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.008 -0.034 -0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.722\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.005 -0.046 -0.006 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.544\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.001 -0.052 -0.0 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.629\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.003 -0.053 0.005 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.739\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.001 -0.048 -0.001 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.623\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.003 -0.049 0.004 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.744\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.007 -0.044 0.01 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.658\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.003 -0.034 0.004 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.001 -0.031 -0.002 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.741\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.003 -0.033 0.003 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.798\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.001 -0.029 -0.003 output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.758\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.005 -0.032 -0.009 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.793\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.001 -0.041 -0.003 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.63\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.003 -0.044 0.003 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.729\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.001 -0.041 -0.004 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.727\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.005 -0.045 -0.01 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.714\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.009 -0.055 -0.016 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.544\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.013 -0.07 -0.022 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.344\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.009 -0.092 -0.017 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.114\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.005 -0.109 -0.011 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.144\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.001 -0.12 -0.006 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.196\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 -0.003 -0.127 -0.001 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.27\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.001 -0.128 -0.008 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.366\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.003 -0.135 -0.003 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.196\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 -0.007 -0.138 0.002 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.284\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.011 -0.136 0.007 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.286\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.007 -0.129 0.0 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.173\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.003 -0.129 -0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.373\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.001 -0.135 -0.013 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.232\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.003 -0.148 -0.008 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.037\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.001 -0.155 -0.015 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.1\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.003 -0.17 -0.01 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.105\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.006 -0.18 -0.005 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.055\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.003 -0.185 -0.012 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.016\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.001 -0.197 -0.019 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.18\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.002 -0.215 -0.014 output tensor([[11,  0]]) selected_action: tensor([0]) rew: -0.409\n",
            "output tensor([[8, 2]]) selected_action: tensor([0]) rew: -5.385997559810831\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 628 lasted for 40 time steps with total reward of 11.103154724477942\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.005 -0.046 -0.006 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.772\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.001 -0.051 -0.0 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.637\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.003 -0.052 0.005 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.748\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.001 -0.046 -0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.005 -0.047 -0.007 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.756\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.009 -0.054 -0.013 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.598\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.005 -0.068 -0.008 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.411\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.001 -0.075 -0.002 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.486\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.005 -0.077 -0.009 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.001 -0.086 -0.003 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.003 -0.089 0.002 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.508\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.001 -0.087 -0.004 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.522\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.003 -0.092 0.001 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.475\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.001 -0.091 -0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.539\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.005 -0.096 -0.012 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.431\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.009 -0.108 -0.018 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.245\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.013 -0.126 -0.025 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.028\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.009 -0.151 -0.02 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.222\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.005 -0.171 -0.015 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.214\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.001 -0.186 -0.01 output tensor([[9, 3]]) selected_action: tensor([0]) rew: -0.188\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.005 -0.196 -0.017 output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.142\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.001 -0.213 -0.013 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.363\n",
            "output tensor([[8, 6]]) selected_action: tensor([0]) rew: -5.332349957341258\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 629 lasted for 23 time steps with total reward of 2.3166911533258787\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.004 0.013 -0.006 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.936\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.0 0.008 0.0 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.793\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.004 0.008 -0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.96\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.0 0.002 0.0 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 0.823\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.004 0.002 -0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.983\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.0 -0.003 0.0 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.85\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.004 -0.003 0.006 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.977\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.0 0.003 0.0 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.833\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.004 0.003 0.006 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.978\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.008 0.01 0.012 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.83\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.012 0.022 0.018 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.654\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.015 0.04 0.024 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.449\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.019 0.063 0.03 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 0.213\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.015 0.093 0.025 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.053\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.019 0.118 0.031 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.061\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.023 0.149 0.038 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.338\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.027 0.187 0.044 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.65\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.15 -0.031 0.231 0.051 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.996\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: -6.379319138442899\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 630 lasted for 19 time steps with total reward of 1.8002242611678838\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.004 0.04 0.006 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.806\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.008 0.046 0.012 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.655\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.012 0.059 0.018 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.474\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.008 0.077 0.013 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.262\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.012 0.09 0.019 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.311\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.008 0.109 0.014 output tensor([[6, 9]]) selected_action: tensor([1]) rew: 0.092\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.012 0.123 0.02 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.132\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.009 0.143 0.015 output tensor([[1, 3]]) selected_action: tensor([1]) rew: -0.097\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.005 0.159 0.01 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.068\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.009 0.169 0.017 output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.019\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.013 0.186 0.024 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.238\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.016 0.21 0.031 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.491\n",
            "output tensor([[6, 8]]) selected_action: tensor([1]) rew: -5.778258088858713\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 631 lasted for 13 time steps with total reward of -3.958999927394615\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.004 -0.023 0.005 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.878\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.0 -0.018 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.763\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.004 -0.019 0.005 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.892\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.008 -0.014 0.011 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.789\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.004 -0.003 0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.0 0.001 -0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.868\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.004 0.0 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.963\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.008 0.005 0.01 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.884\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.004 0.015 0.005 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.716\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.0 0.02 -0.001 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.811\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.004 0.019 0.005 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.878\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.008 0.024 0.011 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.789\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.012 0.034 0.017 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.616\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.016 0.051 0.023 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.414\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.02 0.074 0.029 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.182\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.024 0.103 0.035 output tensor([[16,  0]]) selected_action: tensor([0]) rew: -0.082\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.02 0.139 0.03 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.378\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.174 -0.024 0.169 0.037 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.198 -0.028 0.206 0.044 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.728\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.226 -0.024 0.249 0.039 output tensor([[0, 9]]) selected_action: tensor([1]) rew: -1.074\n",
            "output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: -6.170869448292496\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 632 lasted for 21 time steps with total reward of 2.2624646666027726\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.005 0.009 0.006 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.953\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.009 0.014 0.012 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.012 0.026 0.018 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.641\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.016 0.044 0.024 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.437\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.02 0.067 0.03 output tensor([[11,  4]]) selected_action: tensor([0]) rew: 0.203\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.024 0.097 0.036 output tensor([[14,  2]]) selected_action: tensor([0]) rew: -0.063\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.028 0.133 0.042 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.361\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -0.032 0.175 0.049 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.693\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.157 -0.036 0.224 0.056 output tensor([[9, 1]]) selected_action: tensor([0]) rew: -1.061\n",
            "output tensor([[1, 7]]) selected_action: tensor([1]) rew: -6.465273037193016\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 633 lasted for 10 time steps with total reward of -5.59417316455982\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.004 -0.005 0.006 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.971\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.008 0.001 0.012 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.826\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.004 0.013 0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.699\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.008 0.019 0.012 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.788\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.012 0.031 0.018 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.611\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.016 0.049 0.024 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.405\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.02 0.073 0.03 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.169\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.016 0.103 0.025 output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.1\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.012 0.128 0.02 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.109\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.016 0.147 0.026 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.097\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.02 0.173 0.033 output tensor([[9, 3]]) selected_action: tensor([0]) rew: -0.356\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.024 0.206 0.04 output tensor([[12,  2]]) selected_action: tensor([0]) rew: -0.648\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.028 0.246 0.047 output tensor([[15,  1]]) selected_action: tensor([0]) rew: -0.975\n",
            "output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: -6.338895940164109\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 634 lasted for 14 time steps with total reward of -4.153940488919821\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.004 0.005 0.005 output tensor([[15,  4]]) selected_action: tensor([0]) rew: 0.958\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.008 0.01 0.011 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.846\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.012 0.021 0.017 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.674\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.016 0.038 0.023 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.473\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.02 0.061 0.029 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.242\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.024 0.09 0.035 output tensor([[10,  2]]) selected_action: tensor([0]) rew: -0.02\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.028 0.126 0.042 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.315\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -0.024 0.167 0.037 output tensor([[7, 8]]) selected_action: tensor([1]) rew: -0.643\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.163 -0.028 0.204 0.044 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.717\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.191 -0.024 0.248 0.039 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -1.062\n",
            "output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: -6.15808565199209\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 635 lasted for 11 time steps with total reward of -5.722367371804202\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.004 -0.003 -0.006 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.973\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.008 -0.01 -0.012 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.824\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.012 -0.022 -0.018 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.646\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.008 -0.04 -0.012 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.44\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.004 -0.053 -0.007 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.496\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.008 -0.06 -0.013 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.576\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.004 -0.073 -0.008 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.389\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.0 -0.08 -0.002 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.463\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.003 -0.082 0.003 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.561\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.007 -0.079 0.008 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.528\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.003 -0.071 0.002 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.41\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.001 -0.069 -0.004 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.003 -0.073 0.001 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.566\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.007 -0.072 0.007 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.003 -0.065 0.0 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.492\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.001 -0.065 -0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.681\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.005 -0.071 -0.012 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.541\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.008 -0.083 -0.018 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.356\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.012 -0.102 -0.025 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.141\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.016 -0.126 -0.031 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: -0.106\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.02 -0.158 -0.038 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.386\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.024 -0.196 -0.045 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.7\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.028 -0.24 -0.052 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -1.05\n",
            "output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: -6.435616078033572\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 636 lasted for 24 time steps with total reward of 1.6384334285375957\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.004 -0.01 0.005 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 0.938\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.007 -0.005 0.011 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 0.822\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.011 0.006 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.702\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.007 0.023 0.011 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.553\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.011 0.034 0.017 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.618\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.007 0.051 0.011 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.004 0.062 0.006 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.476\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.008 0.068 0.012 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.56\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.004 0.08 0.007 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.377\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.0 0.086 0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.454\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.004 0.088 -0.004 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.556\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.008 0.084 -0.009 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.482\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.004 0.074 -0.003 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.369\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.008 0.071 -0.008 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.572\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.004 0.063 -0.002 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.452\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.0 0.061 0.004 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.649\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.004 0.065 0.01 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.603\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.0 0.076 0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.427\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.004 0.081 0.011 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.513\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.0 0.092 0.006 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.331\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.004 0.098 0.012 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.41\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.008 0.11 0.019 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.221\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.012 0.129 0.025 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.001\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.016 0.155 0.032 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.251\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.02 0.187 0.039 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.538\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.024 0.225 0.046 output tensor([[5, 1]]) selected_action: tensor([0]) rew: -0.859\n",
            "output tensor([[16,  1]]) selected_action: tensor([0]) rew: -6.215534959774068\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 637 lasted for 27 time steps with total reward of 3.6390371738446934\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.003 -0.038 0.005 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.81\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.007 -0.033 0.011 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.692\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.003 -0.022 0.005 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.576\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.001 -0.017 -0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.779\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.004 -0.019 -0.007 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.885\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.008 -0.026 -0.013 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.004 -0.039 -0.008 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.008 -0.047 -0.014 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.625\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.005 -0.06 -0.008 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.001 -0.068 -0.003 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.51\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.003 -0.071 0.003 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.608\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.007 -0.068 0.008 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.592\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.011 -0.06 0.014 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.47\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.015 -0.047 0.019 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.374\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.011 -0.027 0.013 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.302\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.007 -0.015 0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.011 -0.008 0.013 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.759\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.007 0.005 0.007 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.647\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.003 0.012 0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.809\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.007 0.013 0.007 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.922\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.003 0.019 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.77\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.007 0.02 0.007 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.881\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.011 0.027 0.013 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.727\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.015 0.04 0.019 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.544\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.011 0.059 0.013 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.331\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.015 0.073 0.02 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.38\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.011 0.093 0.014 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.161\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.129 -0.007 0.107 0.009 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.201\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.011 0.116 0.016 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.264\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.007 0.131 0.01 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.059\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.154 -0.011 0.142 0.017 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.112\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.165 -0.007 0.159 0.012 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.104\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -0.011 0.171 0.019 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.063\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.184 -0.007 0.19 0.014 output tensor([[7, 8]]) selected_action: tensor([1]) rew: -0.29\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.191 -0.004 0.204 0.01 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.263\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.195 -0.007 0.214 0.017 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.217\n",
            "output tensor([[12,  1]]) selected_action: tensor([0]) rew: -5.43805669308847\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 638 lasted for 37 time steps with total reward of 10.671898363703818\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.005 0.014 0.006 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.928\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.001 0.02 -0.0 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.789\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.005 0.02 0.006 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.906\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.008 0.026 0.012 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.757\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.012 0.038 0.018 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 0.579\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.016 0.056 0.024 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.371\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.012 0.08 0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.134\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.009 0.098 0.013 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.156\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.012 0.111 0.02 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.201\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.016 0.131 0.026 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.023\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.013 0.157 0.021 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.28\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.016 0.178 0.028 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.28\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.02 0.206 0.035 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.55\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.024 0.241 0.042 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.855\n",
            "output tensor([[6, 5]]) selected_action: tensor([0]) rew: -6.195523738637178\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 639 lasted for 15 time steps with total reward of -3.3624174236560114\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.003 -0.045 0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.762\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.001 -0.039 0.0 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.627\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.004 -0.039 -0.006 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.81\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.008 -0.045 -0.012 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.667\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.004 -0.057 -0.006 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.487\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.001 -0.063 -0.001 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.569\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.004 -0.064 -0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.675\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.008 -0.071 -0.013 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.514\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.005 -0.085 -0.008 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.324\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.001 -0.093 -0.003 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.394\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 -0.003 -0.095 0.002 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.001 -0.093 -0.004 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.482\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.003 -0.097 0.001 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.457\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 -0.007 -0.096 0.007 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.505\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.003 -0.089 0.0 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.381\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.001 -0.089 -0.006 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.572\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.003 -0.095 -0.001 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.001 -0.096 -0.007 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.519\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.005 -0.104 -0.014 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.353\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.009 -0.118 -0.02 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.157\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.005 -0.138 -0.015 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.071\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.009 -0.153 -0.022 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.042\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.005 -0.175 -0.017 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.281\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.009 -0.192 -0.024 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.265\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.013 -0.216 -0.031 output tensor([[3, 5]]) selected_action: tensor([1]) rew: -0.518\n",
            "output tensor([[3, 3]]) selected_action: tensor([0]) rew: -5.805838047242304\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 640 lasted for 26 time steps with total reward of 3.179399308514821\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.003 -0.009 -0.006 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.948\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.007 -0.015 -0.012 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.798\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.003 -0.028 -0.007 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.62\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.007 -0.034 -0.013 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.705\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.003 -0.047 -0.007 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.523\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.007 -0.054 -0.013 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.604\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.011 -0.067 -0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.015 -0.086 -0.026 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.2\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.019 -0.111 -0.032 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.048\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.015 -0.143 -0.027 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.329\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.011 -0.17 -0.022 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.353\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.084 0.007 -0.192 -0.017 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.359\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.003 -0.209 -0.013 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.346\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 0.007 -0.222 -0.02 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.314\n",
            "output tensor([[2, 6]]) selected_action: tensor([1]) rew: -5.55012681778973\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 641 lasted for 15 time steps with total reward of -2.4849269496010087\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.003 0.018 0.005 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.89\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.007 0.023 0.011 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.786\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.011 0.034 0.017 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.612\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.007 0.051 0.011 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.409\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.003 0.063 0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.468\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.007 0.069 0.012 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.551\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.011 0.081 0.018 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.367\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.015 0.099 0.025 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.152\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.019 0.124 0.031 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.095\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.015 0.155 0.026 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.374\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.019 0.182 0.033 output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.398\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.023 0.215 0.04 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.691\n",
            "output tensor([[4, 8]]) selected_action: tensor([1]) rew: -6.02053580692552\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 642 lasted for 13 time steps with total reward of -3.344326760040131\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.004 0.048 0.007 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.756\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.008 0.055 0.013 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.599\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.004 0.068 0.008 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.413\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.0 0.075 0.002 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.004 0.077 -0.003 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.587\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.0 0.074 0.003 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.549\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.004 0.077 0.009 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.008 0.087 0.016 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.396\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.004 0.102 0.01 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.193\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.0 0.113 0.005 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.25\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.004 0.118 0.012 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.33\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.008 0.13 0.018 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.142\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.004 0.148 0.013 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.078\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.008 0.162 0.02 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.041\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.012 0.182 0.027 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.272\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.016 0.208 0.034 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.538\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.02 0.242 0.041 output tensor([[12,  0]]) selected_action: tensor([0]) rew: -0.837\n",
            "output tensor([[8, 4]]) selected_action: tensor([0]) rew: -6.173130540169915\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 643 lasted for 18 time steps with total reward of -2.6679999202264644\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.004 0.016 0.007 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 0.91\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.008 0.023 0.013 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.758\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.012 0.035 0.019 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.577\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.016 0.054 0.025 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.367\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.012 0.079 0.019 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.127\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.016 0.098 0.025 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.147\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.02 0.123 0.032 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.103\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.024 0.155 0.038 output tensor([[8, 2]]) selected_action: tensor([0]) rew: -0.385\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.02 0.193 0.034 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.701\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.024 0.227 0.041 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.765\n",
            "output tensor([[7, 7]]) selected_action: tensor([0]) rew: -6.098333814546919\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 644 lasted for 11 time steps with total reward of -5.167067022336478\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.004 0.017 0.006 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.911\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.0 0.023 0.001 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.76\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.004 0.024 0.007 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.873\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.0 0.031 0.001 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.72\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.004 0.032 0.007 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.83\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.008 0.039 0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.675\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.012 0.052 0.019 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.49\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.016 0.071 0.025 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.276\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.02 0.096 0.032 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.03\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.024 0.128 0.038 output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.247\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.02 0.166 0.033 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.559\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.129 -0.016 0.198 0.028 output tensor([[1, 8]]) selected_action: tensor([1]) rew: -0.615\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.02 0.227 0.035 output tensor([[13,  1]]) selected_action: tensor([0]) rew: -0.654\n",
            "output tensor([[16,  3]]) selected_action: tensor([0]) rew: -5.962478014487485\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 645 lasted for 14 time steps with total reward of -2.4730012483995596\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.005 -0.005 0.006 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.972\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.001 0.001 0.0 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.827\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.003 0.001 -0.006 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.995\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.001 -0.005 0.0 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.851\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.003 -0.005 -0.006 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.975\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.007 -0.011 -0.012 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.831\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.011 -0.022 -0.018 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.015 -0.04 -0.024 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.452\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.019 -0.064 -0.03 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.218\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.022 -0.093 -0.036 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: -0.047\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.026 -0.129 -0.042 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: -0.345\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.03 -0.172 -0.049 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.676\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.026 -0.221 -0.044 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -1.044\n",
            "output tensor([[3, 7]]) selected_action: tensor([1]) rew: -6.159590797269479\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 646 lasted for 14 time steps with total reward of -1.4935606193128113\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.003 0.037 -0.006 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.007 0.031 -0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.676\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.003 0.019 -0.005 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.564\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.007 0.014 -0.011 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.77\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.003 0.003 -0.005 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.653\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 -0.001 -0.003 0.001 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.855\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 -0.005 -0.002 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.973\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 -0.001 0.005 0.001 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.83\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 -0.005 0.005 0.006 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.964\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 -0.009 0.012 0.012 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.814\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.005 0.024 0.007 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.636\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.009 0.03 0.013 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.722\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.005 0.043 0.007 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.541\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.009 0.05 0.013 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.005 0.063 0.007 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.001 0.07 0.002 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.512\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.005 0.072 0.008 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.613\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.009 0.081 0.015 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.446\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.013 0.095 0.021 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.249\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.017 0.116 0.027 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.02\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.021 0.144 0.034 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.24\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.025 0.178 0.041 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.534\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.028 0.218 0.047 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.863\n",
            "output tensor([[8, 6]]) selected_action: tensor([0]) rew: -6.22801760125427\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 647 lasted for 24 time steps with total reward of 4.84613614938863\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.003 0.045 0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.771\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.007 0.052 0.013 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.615\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.011 0.065 0.019 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.429\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.015 0.084 0.025 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.213\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.019 0.109 0.032 output tensor([[14,  0]]) selected_action: tensor([0]) rew: -0.034\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.023 0.141 0.038 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.313\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.027 0.179 0.045 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.627\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.023 0.224 0.04 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.976\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: -6.073204614719581\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 648 lasted for 9 time steps with total reward of -5.994682598729138\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.004 -0.025 0.006 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.879\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.008 -0.02 0.011 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.738\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.004 -0.008 0.005 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.0 -0.003 -0.001 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.826\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.004 -0.003 -0.006 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 0.974\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.0 -0.01 -0.001 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.824\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.004 -0.01 -0.006 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.94\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.0 -0.017 -0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.789\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.004 -0.017 0.005 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.903\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.008 -0.012 0.011 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.79\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.012 -0.002 0.017 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.671\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.016 0.015 0.022 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.578\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.012 0.037 0.017 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.368\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.016 0.054 0.023 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 0.404\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.012 0.077 0.017 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.173\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.016 0.094 0.024 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.202\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.02 0.118 0.03 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.039\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.016 0.148 0.025 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.311\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.012 0.173 0.02 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.328\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.008 0.193 0.015 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.325\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.163 -0.012 0.208 0.022 output tensor([[10,  2]]) selected_action: tensor([0]) rew: -0.304\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.175 -0.016 0.23 0.029 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.551\n",
            "output tensor([[19,  0]]) selected_action: tensor([0]) rew: -5.832025245426659\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 649 lasted for 23 time steps with total reward of 2.990377422472876\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.003 -0.045 -0.006 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.007 -0.051 -0.012 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.639\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.011 -0.063 -0.018 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.457\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.007 -0.081 -0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.245\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.003 -0.094 -0.007 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.295\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.0 -0.101 -0.002 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.367\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.004 -0.103 0.003 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.462\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.008 -0.1 0.008 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.432\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.004 -0.092 0.002 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.317\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.0 -0.09 -0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.518\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.004 -0.095 -0.011 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 0.45\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.008 -0.106 -0.018 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.268\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.011 -0.124 -0.024 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.054\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.015 -0.148 -0.031 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.192\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.012 -0.178 -0.026 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.471\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.008 -0.204 -0.021 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.495\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.004 -0.225 -0.017 output tensor([[10,  1]]) selected_action: tensor([0]) rew: -0.502\n",
            "output tensor([[16,  2]]) selected_action: tensor([0]) rew: -5.491410610986569\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 650 lasted for 18 time steps with total reward of -1.865537663269862\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.003 0.013 0.006 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 0.933\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.007 0.02 0.012 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.783\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.003 0.032 0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.606\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.001 0.038 0.001 output tensor([[8, 9]]) selected_action: tensor([1]) rew: 0.692\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.003 0.039 0.007 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.803\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.007 0.045 0.013 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.648\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.011 0.058 0.019 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.463\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.015 0.077 0.025 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.249\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.011 0.102 0.02 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.003\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.015 0.122 0.026 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.017\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.019 0.148 0.033 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.239\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.023 0.181 0.04 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.529\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.019 0.221 0.035 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.853\n",
            "output tensor([[5, 5]]) selected_action: tensor([0]) rew: -5.9261849910007065\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 651 lasted for 14 time steps with total reward of -2.351570158573635\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.003 -0.042 0.005 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.789\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.001 -0.037 -0.001 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.005 -0.038 -0.007 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.798\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.009 -0.045 -0.013 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.641\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.012 -0.058 -0.019 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.455\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.009 -0.078 -0.014 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.239\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.012 -0.091 -0.02 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.284\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.009 -0.112 -0.015 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.06\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.013 -0.126 -0.021 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.096\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.009 -0.148 -0.016 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.138\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.013 -0.164 -0.023 output tensor([[1, 4]]) selected_action: tensor([1]) rew: -0.115\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 0.009 -0.187 -0.018 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.361\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.136 0.005 -0.206 -0.014 output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.352\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.14 0.001 -0.219 -0.009 output tensor([[14,  0]]) selected_action: tensor([0]) rew: -0.326\n",
            "output tensor([[4, 8]]) selected_action: tensor([1]) rew: -5.280727271456062\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 652 lasted for 15 time steps with total reward of -2.540925016575642\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.004 0.016 0.006 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.923\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.008 0.022 0.012 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.773\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.004 0.034 0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.595\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.008 0.04 0.012 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 0.681\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.004 0.052 0.007 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.499\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.0 0.059 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.58\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.004 0.061 0.007 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.686\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.008 0.068 0.014 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.524\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.012 0.082 0.02 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.333\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.016 0.102 0.026 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.111\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.02 0.128 0.033 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.143\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.024 0.161 0.039 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.43\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.116 -0.028 0.2 0.046 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.751\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.032 0.246 0.053 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -1.108\n",
            "output tensor([[16,  2]]) selected_action: tensor([0]) rew: -6.502061388513581\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 653 lasted for 15 time steps with total reward of -3.2305967769068555\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.005 0.018 0.007 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.897\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.001 0.025 0.001 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.744\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.003 0.026 -0.005 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.855\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.007 0.021 -0.01 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.761\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.003 0.011 -0.004 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.641\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.007 0.007 -0.01 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.84\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.003 -0.004 -0.004 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.716\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 -0.001 -0.008 0.002 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.877\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 -0.005 -0.006 0.007 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.923\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.008 0.001 0.013 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.785\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.012 0.014 0.019 output tensor([[13,  4]]) selected_action: tensor([0]) rew: 0.664\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.008 0.034 0.013 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.454\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.005 0.047 0.008 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.507\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.001 0.055 0.002 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.003 0.057 -0.003 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.686\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.007 0.053 -0.009 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.003 0.044 -0.003 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.524\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.007 0.042 -0.008 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.721\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.011 0.034 -0.014 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.595\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.007 0.02 -0.008 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.494\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.003 0.012 -0.002 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 0.712\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 -0.001 0.01 0.004 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.898\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.005 0.014 0.01 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.848\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 -0.001 0.024 0.004 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.68\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.003 0.029 -0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.776\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.007 0.027 -0.007 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.828\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.011 0.02 -0.013 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.693\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.015 0.007 -0.018 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.011 -0.011 -0.013 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.503\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 0.007 -0.024 -0.007 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.633\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.011 -0.03 -0.013 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.718\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.015 -0.043 -0.019 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 0.011 -0.062 -0.013 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.323\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 0.007 -0.075 -0.008 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.373\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.149 0.003 -0.083 -0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.446\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.152 0.007 -0.086 -0.009 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.542\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.16 0.011 -0.094 -0.015 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.372\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.171 0.007 -0.109 -0.01 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.171\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.178 0.003 -0.119 -0.005 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.229\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.181 -0.001 -0.124 0.0 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.31\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.18 0.003 -0.124 -0.006 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.401\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.184 0.007 -0.13 -0.013 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.25\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.191 0.011 -0.143 -0.02 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.055\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.202 0.007 -0.163 -0.015 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.172\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.21 0.011 -0.177 -0.021 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.143\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.221 0.015 -0.199 -0.028 output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.383\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.236 0.011 -0.227 -0.024 output tensor([[12,  1]]) selected_action: tensor([0]) rew: -0.656\n",
            "output tensor([[7, 4]]) selected_action: tensor([0]) rew: -5.678949582040444\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 654 lasted for 48 time steps with total reward of 18.75850074501325\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.003 -0.049 0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.754\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.001 -0.044 -0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.639\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.005 -0.045 -0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.76\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.001 -0.053 -0.002 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.602\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.003 -0.054 0.004 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.706\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.007 -0.051 0.009 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.011 -0.041 0.015 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.525\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.015 -0.026 0.02 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.432\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.011 -0.006 0.014 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.363\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.007 0.008 0.009 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.611\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.003 0.017 0.003 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 0.747\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.007 0.02 0.009 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.851\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.003 0.028 0.003 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.689\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 0.001 0.031 -0.003 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.791\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.003 0.029 0.003 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.783\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.007 0.032 0.009 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.011 0.041 0.015 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.613\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.007 0.057 0.01 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.003 0.067 0.004 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.007 0.071 0.011 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.574\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.003 0.082 0.005 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.396\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 0.001 0.087 -0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.48\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.003 0.087 0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.583\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.007 0.093 0.013 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.43\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.003 0.106 0.007 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.24\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.007 0.113 0.014 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.311\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -0.011 0.127 0.02 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.114\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.015 0.147 0.027 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.116\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.152 -0.011 0.174 0.022 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.378\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.163 -0.015 0.196 0.029 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.385\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.178 -0.011 0.225 0.024 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.662\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: -5.687283937413536\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 655 lasted for 32 time steps with total reward of 8.090376923696635\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.004 0.009 0.006 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 0.953\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.008 0.015 0.012 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.814\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.004 0.026 0.006 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.638\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.0 0.032 0.0 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 0.727\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.004 0.032 0.006 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.841\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.008 0.039 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.688\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.012 0.051 0.018 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 0.507\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.016 0.069 0.025 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.296\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.02 0.094 0.031 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.054\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.016 0.125 0.026 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.221\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.012 0.151 0.021 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.237\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.016 0.171 0.027 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.234\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.02 0.198 0.034 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.5\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.024 0.233 0.041 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.801\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: -6.1377407090448735\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 656 lasted for 15 time steps with total reward of -2.61382174328783\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.005 0.049 -0.006 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.739\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 0.009 0.043 -0.012 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.605\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.013 0.031 -0.018 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.498\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.016 0.013 -0.023 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.415\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.013 -0.01 -0.017 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.357\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.009 -0.027 -0.011 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.522\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.013 -0.039 -0.017 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.017 -0.056 -0.024 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.379\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.02 -0.08 -0.03 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.143\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.017 -0.109 -0.024 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.124\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 0.02 -0.134 -0.031 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.133\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.017 -0.165 -0.026 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.412\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 0.013 -0.191 -0.021 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.435\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 0.009 -0.212 -0.017 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.441\n",
            "output tensor([[4, 7]]) selected_action: tensor([1]) rew: -5.42835771444711\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 657 lasted for 15 time steps with total reward of -2.7295066942492565\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.004 0.024 -0.006 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.882\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.0 0.018 0.0 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.74\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.004 0.019 0.006 output tensor([[11,  4]]) selected_action: tensor([0]) rew: 0.906\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.008 0.025 0.012 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.755\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.012 0.037 0.018 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.577\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.008 0.055 0.013 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.368\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.012 0.068 0.019 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.422\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.015 0.087 0.025 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.207\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.019 0.112 0.031 output tensor([[9, 2]]) selected_action: tensor([0]) rew: -0.039\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.023 0.143 0.038 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.027 0.181 0.045 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.631\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.031 0.226 0.051 output tensor([[9, 1]]) selected_action: tensor([0]) rew: -0.978\n",
            "output tensor([[4, 5]]) selected_action: tensor([1]) rew: -6.362302027727811\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 658 lasted for 13 time steps with total reward of -3.470875105054394\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.003 -0.019 0.006 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.908\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.007 -0.013 0.011 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.767\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.003 -0.002 0.005 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 0.651\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.007 0.004 0.011 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.854\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.003 0.015 0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.7\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 0.001 0.02 -0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.003 0.02 0.006 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.896\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.007 0.026 0.012 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.761\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.011 0.037 0.018 output tensor([[16,  3]]) selected_action: tensor([0]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.007 0.055 0.012 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.378\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.011 0.067 0.018 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.434\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.015 0.086 0.025 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.222\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.019 0.11 0.031 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.022\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.015 0.141 0.026 output tensor([[3, 5]]) selected_action: tensor([1]) rew: -0.298\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.019 0.167 0.032 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -0.023 0.199 0.039 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.607\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.185 -0.027 0.238 0.046 output tensor([[5, 1]]) selected_action: tensor([0]) rew: -0.932\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -6.293075666597951\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 659 lasted for 18 time steps with total reward of -0.5219364755735363\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.003 -0.048 -0.006 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.766\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.007 -0.054 -0.012 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.011 -0.066 -0.018 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.44\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.007 -0.084 -0.013 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.228\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.003 -0.097 -0.008 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.276\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.007 -0.105 -0.014 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.347\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.003 -0.119 -0.009 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.15\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.007 -0.128 -0.015 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.212\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.011 -0.143 -0.022 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.006\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.015 -0.165 -0.029 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.232\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.019 -0.194 -0.035 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.504\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.015 -0.229 -0.031 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.811\n",
            "output tensor([[2, 7]]) selected_action: tensor([1]) rew: -5.866995594117459\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 660 lasted for 13 time steps with total reward of -4.3671278273877085\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.004 0.017 -0.006 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.91\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.0 0.01 -0.0 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.768\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 -0.004 0.01 0.006 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.946\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.008 0.016 0.012 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.806\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.004 0.028 0.006 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.631\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.008 0.034 0.012 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.719\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.011 0.046 0.018 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.015 0.064 0.024 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.331\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.019 0.088 0.03 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.092\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.023 0.118 0.037 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.179\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.027 0.155 0.043 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.484\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.031 0.198 0.05 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.823\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.027 0.248 0.046 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -1.198\n",
            "output tensor([[5, 7]]) selected_action: tensor([1]) rew: -6.323866917593061\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 661 lasted for 14 time steps with total reward of -3.2648878371826253\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.004 -0.039 0.007 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.783\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.008 -0.033 0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.648\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.004 -0.021 0.006 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.539\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.0 -0.014 0.0 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.749\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.004 -0.014 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.928\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.0 -0.02 -0.0 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.786\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.004 -0.02 -0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.902\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.0 -0.026 -0.0 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.752\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.004 -0.027 0.005 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.867\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.008 -0.021 0.011 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.739\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.012 -0.01 0.017 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.015 0.007 0.023 output tensor([[19,  2]]) selected_action: tensor([0]) rew: 0.532\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.019 0.029 0.028 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.404\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.023 0.058 0.034 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.149\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.027 0.092 0.041 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.138\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.031 0.133 0.047 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.457\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.027 0.18 0.042 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.811\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.031 0.222 0.049 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.912\n",
            "output tensor([[3, 3]]) selected_action: tensor([0]) rew: -6.2840142854952346\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 662 lasted for 19 time steps with total reward of 0.7992596949113508\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.004 0.015 0.007 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.909\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.008 0.022 0.013 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.756\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.004 0.035 0.007 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.574\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.0 0.042 0.002 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.655\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.004 0.043 0.008 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.762\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.008 0.051 0.014 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.602\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.012 0.065 0.02 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.412\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.008 0.085 0.015 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.192\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.004 0.099 0.009 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.233\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.008 0.108 0.016 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.296\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.012 0.124 0.022 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.091\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.016 0.146 0.029 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.146\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.012 0.175 0.024 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.008 0.199 0.019 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.431\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.004 0.218 0.015 output tensor([[1, 9]]) selected_action: tensor([1]) rew: -0.429\n",
            "output tensor([[9, 6]]) selected_action: tensor([0]) rew: -5.408100173322761\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 663 lasted for 16 time steps with total reward of -1.3459239497881184\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.004 0.005 0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.97\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.008 0.01 0.012 output tensor([[18,  4]]) selected_action: tensor([0]) rew: 0.836\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.011 0.022 0.017 output tensor([[20,  1]]) selected_action: tensor([0]) rew: 0.662\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.015 0.039 0.023 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.459\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.011 0.063 0.018 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.226\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.015 0.081 0.024 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.254\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.019 0.105 0.03 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.013\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.023 0.135 0.037 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.261\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.019 0.172 0.032 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.015 0.204 0.027 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.62\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.012 0.231 0.023 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.655\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: -5.6736729883396375\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 664 lasted for 12 time steps with total reward of -4.3575032391036945\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.004 0.031 -0.005 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.844\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.001 0.026 0.001 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.718\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.004 0.027 -0.005 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.858\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.001 0.022 0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.749\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.003 0.023 0.007 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.87\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.007 0.03 0.013 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.003 0.043 0.007 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.531\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.001 0.05 0.002 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.611\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.004 0.052 -0.004 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.001 0.048 0.002 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.658\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.003 0.051 0.009 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.708\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.007 0.059 0.015 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.542\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.011 0.074 0.021 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.347\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.015 0.095 0.027 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.121\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.011 0.122 0.022 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.137\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.007 0.145 0.017 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.136\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.011 0.162 0.024 output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.115\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.008 0.185 0.019 output tensor([[1, 3]]) selected_action: tensor([1]) rew: -0.364\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.004 0.204 0.014 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.358\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 0.0 0.219 0.01 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.334\n",
            "output tensor([[9, 1]]) selected_action: tensor([0]) rew: -5.291443097433966\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 665 lasted for 21 time steps with total reward of 2.2503654844769114\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.003 -0.002 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.982\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.0 -0.008 -0.001 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.833\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.004 -0.009 0.005 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 0.948\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.008 -0.003 0.011 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.828\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.004 0.008 0.005 output tensor([[5, 9]]) selected_action: tensor([1]) rew: 0.708\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.008 0.013 0.011 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.833\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.004 0.024 0.005 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.661\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.0 0.029 -0.0 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.752\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.004 0.029 0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.85\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.001 0.035 -0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.721\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.004 0.035 0.006 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.834\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.001 0.041 0.0 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.684\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 0.003 0.041 -0.005 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.796\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.007 0.036 -0.011 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.675\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.003 0.025 -0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.56\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.007 0.02 -0.01 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.763\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.011 0.01 -0.016 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.643\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.015 -0.006 -0.022 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.549\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.019 -0.028 -0.028 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.424\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.015 -0.056 -0.022 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.172\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.019 -0.078 -0.028 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.181\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.015 -0.106 -0.023 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.079\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.019 -0.129 -0.029 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.081\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.015 -0.159 -0.024 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.352\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 0.019 -0.183 -0.031 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.368\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.128 0.015 -0.214 -0.027 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.654\n",
            "output tensor([[9, 6]]) selected_action: tensor([0]) rew: -5.687555089715991\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 666 lasted for 27 time steps with total reward of 7.172634980160405\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.004 0.043 0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.0 0.048 0.0 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.004 0.048 -0.006 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.769\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.0 0.043 0.001 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.631\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.004 0.043 -0.005 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.0 0.038 0.001 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.004 0.04 -0.004 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.788\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.008 0.035 -0.01 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.7\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.012 0.025 -0.016 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 0.581\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.008 0.009 -0.01 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.004 -0.0 -0.004 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.713\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.0 -0.004 0.002 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.905\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.004 -0.002 -0.004 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.928\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.008 -0.006 -0.01 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.896\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.012 -0.015 -0.016 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.732\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.008 -0.031 -0.01 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.538\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.004 -0.041 -0.004 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.608\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.008 -0.045 -0.01 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.703\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.004 -0.055 -0.005 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.531\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.0 -0.06 0.001 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 -0.004 -0.059 0.006 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.694\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 -0.008 -0.053 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.562\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 -0.004 -0.041 0.006 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.455\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 -0.008 -0.035 0.011 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.666\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 -0.004 -0.024 0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.553\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.0 -0.019 -0.001 output tensor([[5, 9]]) selected_action: tensor([1]) rew: 0.758\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.004 -0.02 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.888\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.008 -0.027 -0.013 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.734\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.012 -0.039 -0.019 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.552\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.008 -0.058 -0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.341\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 0.012 -0.072 -0.019 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.391\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 0.016 -0.091 -0.026 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.172\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 0.02 -0.117 -0.032 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.078\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.156 0.016 -0.149 -0.027 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.361\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.172 0.012 -0.176 -0.022 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.387\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.185 0.008 -0.198 -0.018 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.395\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.193 0.004 -0.216 -0.013 output tensor([[11,  1]]) selected_action: tensor([0]) rew: -0.385\n",
            "output tensor([[10,  2]]) selected_action: tensor([0]) rew: -5.3562367756972655\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 667 lasted for 38 time steps with total reward of 13.820905361777378\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.004 -0.033 -0.007 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.827\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.008 -0.04 -0.013 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.672\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.011 -0.053 -0.019 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.015 -0.072 -0.025 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.274\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.019 -0.097 -0.031 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.03\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.015 -0.128 -0.026 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.248\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.012 -0.154 -0.021 output tensor([[9, 4]]) selected_action: tensor([0]) rew: -0.267\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.008 -0.176 -0.016 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.267\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 0.004 -0.192 -0.012 output tensor([[5, 1]]) selected_action: tensor([0]) rew: -0.248\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.125 0.008 -0.204 -0.019 output tensor([[7, 8]]) selected_action: tensor([1]) rew: -0.21\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.132 0.004 -0.222 -0.014 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.439\n",
            "output tensor([[5, 2]]) selected_action: tensor([0]) rew: -5.416952984558003\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 668 lasted for 12 time steps with total reward of -4.804615358454658\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.003 0.024 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.865\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.007 0.03 0.011 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.748\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.011 0.041 0.017 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.572\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.015 0.059 0.024 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.367\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.019 0.082 0.03 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.131\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.015 0.112 0.024 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.137\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.011 0.136 0.019 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.146\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.015 0.156 0.026 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.135\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.019 0.182 0.033 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.393\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.023 0.214 0.04 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.686\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -6.013456677638055\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 669 lasted for 11 time steps with total reward of -4.8259328341469505\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.003 0.001 0.006 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.993\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.007 0.007 0.012 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.846\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.003 0.019 0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.671\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.007 0.025 0.012 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.76\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.011 0.037 0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.581\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.007 0.055 0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.374\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.011 0.067 0.019 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.428\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.015 0.086 0.025 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.214\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.019 0.111 0.031 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.031\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.023 0.142 0.038 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.309\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.019 0.18 0.033 output tensor([[1, 3]]) selected_action: tensor([1]) rew: -0.62\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -0.015 0.212 0.028 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.678\n",
            "output tensor([[7, 7]]) selected_action: tensor([0]) rew: -5.718432359389864\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 670 lasted for 13 time steps with total reward of -2.488652650915097\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.005 -0.009 -0.005 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.933\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.008 -0.014 -0.011 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.012 -0.025 -0.017 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.657\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.008 -0.042 -0.011 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.456\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.012 -0.053 -0.017 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.517\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.016 -0.071 -0.024 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.31\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.02 -0.094 -0.03 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.073\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.016 -0.124 -0.025 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.197\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.02 -0.149 -0.031 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.208\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.016 -0.18 -0.026 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.49\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.02 -0.206 -0.033 output tensor([[4, 9]]) selected_action: tensor([1]) rew: -0.518\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.024 -0.239 -0.04 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: -0.815\n",
            "output tensor([[2, 7]]) selected_action: tensor([1]) rew: -6.147970746165633\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 671 lasted for 13 time steps with total reward of -4.599650543890027\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.003 -0.04 -0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.803\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.007 -0.047 -0.012 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.65\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.011 -0.059 -0.019 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 0.467\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.007 -0.078 -0.013 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.254\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.003 -0.091 -0.008 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.302\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 -0.001 -0.098 -0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.374\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 -0.005 -0.101 0.003 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.468\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 -0.009 -0.098 0.008 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.45\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.012 -0.09 0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.333\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.008 -0.077 0.007 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.241\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.012 -0.07 0.012 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.463\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.008 -0.058 0.006 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.362\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.012 -0.053 0.011 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.576\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.016 -0.041 0.017 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.467\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.02 -0.024 0.022 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.383\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.024 -0.002 0.028 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.324\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.028 0.026 0.034 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.288\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.024 0.06 0.028 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.028\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.02 0.088 0.023 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.008\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.024 0.111 0.029 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.01\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.15 -0.028 0.14 0.036 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.258\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.178 -0.024 0.176 0.031 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.56\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.202 -0.02 0.206 0.026 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.607\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.222 -0.024 0.232 0.033 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.637\n",
            "output tensor([[4, 5]]) selected_action: tensor([1]) rew: -5.9360043001002\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 672 lasted for 25 time steps with total reward of -0.744370020412517\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.004 -0.048 0.005 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.755\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.008 -0.043 0.01 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.651\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.004 -0.033 0.004 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.0 -0.029 -0.002 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 0.737\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.004 -0.031 0.004 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.0 -0.027 -0.002 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.758\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.004 -0.029 -0.008 output tensor([[ 1, 19]]) selected_action: tensor([1]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.008 -0.037 -0.014 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.655\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.011 -0.052 -0.02 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.464\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.008 -0.072 -0.015 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.244\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.004 -0.087 -0.009 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.285\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.008 -0.096 -0.016 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.349\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.004 -0.112 -0.011 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.144\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.008 -0.123 -0.017 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.199\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.012 -0.14 -0.024 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.015\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.016 -0.164 -0.03 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.261\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.012 -0.194 -0.026 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.541\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.008 -0.22 -0.021 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.567\n",
            "output tensor([[7, 7]]) selected_action: tensor([0]) rew: -5.576458207044795\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 673 lasted for 19 time steps with total reward of 0.4466813580247768\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.003 0.015 0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.907\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.007 0.02 0.011 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.801\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.011 0.031 0.017 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 0.627\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.015 0.048 0.023 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.425\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.019 0.071 0.029 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.192\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.015 0.1 0.024 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.072\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.019 0.124 0.03 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.077\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.015 0.155 0.025 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.352\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.019 0.18 0.032 output tensor([[9, 4]]) selected_action: tensor([0]) rew: -0.372\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.023 0.212 0.039 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.661\n",
            "output tensor([[4, 7]]) selected_action: tensor([1]) rew: -5.9853565758494724\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 674 lasted for 11 time steps with total reward of -4.567750957678244\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.004 -0.037 -0.006 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.82\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.008 -0.043 -0.012 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 0.671\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.004 -0.056 -0.007 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.49\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.0 -0.062 -0.001 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.571\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.004 -0.063 -0.007 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.677\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.0 -0.07 -0.002 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.517\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.004 -0.072 0.004 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.618\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.0 -0.069 -0.003 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.566\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.004 -0.071 -0.009 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.604\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.0 -0.08 -0.004 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.434\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.003 -0.084 0.002 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.526\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.007 -0.082 0.007 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.556\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.011 -0.075 0.012 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.432\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.007 -0.063 0.006 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.332\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.011 -0.057 0.011 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.549\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.015 -0.046 0.017 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.442\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.019 -0.029 0.023 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.359\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.015 -0.006 0.016 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.301\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.019 0.01 0.022 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.559\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.015 0.033 0.016 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.393\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.019 0.049 0.023 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.432\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.015 0.072 0.017 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.201\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -0.019 0.089 0.023 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.232\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -0.023 0.112 0.03 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.006\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.181 -0.019 0.142 0.025 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.277\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.2 -0.015 0.166 0.02 output tensor([[2, 9]]) selected_action: tensor([1]) rew: -0.291\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.215 -0.019 0.186 0.026 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.286\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.234 -0.023 0.212 0.033 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.55\n",
            "output tensor([[5, 4]]) selected_action: tensor([0]) rew: -5.848351548659746\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 675 lasted for 29 time steps with total reward of 4.023564836872188\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 0.005 -0.021 -0.007 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.884\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.009 -0.028 -0.013 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.732\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.013 -0.041 -0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.55\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.017 -0.059 -0.025 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.339\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.02 -0.084 -0.031 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.097\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.024 -0.115 -0.037 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.176\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.028 -0.152 -0.044 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.032 -0.196 -0.051 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.824\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 0.036 -0.247 -0.057 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -1.202\n",
            "output tensor([[9, 2]]) selected_action: tensor([0]) rew: -6.615893628095559\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 676 lasted for 10 time steps with total reward of -6.698087471113961\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.003 0.006 -0.006 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.97\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.007 -0.0 -0.012 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.825\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.003 -0.012 -0.006 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.705\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.0 -0.018 -0.0 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.795\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.003 -0.018 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.912\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.007 -0.024 -0.012 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 0.762\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.003 -0.036 -0.006 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.007 -0.042 -0.012 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.011 -0.055 -0.019 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.487\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.015 -0.073 -0.025 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.274\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.011 -0.098 -0.019 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.031\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 0.015 -0.118 -0.026 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.048\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 0.011 -0.143 -0.021 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.129 0.008 -0.164 -0.016 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.202\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 0.004 -0.18 -0.011 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.179\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.141 0.008 -0.191 -0.018 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -0.136\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.148 0.012 -0.209 -0.025 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.36\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.16 0.016 -0.234 -0.032 output tensor([[3, 8]]) selected_action: tensor([1]) rew: -0.619\n",
            "output tensor([[8, 4]]) selected_action: tensor([0]) rew: -5.912940594979323\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 677 lasted for 19 time steps with total reward of -0.5522440003905036\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.003 0.028 0.006 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.854\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.0 0.034 -0.0 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.725\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.004 0.034 -0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.838\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 0.008 0.028 -0.011 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.697\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.012 0.017 -0.017 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.016 -0.0 -0.023 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.496\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.02 -0.023 -0.029 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 0.429\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.016 -0.052 -0.023 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.174\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.012 -0.075 -0.017 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.181\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.008 -0.092 -0.012 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.209\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.012 -0.104 -0.018 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.26\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.008 -0.122 -0.013 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.043\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.084 0.004 -0.136 -0.008 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.084\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.008 -0.144 -0.015 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.147\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 0.004 -0.159 -0.01 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.059\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 0.008 -0.169 -0.017 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 0.005 -0.185 -0.012 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.224\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.001 -0.198 -0.007 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.186\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 -0.003 -0.205 -0.003 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.129\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 -0.007 -0.208 0.001 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.053\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 -0.003 -0.206 -0.006 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.029\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.001 -0.212 -0.013 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.124\n",
            "output tensor([[7, 5]]) selected_action: tensor([0]) rew: -5.325643277094291\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 678 lasted for 23 time steps with total reward of -0.4153946295889206\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.003 -0.022 -0.007 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.876\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.007 -0.029 -0.013 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.722\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.011 -0.042 -0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.539\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.007 -0.061 -0.013 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.327\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.003 -0.074 -0.008 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.376\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.001 -0.082 -0.003 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.448\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.005 -0.085 0.003 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.545\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.001 -0.082 -0.004 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.527\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.003 -0.085 -0.01 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.52\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.007 -0.095 -0.016 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.344\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.003 -0.112 -0.011 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.137\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.007 -0.123 -0.018 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.19\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.003 -0.14 -0.013 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.026\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.007 -0.153 -0.019 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.016\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.011 -0.172 -0.026 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.211\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.015 -0.198 -0.033 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.471\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.011 -0.231 -0.028 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.766\n",
            "output tensor([[5, 2]]) selected_action: tensor([0]) rew: -5.809716539740283\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 679 lasted for 18 time steps with total reward of -1.7162680775504615\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.003 0.017 0.007 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.902\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.007 0.023 0.013 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 0.749\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.003 0.036 0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.007 0.043 0.013 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.649\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.003 0.057 0.008 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.463\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.001 0.064 0.002 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.003 0.066 0.008 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.64\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.007 0.075 0.015 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.474\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.011 0.089 0.021 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.278\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.015 0.11 0.027 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.05\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.019 0.138 0.034 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.209\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.015 0.171 0.029 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.502\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.019 0.2 0.036 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.023 0.236 0.043 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.849\n",
            "output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: -6.192781177122145\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 680 lasted for 15 time steps with total reward of -2.9789926268663227\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.003 -0.015 -0.007 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.914\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.001 -0.022 -0.001 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.762\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.005 -0.022 0.005 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.874\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.009 -0.018 0.011 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.772\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.005 -0.007 0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.653\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.001 -0.003 -0.001 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.852\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.005 -0.004 0.004 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.954\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.009 0.001 0.01 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.869\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.012 0.011 0.016 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.739\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.016 0.027 0.022 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 0.544\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.02 0.049 0.028 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.024 0.077 0.034 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.062\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.02 0.112 0.029 output tensor([[1, 4]]) selected_action: tensor([1]) rew: -0.226\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.157 -0.016 0.14 0.024 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.256\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -0.013 0.164 0.019 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.266\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.186 -0.009 0.183 0.014 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.257\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.195 -0.013 0.197 0.021 output tensor([[9, 1]]) selected_action: tensor([0]) rew: -0.228\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.207 -0.017 0.218 0.028 output tensor([[10,  1]]) selected_action: tensor([0]) rew: -0.468\n",
            "output tensor([[3, 6]]) selected_action: tensor([1]) rew: -5.742401507666588\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 681 lasted for 19 time steps with total reward of 0.8713118603533268\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.005 -0.027 0.007 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.843\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.008 -0.02 0.012 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.706\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.012 -0.008 0.018 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.596\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.016 0.01 0.024 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.511\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.012 0.034 0.018 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.353\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.016 0.052 0.024 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.384\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.02 0.076 0.03 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.146\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.116 -0.016 0.107 0.025 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.123\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.02 0.132 0.031 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.134\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.153 -0.016 0.163 0.026 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.169 -0.02 0.19 0.033 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.441\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.189 -0.016 0.223 0.029 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.737\n",
            "output tensor([[4, 5]]) selected_action: tensor([1]) rew: -5.78212547123476\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 682 lasted for 13 time steps with total reward of -4.095131271992139\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.004 0.03 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.83\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.008 0.023 -0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.693\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.012 0.011 -0.018 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.583\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.016 -0.007 -0.024 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.499\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.012 -0.031 -0.018 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.372\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.008 -0.049 -0.012 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.404\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.012 -0.061 -0.018 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.459\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.016 -0.08 -0.025 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.247\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.02 -0.104 -0.031 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.003\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.016 -0.135 -0.026 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.273\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.012 -0.161 -0.021 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.292\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 0.008 -0.182 -0.016 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.291\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 0.004 -0.198 -0.012 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.272\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.14 0.0 -0.21 -0.007 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.234\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: -5.176679296087688\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 683 lasted for 15 time steps with total reward of -2.448190341752264\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.005 0.006 -0.005 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.958\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.001 0.001 0.001 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.847\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.003 0.001 0.007 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 0.977\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.001 0.008 0.001 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.827\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.005 0.009 -0.005 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.941\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.009 0.004 -0.011 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.833\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.005 -0.007 -0.005 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.712\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.009 -0.012 -0.011 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.845\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.005 -0.023 -0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.674\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.001 -0.028 0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.767\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.005 -0.027 -0.005 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.851\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.001 -0.032 0.0 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.738\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.003 -0.032 0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.837\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.001 -0.026 -0.0 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.698\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.003 -0.026 0.006 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 0.875\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.007 -0.02 0.011 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.735\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.003 -0.009 0.005 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.62\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.007 -0.004 0.011 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.824\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.011 0.008 0.017 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.704\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.015 0.024 0.023 output tensor([[19,  3]]) selected_action: tensor([0]) rew: 0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.019 0.047 0.029 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.311\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.015 0.076 0.023 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.052\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.011 0.1 0.018 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.053\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.015 0.118 0.024 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.011 0.142 0.019 output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.172\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.015 0.161 0.026 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.161\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.011 0.187 0.021 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.421\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.007 0.209 0.017 output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.426\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.003 0.225 0.012 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.413\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: -5.382335256201158\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 684 lasted for 30 time steps with total reward of 9.319268747016\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.004 -0.023 -0.006 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.882\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.0 -0.029 -0.001 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.004 -0.03 0.005 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.842\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.008 -0.025 0.011 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.735\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.011 -0.014 0.016 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.616\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.008 0.002 0.01 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.525\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.011 0.012 0.016 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.015 0.028 0.022 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.537\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.011 0.051 0.016 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.311\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.008 0.067 0.011 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.348\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.004 0.078 0.006 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.407\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 0.0 0.083 0.0 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.49\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.004 0.084 0.007 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.598\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.008 0.09 0.013 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.438\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.012 0.103 0.019 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.248\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.008 0.122 0.014 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.027\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.012 0.136 0.021 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.064\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.008 0.157 0.016 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.168\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.004 0.173 0.011 output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.143\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -0.008 0.184 0.018 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.098\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -0.012 0.201 0.025 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.321\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.154 -0.016 0.226 0.032 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.578\n",
            "output tensor([[10,  3]]) selected_action: tensor([0]) rew: -5.870082386815387\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 685 lasted for 23 time steps with total reward of 1.3534108787314354\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.003 -0.013 0.006 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.936\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.0 -0.007 -0.0 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.003 -0.007 0.006 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.963\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.007 -0.002 0.012 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.823\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.011 0.01 0.017 output tensor([[16,  5]]) selected_action: tensor([0]) rew: 0.705\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.015 0.027 0.023 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.519\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.011 0.051 0.018 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.288\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.015 0.068 0.024 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.319\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.019 0.092 0.03 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.081\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.023 0.122 0.036 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.189\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.019 0.158 0.031 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.493\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.023 0.19 0.038 output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.027 0.228 0.045 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.859\n",
            "output tensor([[8, 4]]) selected_action: tensor([0]) rew: -6.213449841553847\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 686 lasted for 14 time steps with total reward of -2.869677161368868\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.005 -0.031 -0.007 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.837\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.001 -0.038 -0.001 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 0.682\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.003 -0.039 0.005 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.007 -0.035 0.01 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.7\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.011 -0.024 0.016 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.581\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.007 -0.009 0.01 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.489\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.003 0.001 0.004 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.714\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.007 0.005 0.01 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.898\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.011 0.015 0.016 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.007 0.03 0.01 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.539\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.003 0.04 0.004 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 0.001 0.044 -0.001 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.703\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 0.005 0.043 -0.007 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.753\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 0.001 0.036 -0.001 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.003 0.035 0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.807\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.007 0.04 0.011 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.702\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.011 0.052 0.017 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.525\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.007 0.069 0.012 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -0.011 0.081 0.018 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.373\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.007 0.099 0.013 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.16\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.011 0.112 0.019 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.146 -0.015 0.131 0.026 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.016\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -0.019 0.157 0.032 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.271\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.18 -0.023 0.189 0.039 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.559\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.203 -0.019 0.228 0.035 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.883\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: -5.95519933175884\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 687 lasted for 26 time steps with total reward of 5.058907451715938\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.005 0.044 0.007 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.777\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.009 0.051 0.013 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.005 0.064 0.007 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.009 0.071 0.014 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.513\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.013 0.085 0.02 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.322\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.016 0.104 0.026 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.1\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.013 0.131 0.021 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.154\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.009 0.152 0.016 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.15\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.005 0.168 0.011 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.126\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.009 0.179 0.018 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.082\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.005 0.197 0.013 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.306\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.14 -0.009 0.21 0.02 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.276\n",
            "output tensor([[16,  0]]) selected_action: tensor([0]) rew: -5.51420534947691\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 688 lasted for 13 time steps with total reward of -3.8385430878435933\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.005 0.032 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.817\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.001 0.026 -0.001 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.681\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.003 0.025 0.005 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.864\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.007 0.03 0.011 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.745\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.011 0.042 0.017 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.569\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.007 0.059 0.012 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.363\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.003 0.071 0.006 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.419\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.007 0.078 0.013 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 0.499\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.011 0.09 0.019 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.311\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.015 0.109 0.025 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.093\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.011 0.135 0.02 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.158\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.015 0.155 0.027 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.151\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.011 0.182 0.022 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.414\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.007 0.204 0.018 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.422\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.003 0.222 0.013 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: -0.413\n",
            "output tensor([[3, 4]]) selected_action: tensor([1]) rew: -5.385397666405689\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 689 lasted for 16 time steps with total reward of -1.5823215720356458\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.003 0.034 0.006 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.835\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.007 0.04 0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.69\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.011 0.052 0.018 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.51\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.015 0.07 0.024 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.301\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.011 0.094 0.019 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.06\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.015 0.113 0.025 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.079\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.011 0.138 0.02 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.171\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.007 0.158 0.015 output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.165\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.011 0.174 0.022 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.138\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.015 0.196 0.029 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.38\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.019 0.225 0.036 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.656\n",
            "output tensor([[14,  1]]) selected_action: tensor([0]) rew: -5.966727054376689\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 690 lasted for 12 time steps with total reward of -5.000576767792506\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.005 -0.009 0.006 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 0.95\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.008 -0.002 0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.807\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.012 0.009 0.018 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.691\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.016 0.027 0.024 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.512\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.02 0.051 0.03 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.279\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.024 0.08 0.036 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.016\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -0.02 0.116 0.03 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.28\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.024 0.147 0.037 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.169 -0.02 0.184 0.032 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.626\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.19 -0.016 0.216 0.028 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.681\n",
            "output tensor([[3, 8]]) selected_action: tensor([1]) rew: -5.7197737394179\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 691 lasted for 11 time steps with total reward of -4.370300166022759\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.004 -0.033 0.005 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.838\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.008 -0.027 0.011 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.709\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.012 -0.016 0.017 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.593\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.016 0.0 0.022 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.504\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.02 0.023 0.028 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 0.437\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.023 0.051 0.034 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.184\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.02 0.085 0.029 output tensor([[3, 5]]) selected_action: tensor([1]) rew: -0.101\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -0.023 0.114 0.035 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.127\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.166 -0.02 0.149 0.03 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.423\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.185 -0.016 0.179 0.025 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.463\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.201 -0.02 0.204 0.032 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.486\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.221 -0.024 0.237 0.039 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.777\n",
            "output tensor([[14,  1]]) selected_action: tensor([0]) rew: -6.104686626552503\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 692 lasted for 13 time steps with total reward of -5.216918866196573\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 0.005 -0.026 -0.006 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.876\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.009 -0.032 -0.012 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.725\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.012 -0.044 -0.018 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.009 -0.062 -0.013 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.337\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.005 -0.075 -0.007 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.389\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.001 -0.082 -0.002 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.465\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.005 -0.084 -0.008 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.565\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.001 -0.092 -0.003 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.398\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.003 -0.094 0.002 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.492\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.001 -0.092 -0.004 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.003 -0.096 0.001 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.461\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.001 -0.095 -0.005 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.51\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.003 -0.1 0.0 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.418\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.007 -0.1 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.521\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.011 -0.095 0.01 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.392\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.015 -0.084 0.016 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.287\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.011 -0.069 0.009 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.007 -0.059 0.003 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.44\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.003 -0.056 -0.003 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.641\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.007 -0.059 0.002 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.652\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.003 -0.057 -0.004 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.659\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.007 -0.061 0.002 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.63\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.011 -0.059 0.007 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.007 -0.052 0.001 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.541\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.01 -0.052 0.006 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.007 -0.045 0.0 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.596\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -0.01 -0.045 0.006 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.007 -0.039 -0.0 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.642\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.003 -0.04 -0.007 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.802\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 0.001 -0.046 -0.013 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.648\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.003 -0.059 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.464\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 0.001 -0.066 -0.013 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.543\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 0.005 -0.079 -0.02 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.354\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 0.009 -0.099 -0.026 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.134\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 0.005 -0.125 -0.021 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.117\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 0.009 -0.145 -0.027 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.111\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 0.013 -0.172 -0.034 output tensor([[2, 8]]) selected_action: tensor([1]) rew: -0.374\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 0.017 -0.206 -0.041 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: -0.671\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.021 -0.247 -0.048 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: -1.003\n",
            "output tensor([[5, 2]]) selected_action: tensor([0]) rew: -6.372196356678836\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 693 lasted for 40 time steps with total reward of 9.351740377017245\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.005 -0.036 -0.006 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.824\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.009 -0.042 -0.012 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.671\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.005 -0.055 -0.007 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.489\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.009 -0.061 -0.013 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.569\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.005 -0.074 -0.008 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.382\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.001 -0.082 -0.002 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.457\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 -0.003 -0.084 0.003 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.555\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 -0.007 -0.081 0.008 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.52\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 -0.011 -0.072 0.014 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.402\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.007 -0.059 0.007 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.309\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.003 -0.051 0.001 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.533\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.001 -0.05 -0.005 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.724\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.003 -0.055 0.001 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.639\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.007 -0.054 0.006 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.722\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.011 -0.048 0.012 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.588\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.014 -0.036 0.017 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.48\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.018 -0.019 0.023 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.397\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.014 0.004 0.017 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.338\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.011 0.02 0.011 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.562\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.014 0.031 0.017 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.018 0.048 0.023 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.426\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.022 0.071 0.029 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.193\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.026 0.101 0.035 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.07\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.03 0.136 0.042 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.367\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -0.034 0.178 0.049 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.697\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.175 -0.03 0.226 0.044 output tensor([[0, 9]]) selected_action: tensor([1]) rew: -1.063\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: -6.1779876570916645\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 694 lasted for 27 time steps with total reward of 3.031483826766735\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.004 -0.033 0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.821\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.0 -0.027 0.0 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.683\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.003 -0.026 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.865\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.007 -0.02 0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.725\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.003 -0.009 0.006 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.611\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.007 -0.003 0.011 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.011 0.009 0.017 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.699\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.007 0.026 0.012 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.526\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.003 0.037 0.006 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.589\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 0.0 0.043 0.0 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.676\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.004 0.043 0.006 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.788\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.007 0.05 0.012 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.634\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.011 0.062 0.019 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.451\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.015 0.081 0.025 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.237\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.011 0.106 0.02 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.015 0.125 0.026 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -0.019 0.151 0.033 output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.248\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.17 -0.015 0.184 0.028 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.537\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.186 -0.019 0.212 0.035 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.571\n",
            "output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: -5.8759669843019005\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 695 lasted for 20 time steps with total reward of 1.8883334365391597\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.003 0.034 0.007 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.823\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.001 0.041 0.001 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.668\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.003 0.042 0.007 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.777\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.001 0.049 0.002 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.619\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.003 0.051 0.008 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.724\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.001 0.059 0.002 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.563\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.003 0.061 0.008 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.664\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.007 0.069 0.015 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.498\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.003 0.084 0.009 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.302\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.007 0.093 0.016 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.367\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.011 0.109 0.022 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.164\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.015 0.131 0.029 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.071\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.019 0.159 0.035 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.338\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.023 0.195 0.042 output tensor([[1, 1]]) selected_action: tensor([0]) rew: -0.64\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.027 0.236 0.049 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.976\n",
            "output tensor([[20,  0]]) selected_action: tensor([0]) rew: -6.349312578739854\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 696 lasted for 16 time steps with total reward of -2.2038702810725113\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.003 0.022 0.006 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.888\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.001 0.028 0.001 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.736\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.003 0.029 0.007 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.848\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.007 0.036 0.013 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.695\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.011 0.048 0.019 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.512\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.015 0.067 0.025 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.299\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.019 0.092 0.031 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.056\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.023 0.123 0.038 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.22\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.019 0.161 0.033 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.529\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.023 0.193 0.039 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.582\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.019 0.233 0.035 output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.907\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: -5.981201524987737\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 697 lasted for 12 time steps with total reward of -4.186110714948731\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.004 0.014 0.005 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.905\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.0 0.019 -0.001 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 0.808\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.004 0.018 -0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.89\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.008 0.012 -0.012 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.751\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.004 -0.0 -0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.638\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.0 -0.007 -0.0 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.84\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.004 -0.007 0.005 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.956\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.0 -0.002 -0.001 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 0.832\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.004 -0.002 0.005 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.978\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.0 0.003 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.856\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.004 0.002 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.972\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.007 0.008 0.011 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.856\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.004 0.019 0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.684\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.0 0.024 -0.0 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.776\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.004 0.024 0.006 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.875\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.007 0.029 0.012 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.745\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.004 0.041 0.006 output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.0 0.047 0.0 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.654\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.004 0.047 -0.005 output tensor([[6, 9]]) selected_action: tensor([1]) rew: 0.765\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.008 0.042 -0.011 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.644\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.004 0.032 -0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.53\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.008 0.027 -0.01 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.734\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.004 0.017 -0.004 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.615\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.008 0.013 -0.01 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.814\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.004 0.003 -0.004 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.691\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.0 -0.001 0.002 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.887\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.004 0.0 -0.004 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.947\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.008 -0.004 -0.01 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.897\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.012 -0.014 -0.016 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 0.736\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.016 -0.029 -0.022 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.542\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.02 -0.051 -0.028 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.016 -0.079 -0.022 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.064\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.012 -0.101 -0.017 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.069\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 0.008 -0.118 -0.012 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.097\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.012 -0.129 -0.018 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.146\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.125 0.016 -0.148 -0.025 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.073\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.141 0.02 -0.172 -0.032 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.326\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.161 0.024 -0.204 -0.038 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.612\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.185 0.028 -0.242 -0.045 output tensor([[1, 6]]) selected_action: tensor([1]) rew: -0.933\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: -6.2902245935533845\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 698 lasted for 40 time steps with total reward of 15.846944390647009\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.003 0.015 -0.006 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.907\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.0 0.008 -0.001 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.767\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.004 0.008 0.005 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.947\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.0 0.013 -0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.828\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.003 0.013 -0.006 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.926\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.0 0.007 -0.0 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.784\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.003 0.006 -0.006 output tensor([[7, 9]]) selected_action: tensor([1]) rew: 0.962\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.007 0.0 -0.012 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.003 -0.011 -0.006 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.701\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.0 -0.018 -0.0 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.795\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.004 -0.018 0.006 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.91\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.0 -0.012 -0.0 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 0.778\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.003 -0.013 -0.006 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.93\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.0 -0.019 -0.001 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.004 -0.02 -0.007 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.893\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.0 -0.026 -0.001 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.741\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.004 -0.027 0.005 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.853\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.008 -0.022 0.011 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.75\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.004 -0.012 0.005 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.631\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.0 -0.007 -0.001 output tensor([[5, 9]]) selected_action: tensor([1]) rew: 0.831\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.004 -0.009 -0.007 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.929\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.0 -0.016 -0.002 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.775\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.004 -0.018 0.004 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.885\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.0 -0.013 -0.002 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.81\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.004 -0.015 -0.008 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.893\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.007 -0.023 -0.014 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.736\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.004 -0.036 -0.008 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.551\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.0 -0.044 -0.002 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.004 -0.047 -0.008 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.731\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.008 -0.055 -0.015 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.567\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.004 -0.07 -0.009 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.373\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.0 -0.079 -0.004 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.441\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.004 -0.082 0.002 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.533\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.008 -0.081 0.007 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.565\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.012 -0.074 0.012 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.44\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.016 -0.061 0.018 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.341\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.02 -0.044 0.023 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.265\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.016 -0.021 0.017 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.213\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.012 -0.004 0.011 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.016 0.008 0.017 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.707\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.02 0.024 0.023 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.542\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.024 0.047 0.029 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.314\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.02 0.076 0.023 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.055\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.116 -0.024 0.099 0.03 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.057\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -0.02 0.129 0.024 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.212\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.159 -0.024 0.153 0.031 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.223\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.182 -0.028 0.184 0.038 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.504\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.21 -0.032 0.222 0.045 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.82\n",
            "output tensor([[3, 8]]) selected_action: tensor([1]) rew: -6.171338785941868\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 699 lasted for 49 time steps with total reward of 21.42946255393412\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.003 -0.024 -0.007 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.868\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.0 -0.031 -0.001 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.004 -0.032 0.005 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.825\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.008 -0.028 0.01 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.732\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.012 -0.017 0.016 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.613\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.008 -0.001 0.01 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 0.52\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.012 0.009 0.016 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.745\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.016 0.024 0.022 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.565\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.012 0.046 0.016 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.342\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.008 0.062 0.01 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.381\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.012 0.072 0.017 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.443\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.008 0.089 0.011 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.238\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -0.012 0.1 0.018 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.293\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.153 -0.008 0.118 0.012 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.079\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -0.004 0.13 0.007 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.125\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.165 -0.001 0.138 0.002 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.192\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.166 0.003 0.14 -0.002 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.281\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.163 0.007 0.138 -0.007 output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.268\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 0.003 0.13 -0.001 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.157\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.152 -0.001 0.13 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.359\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.153 -0.005 0.136 0.012 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.234\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -0.001 0.148 0.008 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.04\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 0.003 0.156 0.003 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.104\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.001 0.158 0.009 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.19\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -0.005 0.168 0.016 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.008\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -0.001 0.184 0.011 output tensor([[7, 8]]) selected_action: tensor([1]) rew: -0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -0.005 0.195 0.018 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.166\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -0.009 0.214 0.025 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.393\n",
            "output tensor([[7, 3]]) selected_action: tensor([0]) rew: -5.6542365657049825\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 700 lasted for 29 time steps with total reward of 2.8957185958412417\n",
            "\n",
            "\n",
            " Model saved!\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.004 -0.027 -0.007 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.853\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.008 -0.034 -0.013 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.698\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.012 -0.047 -0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.514\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.016 -0.066 -0.025 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.301\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.012 -0.091 -0.02 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.056\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.008 -0.111 -0.014 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.072\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.084 0.012 -0.125 -0.021 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.109\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 0.008 -0.146 -0.016 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.123\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 0.004 -0.162 -0.011 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.098\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.0 -0.173 -0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.052\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 -0.004 -0.18 -0.002 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.013\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 -0.007 -0.181 0.003 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.1\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 -0.003 -0.178 -0.004 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.061\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 -0.007 -0.182 0.001 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.049\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 -0.003 -0.182 -0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.112\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 -0.007 -0.188 -0.002 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.023\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 -0.011 -0.19 0.003 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.063\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 -0.007 -0.187 -0.004 output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.021\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 -0.011 -0.191 0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.01\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.015 -0.19 0.005 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.019 -0.185 0.01 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.036\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.015 -0.175 0.003 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: -0.125\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.019 -0.172 0.007 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.094\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.015 -0.165 0.001 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.009\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.018 -0.164 0.005 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.198\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.022 -0.159 0.01 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.082\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.026 -0.149 0.015 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.011\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.03 -0.134 0.02 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.082\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.034 -0.114 0.025 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.132\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.165 -0.038 -0.089 0.03 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.162\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.202 -0.042 -0.06 0.035 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.171\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.244 -0.038 -0.025 0.029 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: -0.161\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.282 -0.034 0.004 0.023 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.162\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.315 -0.03 0.027 0.017 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.409\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.345 -0.026 0.044 0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.446\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.371 -0.022 0.055 0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.507\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.393 -0.018 0.061 0.0 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.592\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.412 -0.014 0.061 -0.005 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.702\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.426 -0.018 0.056 0.001 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.577\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.444 -0.022 0.057 0.007 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.707\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.466 -0.018 0.064 0.002 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.547\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.484 -0.014 0.066 -0.004 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.65\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.499 -0.01 0.062 -0.009 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.592\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.509 -0.014 0.053 -0.003 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.474\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.524 -0.011 0.05 -0.008 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.674\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.534 -0.007 0.042 -0.014 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.55\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.541 -0.011 0.028 -0.008 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.452\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.551 -0.014 0.02 -0.002 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.671\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.566 -0.011 0.018 -0.008 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.859\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.576 -0.014 0.011 -0.002 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.724\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.591 -0.011 0.009 -0.007 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.909\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.602 -0.007 0.002 -0.013 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.772\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.608 -0.011 -0.011 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.662\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.619 -0.014 -0.018 -0.001 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.766\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.633 -0.018 -0.02 0.004 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.876\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.652 -0.014 -0.016 -0.002 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.799\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.666 -0.018 -0.017 0.004 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.883\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.684 -0.014 -0.013 -0.002 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.699 -0.011 -0.015 -0.008 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.889\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.709 -0.014 -0.023 -0.002 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.731\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.724 -0.011 -0.025 -0.008 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.838\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.734 -0.014 -0.033 -0.002 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.678\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.749 -0.018 -0.036 0.003 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.781\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.767 -0.014 -0.032 -0.003 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.749\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.782 -0.018 -0.035 0.003 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.774\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.8 -0.022 -0.032 0.008 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.762\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.822 -0.026 -0.024 0.014 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.634\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.848 -0.022 -0.01 0.008 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.533\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.87 -0.026 -0.002 0.014 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.751\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.896 -0.03 0.012 0.02 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.926 -0.034 0.032 0.026 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.45\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.96 -0.03 0.057 0.02 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.207\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.99 -0.034 0.077 0.026 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.226\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.024 -0.038 0.104 0.033 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.024\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.062 -0.042 0.136 0.039 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.307\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.104 -0.046 0.175 0.046 output tensor([[12,  1]]) selected_action: tensor([0]) rew: -0.624\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.149 -0.042 0.221 0.041 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.976\n",
            "output tensor([[3, 3]]) selected_action: tensor([0]) rew: -6.076681748980267\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 701 lasted for 78 time steps with total reward of 20.71290232171167\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.003 -0.008 0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.943\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.001 -0.003 -0.001 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.837\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.003 -0.004 0.005 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.96\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.007 0.001 0.011 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 0.858\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.011 0.011 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.015 0.028 0.022 output tensor([[19,  3]]) selected_action: tensor([0]) rew: 0.533\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.019 0.05 0.028 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.306\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.015 0.079 0.023 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.048\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.011 0.102 0.018 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.05\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.015 0.119 0.024 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.074\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.019 0.143 0.031 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.171\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.015 0.174 0.026 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.45\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.011 0.2 0.021 output tensor([[3, 8]]) selected_action: tensor([1]) rew: -0.474\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.015 0.221 0.028 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.48\n",
            "output tensor([[11,  3]]) selected_action: tensor([0]) rew: -5.7548549830391735\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 702 lasted for 15 time steps with total reward of -1.9926946883077679\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.004 -0.036 0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.817\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.007 -0.031 0.011 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.699\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.004 -0.02 0.005 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.583\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.007 -0.016 0.01 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.786\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.003 -0.005 0.004 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.665\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 0.0 -0.001 -0.001 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.863\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.003 -0.002 0.004 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.96\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.007 0.002 0.01 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.879\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.003 0.012 0.004 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.007 0.017 0.01 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.83\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.003 0.027 0.005 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.661\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.007 0.032 0.011 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.755\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.011 0.042 0.017 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.582\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.015 0.059 0.023 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.38\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.019 0.082 0.029 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.148\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.023 0.111 0.035 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.116\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.171 -0.027 0.146 0.042 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.413\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.198 -0.031 0.188 0.049 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.745\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.228 -0.035 0.237 0.055 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -1.111\n",
            "output tensor([[6, 5]]) selected_action: tensor([0]) rew: -6.515177312800178\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 703 lasted for 20 time steps with total reward of 1.4410411457841095\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.004 -0.043 -0.006 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 0.795\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.001 -0.049 -0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.642\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.003 -0.049 0.005 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.752\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.001 -0.044 -0.001 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.64\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.005 -0.046 -0.007 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.758\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.001 -0.053 -0.002 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.599\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.005 -0.055 -0.008 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.703\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.008 -0.063 -0.014 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.005 -0.077 -0.009 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.347\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.009 -0.086 -0.015 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.012 -0.101 -0.021 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.216\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.016 -0.122 -0.028 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.014\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.02 -0.15 -0.034 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: -0.278\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.024 -0.184 -0.041 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.575\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.02 -0.225 -0.036 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.906\n",
            "output tensor([[12,  1]]) selected_action: tensor([0]) rew: -5.987330794413636\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 704 lasted for 16 time steps with total reward of -1.3511956799711502\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.004 -0.037 -0.006 output tensor([[7, 9]]) selected_action: tensor([1]) rew: 0.819\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.0 -0.044 -0.001 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.666\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.004 -0.044 -0.007 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.777\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.008 -0.051 -0.013 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.012 -0.064 -0.019 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.015 -0.083 -0.025 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.22\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.012 -0.108 -0.02 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.027\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.008 -0.128 -0.015 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.015\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.012 -0.143 -0.021 output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.019\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.008 -0.164 -0.017 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.217\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.012 -0.181 -0.023 output tensor([[1, 9]]) selected_action: tensor([1]) rew: -0.197\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.016 -0.204 -0.03 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.446\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 0.012 -0.234 -0.026 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.729\n",
            "output tensor([[4, 3]]) selected_action: tensor([0]) rew: -5.7614085866862625\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 705 lasted for 14 time steps with total reward of -3.833454560524295\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.004 0.025 -0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.864\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.008 0.019 -0.012 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.724\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.004 0.007 -0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.612\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.0 0.001 -0.0 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.004 0.001 -0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.994\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.008 -0.005 -0.012 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.847\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.012 -0.016 -0.018 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.685\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.008 -0.034 -0.012 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.482\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.012 -0.046 -0.018 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.542\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.016 -0.064 -0.024 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.334\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.02 -0.088 -0.03 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.095\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.024 -0.118 -0.037 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.176\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 0.02 -0.155 -0.032 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.48\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 0.024 -0.186 -0.038 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.528\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.166 0.028 -0.225 -0.045 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.847\n",
            "output tensor([[7, 4]]) selected_action: tensor([0]) rew: -6.202132641467996\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 706 lasted for 16 time steps with total reward of -1.2363524562283263\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.005 -0.039 0.005 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.809\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.009 -0.034 0.011 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.678\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.013 -0.023 0.017 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.564\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.009 -0.006 0.011 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.475\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.005 0.005 0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.705\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.001 0.009 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.86\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.005 0.008 0.005 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 0.929\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.001 0.013 -0.001 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.84\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 0.003 0.012 -0.007 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 0.914\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.001 0.005 -0.001 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.774\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.003 0.004 -0.007 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.954\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.007 -0.002 -0.012 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.813\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 0.003 -0.015 -0.007 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.678\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.007 -0.021 -0.013 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.765\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.003 -0.034 -0.007 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.007 -0.041 -0.013 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.668\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.003 -0.054 -0.007 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.001 -0.061 -0.002 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.562\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.003 -0.063 -0.008 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.664\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.001 -0.071 -0.003 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.5\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.003 -0.073 -0.009 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.598\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.001 -0.082 -0.003 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.429\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.003 -0.086 -0.01 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.521\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.007 -0.095 -0.016 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.345\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.011 -0.112 -0.023 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.139\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.015 -0.134 -0.029 output tensor([[1, 9]]) selected_action: tensor([1]) rew: -0.099\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.011 -0.163 -0.024 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.37\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.007 -0.188 -0.019 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.385\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.011 -0.207 -0.026 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.382\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.015 -0.233 -0.033 output tensor([[2, 8]]) selected_action: tensor([1]) rew: -0.647\n",
            "output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: -5.947841048011918\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 707 lasted for 31 time steps with total reward of 8.42063166715972\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.005 -0.035 0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.813\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.001 -0.028 0.0 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.676\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.003 -0.028 -0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.858\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.007 -0.034 -0.012 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.724\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.011 -0.045 -0.018 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.015 -0.063 -0.024 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.338\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.011 -0.087 -0.019 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.1\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.007 -0.106 -0.013 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.121\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.011 -0.119 -0.02 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.165\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.015 -0.139 -0.026 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.06\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.011 -0.165 -0.021 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.007 -0.186 -0.017 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.32\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.004 -0.203 -0.012 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.304\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 -0.0 -0.215 -0.008 output tensor([[9, 1]]) selected_action: tensor([0]) rew: -0.268\n",
            "output tensor([[5, 7]]) selected_action: tensor([1]) rew: -5.214354034021471\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 708 lasted for 15 time steps with total reward of -2.145430286428304\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.005 -0.047 -0.006 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.76\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.001 -0.053 -0.0 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.634\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.003 -0.053 0.005 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.746\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.001 -0.047 -0.001 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.612\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.003 -0.048 0.005 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.755\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.007 -0.043 0.01 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.65\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.011 -0.033 0.016 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.007 -0.017 0.01 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.445\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.003 -0.007 0.004 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.672\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 0.001 -0.003 -0.002 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.868\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.003 -0.005 0.004 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.934\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.007 -0.001 0.01 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.879\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.011 0.008 0.015 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.752\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.015 0.024 0.021 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.573\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.011 0.045 0.016 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 0.352\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.007 0.061 0.01 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.392\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.011 0.071 0.016 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.456\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -0.015 0.087 0.023 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.252\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -0.019 0.11 0.029 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.017\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.174 -0.023 0.139 0.036 output tensor([[1, 1]]) selected_action: tensor([0]) rew: -0.25\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.197 -0.027 0.174 0.042 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.551\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.224 -0.023 0.217 0.037 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.887\n",
            "output tensor([[6, 4]]) selected_action: tensor([0]) rew: -5.971440186663642\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 709 lasted for 23 time steps with total reward of 3.6249831053171206\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.005 0.008 -0.006 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.959\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.001 0.002 0.0 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.819\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.003 0.003 0.006 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.983\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.007 0.009 0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.836\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.003 0.021 0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.66\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.007 0.027 0.012 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.748\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.011 0.039 0.018 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.569\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.015 0.057 0.024 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.361\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.011 0.081 0.019 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.122\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.007 0.1 0.013 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.143\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.011 0.113 0.02 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.186\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.007 0.133 0.015 output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.039\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.011 0.148 0.021 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.015 0.17 0.028 output tensor([[8, 8]]) selected_action: tensor([0]) rew: -0.243\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.019 0.198 0.035 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.513\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.015 0.233 0.03 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.817\n",
            "output tensor([[1, 7]]) selected_action: tensor([1]) rew: -5.871525560967382\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 710 lasted for 17 time steps with total reward of -1.1038870175412736\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.003 0.001 0.005 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.975\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.001 0.006 -0.001 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.869\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.003 0.005 0.005 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.954\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.007 0.01 0.011 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.847\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.011 0.021 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.675\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.007 0.038 0.011 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.474\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.003 0.049 0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.536\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.007 0.055 0.012 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.623\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.011 0.067 0.018 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.442\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.015 0.085 0.024 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.231\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.019 0.109 0.031 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.012\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.023 0.14 0.037 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.287\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.027 0.177 0.044 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.595\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.023 0.221 0.039 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.939\n",
            "output tensor([[2, 4]]) selected_action: tensor([1]) rew: -6.0309558169489605\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 711 lasted for 15 time steps with total reward of -1.2387324019419896\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.004 0.03 0.006 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.853\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.0 0.036 0.0 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.702\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.004 0.037 -0.005 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.814\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.007 0.032 -0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.696\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.011 0.021 -0.016 output tensor([[5, 9]]) selected_action: tensor([1]) rew: 0.58\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.015 0.005 -0.022 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.49\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.019 -0.017 -0.028 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.425\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.015 -0.045 -0.022 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.218\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 0.019 -0.068 -0.028 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.228\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.023 -0.096 -0.035 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.031\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 0.027 -0.131 -0.041 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.323\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.162 0.031 -0.172 -0.048 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.648\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.192 0.035 -0.219 -0.054 output tensor([[1, 8]]) selected_action: tensor([1]) rew: -1.009\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: -6.406545690707116\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 712 lasted for 14 time steps with total reward of -3.412805245036647\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.004 0.026 0.006 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.873\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.0 0.032 0.0 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.723\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.004 0.033 -0.005 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.836\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.008 0.028 -0.011 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.711\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.012 0.017 -0.017 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.595\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.016 0.0 -0.022 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.506\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.02 -0.022 -0.028 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.441\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.024 -0.051 -0.034 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.188\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.02 -0.085 -0.029 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.096\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 0.024 -0.113 -0.035 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.121\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.02 -0.148 -0.03 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.136 0.016 -0.178 -0.025 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.456\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.152 0.02 -0.203 -0.032 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.478\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.171 0.016 -0.235 -0.027 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.769\n",
            "output tensor([[16,  0]]) selected_action: tensor([0]) rew: -5.809549642542932\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 713 lasted for 15 time steps with total reward of -3.2725461181811344\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.005 0.039 -0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.001 0.033 0.0 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.675\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.003 0.034 0.007 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.001 0.04 0.001 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.675\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.003 0.041 0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.785\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.007 0.048 0.013 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.011 0.061 0.019 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.442\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.015 0.081 0.026 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.226\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.019 0.106 0.032 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.022\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.023 0.138 0.038 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.302\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -0.019 0.176 0.033 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.616\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.015 0.21 0.029 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.676\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: -5.718559631555542\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 714 lasted for 13 time steps with total reward of -2.259620625798083\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.004 0.02 0.006 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.901\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.008 0.026 0.012 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.752\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.012 0.038 0.018 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.573\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.008 0.056 0.012 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.365\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.012 0.069 0.019 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.419\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.016 0.088 0.025 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.205\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.012 0.112 0.02 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.041\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.009 0.132 0.015 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.028\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.005 0.147 0.01 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.006\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.001 0.156 0.005 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.061\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.003 0.161 -0.0 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.137\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.001 0.161 0.007 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.229\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.003 0.167 0.002 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.065\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.007 0.169 -0.003 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.152\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.003 0.167 0.004 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.122\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.001 0.171 0.011 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.104\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.005 0.181 0.018 output tensor([[8, 2]]) selected_action: tensor([0]) rew: -0.086\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.009 0.199 0.025 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.308\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.005 0.224 0.02 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.564\n",
            "output tensor([[9, 7]]) selected_action: tensor([0]) rew: -5.56908975870216\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 715 lasted for 20 time steps with total reward of -2.5028189996826455\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.004 -0.011 0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.93\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.008 -0.006 0.011 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.821\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.012 0.004 0.017 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.701\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.016 0.021 0.022 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.564\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.02 0.043 0.028 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.338\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.024 0.072 0.035 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.081\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.028 0.107 0.041 output tensor([[13,  0]]) selected_action: tensor([0]) rew: -0.208\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.032 0.147 0.047 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.53\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -0.036 0.195 0.054 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.887\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.187 -0.04 0.249 0.061 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -1.28\n",
            "output tensor([[4, 7]]) selected_action: tensor([1]) rew: -6.710195635990315\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 716 lasted for 11 time steps with total reward of -6.179174348093303\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.003 -0.038 -0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.007 -0.044 -0.012 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 0.663\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.011 -0.056 -0.018 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.481\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.007 -0.075 -0.013 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.269\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.011 -0.088 -0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.007 -0.107 -0.014 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.099\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.003 -0.121 -0.009 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.138\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.007 -0.13 -0.015 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.2\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.003 -0.145 -0.01 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.006\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.007 -0.156 -0.017 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.044\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.004 -0.173 -0.012 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.173\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.0 -0.185 -0.008 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.135\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.004 -0.193 -0.015 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.077\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.0 -0.208 -0.01 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.286\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.004 -0.218 -0.006 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.243\n",
            "output tensor([[6, 4]]) selected_action: tensor([0]) rew: -5.18027303417122\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 717 lasted for 16 time steps with total reward of -3.069868442659356\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.003 0.005 -0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.966\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.001 -0.0 0.001 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.844\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.003 0.0 -0.005 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.984\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.001 -0.005 0.001 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.867\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.003 -0.005 -0.005 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.961\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.007 -0.01 -0.011 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.845\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.011 -0.021 -0.017 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.673\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.007 -0.038 -0.011 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.472\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.011 -0.05 -0.017 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.533\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.007 -0.067 -0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.326\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.003 -0.079 -0.007 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.381\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.007 -0.086 -0.013 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.46\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.003 -0.098 -0.008 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.27\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.007 -0.106 -0.014 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.341\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.011 -0.12 -0.02 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.144\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.007 -0.14 -0.015 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.085\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.107 0.003 -0.156 -0.011 output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.057\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.111 0.007 -0.166 -0.017 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.008\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 0.011 -0.184 -0.024 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.227\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.129 0.007 -0.208 -0.019 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.479\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.136 0.003 -0.227 -0.015 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.479\n",
            "output tensor([[2, 7]]) selected_action: tensor([1]) rew: -5.461677878767987\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 718 lasted for 22 time steps with total reward of 2.2711448486399695\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.004 0.021 0.006 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.884\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.008 0.027 0.012 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.761\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.012 0.038 0.018 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.008 0.056 0.012 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.379\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.012 0.068 0.018 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.008 0.086 0.013 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.224\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.012 0.099 0.019 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.273\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.008 0.118 0.014 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.053\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.119 -0.012 0.131 0.02 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.092\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.016 0.152 0.027 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.138\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.148 -0.02 0.179 0.034 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.401\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.168 -0.024 0.213 0.041 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.698\n",
            "output tensor([[10,  1]]) selected_action: tensor([0]) rew: -6.030116109083723\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 719 lasted for 13 time steps with total reward of -3.579977898168637\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.004 0.032 -0.005 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.0 0.028 0.001 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.723\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.004 0.029 0.007 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.838\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.008 0.036 0.013 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.682\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.012 0.049 0.019 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.497\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.016 0.069 0.025 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.282\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.02 0.094 0.032 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.036\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.016 0.126 0.027 output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.243\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.012 0.152 0.021 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.263\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.016 0.174 0.028 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.265\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.02 0.202 0.035 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.016 0.237 0.031 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: -0.841\n",
            "output tensor([[7, 2]]) selected_action: tensor([0]) rew: -5.896407808554128\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 720 lasted for 13 time steps with total reward of -4.156183117022806\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.004 -0.047 -0.006 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.772\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.001 -0.053 -0.001 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.618\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.004 -0.054 -0.007 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.726\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.001 -0.061 -0.001 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.005 -0.062 -0.008 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.672\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.001 -0.07 -0.002 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.51\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.003 -0.072 0.003 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.007 -0.069 0.009 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.575\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.003 -0.061 0.002 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.456\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.007 -0.058 0.008 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.653\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.003 -0.051 0.002 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.528\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.007 -0.049 0.007 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.72\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.011 -0.042 0.013 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.589\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.007 -0.03 0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.484\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.003 -0.023 0.0 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.697\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.001 -0.023 -0.006 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.879\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 0.005 -0.028 -0.012 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.753\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.009 -0.04 -0.018 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.576\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.013 -0.057 -0.024 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.37\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.016 -0.081 -0.03 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.134\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.02 -0.111 -0.036 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -0.135\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.024 -0.147 -0.043 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.028 -0.19 -0.049 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.771\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.032 -0.239 -0.056 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -1.142\n",
            "output tensor([[4, 8]]) selected_action: tensor([1]) rew: -6.550501173555373\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 721 lasted for 25 time steps with total reward of 2.8547245940667647\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.005 -0.002 0.007 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.962\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.009 0.005 0.013 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.82\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.013 0.017 0.019 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.662\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.009 0.036 0.013 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.455\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.013 0.049 0.019 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.51\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.017 0.067 0.025 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.297\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.013 0.092 0.02 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.054\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.116 -0.017 0.112 0.026 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.07\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.021 0.138 0.032 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.184\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.153 -0.017 0.17 0.028 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.471\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.17 -0.013 0.198 0.023 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.502\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.183 -0.017 0.221 0.03 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.516\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: -5.7994152883613515\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 722 lasted for 13 time steps with total reward of -3.6431193146619636\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.005 0.044 0.005 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.768\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.009 0.05 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.652\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.005 0.061 0.006 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.473\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.001 0.067 0.001 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.556\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.005 0.068 0.007 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.663\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.009 0.075 0.013 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.504\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.013 0.088 0.019 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.314\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.017 0.107 0.026 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.094\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.013 0.133 0.021 output tensor([[1, 8]]) selected_action: tensor([1]) rew: -0.158\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.017 0.154 0.027 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.153\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.021 0.181 0.034 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.024 0.215 0.041 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.716\n",
            "output tensor([[7, 6]]) selected_action: tensor([0]) rew: -6.049932119942804\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 723 lasted for 13 time steps with total reward of -3.470012682851845\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.003 0.02 0.007 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.887\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.007 0.027 0.013 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.011 0.04 0.019 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.551\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.015 0.058 0.025 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.339\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.019 0.083 0.031 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.096\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.023 0.115 0.038 output tensor([[11,  0]]) selected_action: tensor([0]) rew: -0.179\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.019 0.152 0.032 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.487\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.023 0.185 0.039 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.538\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.019 0.224 0.035 output tensor([[2, 9]]) selected_action: tensor([1]) rew: -0.862\n",
            "output tensor([[7, 6]]) selected_action: tensor([0]) rew: -5.933506361139408\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 724 lasted for 10 time steps with total reward of -5.39438570733982\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.003 -0.01 -0.005 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.937\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.007 -0.015 -0.011 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 0.819\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.011 -0.026 -0.017 output tensor([[ 2, 20]]) selected_action: tensor([1]) rew: 0.646\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.007 -0.044 -0.012 output tensor([[9, 8]]) selected_action: tensor([0]) rew: 0.443\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.011 -0.055 -0.018 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.503\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.015 -0.073 -0.024 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.295\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.011 -0.097 -0.018 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.056\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.015 -0.115 -0.025 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.076\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.019 -0.14 -0.031 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.173\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.015 -0.172 -0.027 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.455\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.149 0.011 -0.198 -0.022 output tensor([[5, 1]]) selected_action: tensor([0]) rew: -0.482\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.16 0.015 -0.22 -0.029 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.491\n",
            "output tensor([[2, 6]]) selected_action: tensor([1]) rew: -5.769238634092944\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 725 lasted for 13 time steps with total reward of -3.5933759972174446\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.004 -0.038 0.005 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.008 -0.032 0.011 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.685\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.012 -0.021 0.017 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.57\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.016 -0.005 0.022 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.481\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.012 0.018 0.016 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.418\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.008 0.034 0.011 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.503\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.012 0.045 0.017 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.008 0.062 0.011 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.365\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.012 0.073 0.017 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.424\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.14 -0.016 0.09 0.024 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.215\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -0.012 0.114 0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.026\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.168 -0.016 0.133 0.025 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.008\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.184 -0.02 0.158 0.032 output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.26\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.204 -0.024 0.189 0.038 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.545\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.227 -0.02 0.228 0.034 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.865\n",
            "output tensor([[9, 1]]) selected_action: tensor([0]) rew: -5.933772281687234\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 726 lasted for 16 time steps with total reward of -2.5914574647718265\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.003 0.038 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.808\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.007 0.044 0.012 output tensor([[19,  1]]) selected_action: tensor([0]) rew: 0.674\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.003 0.056 0.006 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.494\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.001 0.062 0.001 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.577\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.003 0.063 0.007 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.684\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.007 0.07 0.013 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.524\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.003 0.083 0.008 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.335\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.007 0.091 0.014 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.406\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.003 0.105 0.009 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.21\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.007 0.114 0.015 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.274\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.011 0.13 0.022 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.069\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.015 0.152 0.029 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.168\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.019 0.18 0.035 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.438\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.015 0.215 0.031 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.743\n",
            "output tensor([[3, 9]]) selected_action: tensor([1]) rew: -5.795359288349143\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 727 lasted for 15 time steps with total reward of -2.089754261017081\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.005 0.031 0.005 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.832\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.001 0.036 -0.0 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.717\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.005 0.036 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.819\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.009 0.042 0.012 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.682\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.012 0.054 0.018 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.503\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.016 0.072 0.024 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.293\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.02 0.096 0.03 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.053\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.016 0.126 0.025 output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.22\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.02 0.152 0.032 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.235\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.024 0.183 0.039 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.52\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.028 0.222 0.045 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.84\n",
            "output tensor([[5, 6]]) selected_action: tensor([1]) rew: -6.195415473625958\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 728 lasted for 12 time steps with total reward of -4.109652195644401\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.005 -0.013 -0.006 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.925\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.009 -0.019 -0.011 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.797\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.013 -0.03 -0.017 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.017 -0.048 -0.024 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.418\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.013 -0.071 -0.018 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.184\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.009 -0.089 -0.013 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.21\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.005 -0.102 -0.007 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.259\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.001 -0.109 -0.002 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.331\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.005 -0.111 -0.009 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.425\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 0.009 -0.12 -0.015 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.253\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 0.005 -0.135 -0.01 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.049\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.009 -0.145 -0.017 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.103\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.013 -0.162 -0.023 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.111\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.128 0.009 -0.185 -0.019 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.359\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 0.013 -0.204 -0.026 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.351\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.15 0.009 -0.23 -0.021 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.613\n",
            "output tensor([[1, 7]]) selected_action: tensor([1]) rew: -5.623668760330639\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 729 lasted for 17 time steps with total reward of -2.481682263585518\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.003 -0.028 -0.005 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.848\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.007 -0.033 -0.011 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.011 -0.045 -0.017 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.556\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.007 -0.062 -0.012 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 0.35\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.003 -0.074 -0.006 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.406\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.0 -0.08 -0.001 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.485\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.003 -0.082 -0.007 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.589\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.0 -0.089 -0.002 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.425\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.003 -0.091 -0.008 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.523\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.0 -0.1 -0.003 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.353\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.004 -0.103 0.002 output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.444\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.008 -0.101 0.007 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.46\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.012 -0.094 0.012 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.34\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.016 -0.081 0.018 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.245\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.02 -0.064 0.023 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.173\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.024 -0.041 0.028 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.123\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.02 -0.013 0.022 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.097\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.016 0.009 0.016 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.384\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.02 0.026 0.022 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.548\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.024 0.048 0.028 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.323\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -0.02 0.076 0.023 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.067\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.159 -0.016 0.099 0.017 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.071\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.175 -0.02 0.116 0.024 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.096\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.194 -0.024 0.14 0.03 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.147\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.218 -0.02 0.17 0.025 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.424\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.238 -0.024 0.195 0.032 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.445\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.262 -0.02 0.227 0.028 output tensor([[1, 6]]) selected_action: tensor([1]) rew: -0.737\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: -5.77707208474988\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 730 lasted for 28 time steps with total reward of 1.1085647974618613\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.003 0.002 0.006 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.986\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.001 0.007 -0.0 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.851\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.003 0.007 0.006 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.961\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.001 0.013 -0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.823\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.003 0.013 0.006 output tensor([[20,  1]]) selected_action: tensor([0]) rew: 0.936\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.001 0.019 0.0 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.793\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.005 0.019 -0.006 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.91\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.001 0.013 0.0 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.769\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.003 0.013 0.006 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.93\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.001 0.02 0.0 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 0.781\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.003 0.02 0.006 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.895\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.007 0.026 0.012 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.743\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.011 0.039 0.018 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.563\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.015 0.057 0.025 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.353\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.011 0.082 0.019 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.113\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.007 0.101 0.014 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.133\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.003 0.115 0.009 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.175\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.007 0.123 0.015 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.239\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.011 0.138 0.022 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.035\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.007 0.16 0.017 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.201\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.004 0.177 0.012 output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.182\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.007 0.189 0.019 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.142\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.011 0.207 0.026 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.371\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.015 0.233 0.033 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.633\n",
            "output tensor([[12,  3]]) selected_action: tensor([0]) rew: -5.931123639514433\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 731 lasted for 25 time steps with total reward of 4.528312202265838\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.004 0.013 0.007 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.925\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.008 0.02 0.013 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.774\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.012 0.032 0.018 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.594\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.008 0.051 0.013 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.385\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.004 0.063 0.007 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.438\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.008 0.071 0.014 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.514\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.004 0.084 0.008 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.324\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.008 0.092 0.015 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.394\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.004 0.107 0.009 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.196\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.008 0.116 0.016 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.257\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.012 0.132 0.022 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.051\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.016 0.154 0.029 output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.188\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.012 0.183 0.024 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.46\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.016 0.207 0.031 output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.478\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.171 -0.012 0.238 0.027 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.765\n",
            "output tensor([[7, 6]]) selected_action: tensor([0]) rew: -5.801954653231954\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 732 lasted for 16 time steps with total reward of -2.841900783854037\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.005 -0.001 0.005 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.976\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.001 0.004 -0.001 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.873\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.003 0.003 -0.007 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.957\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.007 -0.003 -0.013 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.011 -0.016 -0.018 output tensor([[ 3, 16]]) selected_action: tensor([1]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.007 -0.034 -0.013 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.462\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.003 -0.047 -0.007 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.518\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.001 -0.054 -0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.598\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.005 -0.056 0.004 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.703\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.001 -0.052 -0.002 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.634\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.005 -0.054 0.003 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.697\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.009 -0.051 0.009 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.66\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.005 -0.042 0.003 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.538\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.001 -0.039 -0.003 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.734\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.005 -0.043 0.002 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.726\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.001 -0.041 -0.004 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.743\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.003 -0.045 -0.01 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.707\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.007 -0.055 -0.016 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.011 -0.071 -0.022 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.334\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.015 -0.093 -0.029 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.102\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.019 -0.122 -0.035 output tensor([[4, 9]]) selected_action: tensor([1]) rew: -0.162\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.015 -0.157 -0.03 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.46\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.011 -0.187 -0.025 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.501\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.015 -0.212 -0.032 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.524\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: -5.8174341186274265\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 733 lasted for 25 time steps with total reward of 5.5157230248375\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.003 0.048 -0.005 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.754\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.001 0.043 0.002 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.657\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.005 0.045 0.008 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.757\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.008 0.052 0.014 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.597\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.012 0.066 0.02 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.407\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.008 0.086 0.015 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.187\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.012 0.1 0.021 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.227\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.016 0.121 0.027 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.001\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.02 0.149 0.034 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.262\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.024 0.182 0.041 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.556\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.028 0.223 0.047 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.886\n",
            "output tensor([[5, 3]]) selected_action: tensor([0]) rew: -6.251085701921002\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 734 lasted for 12 time steps with total reward of -4.3707855207594015\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.005 -0.018 0.005 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.899\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.001 -0.013 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.005 -0.014 0.005 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.913\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.009 -0.009 0.011 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.012 0.001 0.016 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.694\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.009 0.018 0.01 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.005 0.028 0.005 output tensor([[ 3, 18]]) selected_action: tensor([1]) rew: 0.654\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.009 0.033 0.011 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.747\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.005 0.044 0.005 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.574\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.009 0.049 0.011 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.664\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.012 0.06 0.017 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.486\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.009 0.077 0.012 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.279\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.005 0.089 0.007 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.332\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.009 0.096 0.013 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.409\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.013 0.109 0.019 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.218\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.009 0.128 0.014 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.004\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.005 0.143 0.009 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.031\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.15 -0.001 0.152 0.004 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.088\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -0.005 0.156 0.011 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.166\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -0.001 0.167 0.006 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.023\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.157 0.003 0.174 0.002 output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.044\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.154 0.007 0.175 -0.003 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.132\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.148 0.003 0.172 0.004 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.087\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 0.007 0.176 -0.001 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.084\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 0.003 0.175 0.006 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.138\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.001 0.181 0.013 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.016\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.005 0.194 0.02 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.184\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.009 0.213 0.027 output tensor([[11,  1]]) selected_action: tensor([0]) rew: -0.417\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: -5.683861191545258\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 735 lasted for 29 time steps with total reward of 3.5362038906229607\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.005 0.005 -0.005 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.97\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.009 -0.001 -0.011 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 0.844\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.013 -0.012 -0.017 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.009 -0.029 -0.011 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.515\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.005 -0.04 -0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.578\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.001 -0.046 -0.0 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.666\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.003 -0.046 0.006 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.779\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.007 -0.041 0.011 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.642\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.011 -0.03 0.017 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.529\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.007 -0.013 0.011 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.442\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.011 -0.002 0.016 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.673\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.015 0.014 0.022 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.579\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.011 0.036 0.016 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.376\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.007 0.053 0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.414\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.003 0.064 0.005 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.476\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.007 0.069 0.012 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.562\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.003 0.081 0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.381\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.007 0.087 0.013 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.46\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.011 0.099 0.019 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.272\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.007 0.118 0.014 output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.053\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.011 0.132 0.02 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.093\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.007 0.152 0.015 output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.136\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.003 0.167 0.01 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.109\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 0.001 0.178 0.006 output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.061\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.003 0.184 0.013 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 0.001 0.196 0.008 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.192\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 0.004 0.204 0.004 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.137\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 0.008 0.208 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.063\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 0.012 0.207 -0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.015\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.016 0.202 -0.01 output tensor([[1, 9]]) selected_action: tensor([1]) rew: -0.121\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.02 0.192 -0.014 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.016 0.178 -0.007 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.271\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.012 0.17 -0.0 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.03\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.008 0.17 0.006 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.176\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.004 0.176 0.013 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.029\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.0 0.189 0.02 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.172\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.004 0.209 0.027 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.406\n",
            "output tensor([[14,  1]]) selected_action: tensor([0]) rew: -5.674543982691611\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 736 lasted for 38 time steps with total reward of 3.6409297005352004\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.003 -0.049 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.749\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.001 -0.044 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.649\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.003 -0.046 0.004 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.752\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.007 -0.042 0.01 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.679\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.011 -0.032 0.015 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.56\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.007 -0.017 0.009 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.466\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.003 -0.007 0.003 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 0.69\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.007 -0.004 0.009 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.883\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.011 0.005 0.015 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.753\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.015 0.02 0.021 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.605\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.019 0.04 0.027 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.387\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.015 0.067 0.021 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.139\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.019 0.088 0.027 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.151\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.023 0.116 0.034 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.107\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.019 0.149 0.029 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.397\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.14 -0.023 0.178 0.035 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.431\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.163 -0.019 0.214 0.031 output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.736\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -5.788453988342452\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 737 lasted for 18 time steps with total reward of 0.005051980805747647\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.003 -0.011 -0.007 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.927\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.001 -0.018 -0.001 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.775\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.005 -0.019 0.005 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.887\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.001 -0.015 -0.001 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.791\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.005 -0.016 0.004 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.898\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.001 -0.011 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.812\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.003 -0.013 -0.007 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.908\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.001 -0.02 -0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.753\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.005 -0.022 0.004 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.862\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.009 -0.018 0.01 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.793\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.012 -0.008 0.016 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.67\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.016 0.007 0.021 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.573\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.02 0.029 0.027 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.431\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.016 0.056 0.022 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.181\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.02 0.078 0.028 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.193\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.024 0.105 0.034 output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.065\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.028 0.139 0.041 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.355\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.024 0.18 0.036 output tensor([[2, 8]]) selected_action: tensor([1]) rew: -0.68\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -0.028 0.216 0.042 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.75\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: -6.091927476543193\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 738 lasted for 20 time steps with total reward of 2.5116348075373596\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.003 0.045 0.005 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.756\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.001 0.05 -0.0 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.657\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.005 0.05 -0.006 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.75\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.001 0.044 0.0 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.614\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.003 0.044 0.006 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.786\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.007 0.05 0.012 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.632\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.003 0.063 0.007 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.449\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.007 0.07 0.013 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.528\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.011 0.083 0.019 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.339\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.015 0.102 0.026 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.119\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.019 0.128 0.032 output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.132\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.023 0.16 0.039 output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.417\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.019 0.199 0.034 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.735\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.015 0.233 0.03 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.801\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: -5.851478768728847\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 739 lasted for 15 time steps with total reward of -2.304983203315122\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.004 0.05 -0.006 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.762\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.008 0.044 -0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.625\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.012 0.033 -0.017 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.513\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.008 0.017 -0.011 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.426\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.004 0.006 -0.005 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.657\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.008 0.001 -0.01 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.856\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.012 -0.009 -0.016 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.015 -0.025 -0.022 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.55\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.019 -0.048 -0.028 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.324\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.023 -0.076 -0.034 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.068\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 0.027 -0.11 -0.041 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.22\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 0.031 -0.151 -0.047 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: -0.541\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.157 0.027 -0.198 -0.042 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.898\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.184 0.023 -0.24 -0.038 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -1.001\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: -6.090382835864909\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 740 lasted for 15 time steps with total reward of -3.2351114244282466\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.003 -0.013 0.006 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.926\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.001 -0.007 0.0 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.784\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.003 -0.007 0.006 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.961\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.001 -0.001 0.0 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.817\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.005 -0.001 -0.006 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.993\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.009 -0.007 -0.012 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.849\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.012 -0.019 -0.018 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.674\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.009 -0.036 -0.012 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.471\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.005 -0.048 -0.006 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.53\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.001 -0.054 -0.001 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.614\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 -0.003 -0.055 0.005 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.723\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 -0.007 -0.05 0.01 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.616\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 -0.011 -0.04 0.016 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.502\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 -0.015 -0.024 0.021 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.413\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.019 -0.003 0.027 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.349\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.023 0.025 0.033 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.309\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.026 0.057 0.039 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.058\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.03 0.096 0.045 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.249\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.034 0.142 0.052 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.59\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.038 0.193 0.058 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.966\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.129 -0.034 0.252 0.054 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -1.379\n",
            "output tensor([[16,  1]]) selected_action: tensor([0]) rew: -6.542937741937114\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 741 lasted for 22 time steps with total reward of 0.862192793698771\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.004 -0.031 0.006 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.847\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.0 -0.025 -0.0 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.706\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.004 -0.025 0.006 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.877\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.008 -0.02 0.011 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.742\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.004 -0.008 0.005 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.626\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.0 -0.003 -0.001 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.004 -0.004 -0.007 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.968\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.007 -0.01 -0.012 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.011 -0.023 -0.018 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.64\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.015 -0.041 -0.024 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.433\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.019 -0.065 -0.03 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.195\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.023 -0.096 -0.037 output tensor([[2, 9]]) selected_action: tensor([1]) rew: -0.073\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 0.027 -0.133 -0.043 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.375\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.123 0.023 -0.176 -0.038 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.71\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.146 0.027 -0.214 -0.045 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.792\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: -6.1445568966923485\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 742 lasted for 16 time steps with total reward of -0.4128557044932597\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.003 0.039 -0.006 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.789\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.001 0.033 -0.0 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.652\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.005 0.033 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.835\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.009 0.038 0.012 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.7\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.005 0.05 0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.522\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.001 0.056 0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.605\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.003 0.057 -0.005 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.714\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.001 0.052 0.001 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.607\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.005 0.054 0.008 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.716\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.001 0.061 0.002 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.555\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.003 0.063 -0.003 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.007 0.06 -0.009 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.613\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.011 0.051 -0.014 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.493\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.015 0.037 -0.02 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.399\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.018 0.017 -0.025 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.329\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.015 -0.009 -0.019 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.283\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.011 -0.028 -0.014 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.472\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 0.015 -0.042 -0.02 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.524\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.111 0.011 -0.061 -0.014 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.308\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 0.007 -0.076 -0.009 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 0.354\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.128 0.011 -0.084 -0.015 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.422\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.139 0.015 -0.099 -0.021 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.223\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.154 0.011 -0.121 -0.016 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.165 0.007 -0.137 -0.011 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.021\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.171 0.003 -0.148 -0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.07\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.174 -0.001 -0.154 -0.001 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.14\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.174 0.003 -0.155 -0.008 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.232\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.177 -0.001 -0.163 -0.003 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.058\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.176 -0.005 -0.167 0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.139\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.171 -0.008 -0.165 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.167\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.163 -0.012 -0.159 0.011 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.057\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.151 -0.008 -0.148 0.004 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.032\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 -0.012 -0.144 0.009 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.189\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 -0.016 -0.135 0.014 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.088\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 -0.012 -0.121 0.007 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.009\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 -0.016 -0.113 0.012 output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.242\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 -0.02 -0.101 0.017 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 0.15\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 -0.024 -0.084 0.023 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.081\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.02 -0.061 0.016 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.035\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.016 -0.045 0.01 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 0.302\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.012 -0.035 0.004 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.008 -0.031 -0.002 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.736\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.012 -0.033 0.003 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.798\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.016 -0.03 0.009 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.755\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.012 -0.021 0.003 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.63\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.008 -0.017 -0.003 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.825\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.004 -0.02 -0.009 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 0.844\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.008 -0.029 -0.003 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.681\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.012 -0.032 0.003 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.782\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.016 -0.03 0.008 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.782\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.02 -0.022 0.014 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.653\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.024 -0.008 0.02 output tensor([[11,  4]]) selected_action: tensor([0]) rew: 0.551\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.02 0.012 0.014 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.474\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -0.016 0.025 0.008 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.603\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.18 -0.012 0.033 0.002 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.682\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.191 -0.008 0.035 -0.003 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.787\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.199 -0.004 0.032 -0.009 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.744\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.204 -0.008 0.023 -0.003 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.62\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.212 -0.012 0.02 0.003 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.223 -0.016 0.023 0.009 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.831\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.239 -0.012 0.032 0.003 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 0.668\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.251 -0.016 0.035 0.009 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.768\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.267 -0.02 0.044 0.015 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.601\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.287 -0.016 0.06 0.01 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.405\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.303 -0.02 0.069 0.016 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.471\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.322 -0.016 0.085 0.011 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.269\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.338 -0.012 0.096 0.005 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.327\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.35 -0.016 0.101 0.012 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.409\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.366 -0.012 0.113 0.007 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.223\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.378 -0.016 0.12 0.013 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.297\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.394 -0.02 0.133 0.02 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.102\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.414 -0.016 0.152 0.015 output tensor([[2, 8]]) selected_action: tensor([1]) rew: -0.124\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.43 -0.012 0.167 0.01 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.094\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.443 -0.016 0.177 0.017 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.043\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.459 -0.02 0.193 0.023 output tensor([[8, 2]]) selected_action: tensor([0]) rew: -0.26\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.479 -0.016 0.217 0.019 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.511\n",
            "output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: -5.509979364982656\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 743 lasted for 77 time steps with total reward of 25.841104998363047\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.004 -0.017 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.899\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.0 -0.01 0.0 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.759\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.004 -0.01 -0.005 output tensor([[ 3, 18]]) selected_action: tensor([1]) rew: 0.939\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.0 -0.015 0.0 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.817\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.004 -0.015 -0.006 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.918\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.008 -0.021 -0.012 output tensor([[ 6, 11]]) selected_action: tensor([1]) rew: 0.789\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.004 -0.032 -0.006 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.614\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.0 -0.038 -0.0 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.702\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.004 -0.038 0.005 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.008 -0.033 0.011 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.682\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.004 -0.022 0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.008 -0.017 0.011 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.772\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.011 -0.006 0.016 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.653\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.008 0.011 0.011 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.561\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.011 0.021 0.016 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.685\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.015 0.038 0.022 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.487\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.019 0.06 0.029 output tensor([[22,  0]]) selected_action: tensor([0]) rew: 0.258\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.023 0.089 0.035 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.001\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.027 0.123 0.041 output tensor([[4, 3]]) selected_action: tensor([0]) rew: -0.293\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.031 0.165 0.048 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.619\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.035 0.212 0.054 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.979\n",
            "output tensor([[4, 8]]) selected_action: tensor([1]) rew: -6.376192067049929\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 744 lasted for 22 time steps with total reward of 3.6495724041564968\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.003 0.03 -0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.848\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.001 0.024 0.0 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.707\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.005 0.024 0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.881\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.009 0.031 0.012 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.013 0.043 0.018 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.551\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.009 0.061 0.013 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.342\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.013 0.073 0.019 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.395\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.009 0.092 0.013 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.179\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.013 0.106 0.02 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.224\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.009 0.126 0.015 output tensor([[1, 8]]) selected_action: tensor([1]) rew: -0.001\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.005 0.14 0.01 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.033\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.009 0.15 0.016 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.089\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.013 0.166 0.023 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.124\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.017 0.189 0.03 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.37\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.013 0.219 0.025 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.65\n",
            "output tensor([[13,  1]]) selected_action: tensor([0]) rew: -5.678824181335919\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 745 lasted for 16 time steps with total reward of -1.8460782398260385\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.004 -0.043 -0.005 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 0.772\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.0 -0.049 0.0 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.658\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.004 -0.049 0.006 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.764\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.0 -0.043 -0.0 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.626\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.004 -0.043 0.005 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.784\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.007 -0.038 0.011 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.665\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.003 -0.027 0.005 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.55\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.007 -0.023 0.01 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.754\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.003 -0.013 0.004 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.634\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.007 -0.008 0.01 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.833\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.003 0.002 0.004 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.709\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.0 0.006 -0.002 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.888\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.004 0.004 -0.008 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.93\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.0 -0.003 -0.002 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.003 -0.005 0.004 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.944\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.0 -0.001 -0.002 output tensor([[7, 9]]) selected_action: tensor([1]) rew: 0.871\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.003 -0.002 0.004 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.955\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.007 0.002 0.01 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.884\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.011 0.012 0.016 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.741\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.007 0.028 0.01 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.011 0.038 0.016 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.615\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.015 0.054 0.022 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.019 0.076 0.028 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.187\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.015 0.105 0.023 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.074\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.019 0.128 0.03 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.023 0.157 0.036 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.347\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.027 0.193 0.043 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.653\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.182 -0.031 0.236 0.05 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.994\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: -6.371002196294934\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 746 lasted for 29 time steps with total reward of 8.005514276708428\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.004 0.009 0.007 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.939\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.008 0.015 0.013 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.787\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.012 0.028 0.019 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 0.606\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.008 0.047 0.013 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.396\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.012 0.06 0.019 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.449\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.008 0.079 0.014 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.233\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.004 0.093 0.008 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.278\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.008 0.101 0.015 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.345\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.004 0.116 0.01 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.145\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.008 0.126 0.016 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.204\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.004 0.142 0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.006\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.008 0.153 0.018 output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.042\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.012 0.171 0.025 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.178\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.008 0.196 0.02 output tensor([[1, 4]]) selected_action: tensor([1]) rew: -0.432\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -0.004 0.216 0.015 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -0.432\n",
            "output tensor([[8, 2]]) selected_action: tensor([0]) rew: -5.414175360334412\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 747 lasted for 16 time steps with total reward of -2.0391925613886186\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.003 0.019 0.006 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.907\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.007 0.025 0.012 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.758\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.011 0.037 0.018 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.579\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.007 0.055 0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.372\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.003 0.068 0.007 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.426\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.0 0.075 0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.504\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.003 0.076 0.008 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.607\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.007 0.084 0.014 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.442\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.003 0.098 0.009 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.247\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.0 0.107 0.004 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.312\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.004 0.11 -0.002 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.4\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 0.0 0.109 0.005 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.435\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.004 0.114 0.011 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.356\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.008 0.125 0.018 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.17\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.011 0.143 0.025 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.048\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.015 0.168 0.031 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.298\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.012 0.199 0.027 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.582\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.015 0.226 0.033 output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.614\n",
            "output tensor([[7, 4]]) selected_action: tensor([0]) rew: -5.914065428983209\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 748 lasted for 19 time steps with total reward of -0.9382778858686702\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.003 0.009 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.943\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.001 0.003 -0.0 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.8\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.005 0.003 0.006 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.978\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.001 0.008 -0.0 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.847\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.003 0.008 -0.006 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.953\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.007 0.002 -0.012 output tensor([[ 1, 20]]) selected_action: tensor([1]) rew: 0.81\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.011 -0.01 -0.018 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.693\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.007 -0.027 -0.012 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.512\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.003 -0.039 -0.006 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.572\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.007 -0.045 -0.012 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.657\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.003 -0.058 -0.007 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.475\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 -0.001 -0.065 -0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.555\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.005 -0.066 0.004 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.66\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.009 -0.062 0.01 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.582\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.005 -0.052 0.003 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.466\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.001 -0.049 -0.003 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.667\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.003 -0.052 -0.009 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.697\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.007 -0.061 -0.015 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.53\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.011 -0.076 -0.021 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.332\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.015 -0.097 -0.028 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.105\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.019 -0.125 -0.034 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: -0.155\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.015 -0.159 -0.029 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.448\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.019 -0.188 -0.036 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.485\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.132 0.015 -0.224 -0.031 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.793\n",
            "output tensor([[6, 7]]) selected_action: tensor([1]) rew: -5.849461713910004\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 749 lasted for 25 time steps with total reward of 5.102834113004698\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.004 -0.043 -0.006 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.008 -0.049 -0.012 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.638\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.012 -0.062 -0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.455\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.016 -0.08 -0.025 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.242\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.02 -0.105 -0.031 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.002\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.016 -0.136 -0.026 output tensor([[5, 1]]) selected_action: tensor([0]) rew: -0.279\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.012 -0.162 -0.021 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.298\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.008 -0.183 -0.016 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.298\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.004 -0.199 -0.012 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.28\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.001 -0.211 -0.007 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.242\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -5.185639907526133\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 750 lasted for 11 time steps with total reward of -4.4576300423232915\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.003 0.02 -0.007 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.875\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.007 0.014 -0.012 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.736\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.003 0.001 -0.006 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.625\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.0 -0.005 -0.001 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.833\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.004 -0.006 0.005 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.959\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.008 -0.001 0.011 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.841\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.004 0.01 0.005 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.721\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.008 0.015 0.011 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.822\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.012 0.026 0.017 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.65\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.008 0.043 0.011 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.448\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.004 0.055 0.006 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.509\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.0 0.061 0.0 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.594\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.004 0.061 0.006 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.704\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.001 0.067 0.001 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.547\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 0.003 0.068 -0.004 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.652\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 0.007 0.064 -0.01 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.564\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 0.011 0.054 -0.015 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.45\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 0.015 0.039 -0.021 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.36\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.019 0.018 -0.026 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.296\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 0.015 -0.008 -0.02 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.254\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.019 -0.028 -0.026 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.453\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.023 -0.055 -0.032 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 0.208\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.019 -0.087 -0.027 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.068\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.023 -0.114 -0.033 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.085\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.019 -0.147 -0.028 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.372\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.023 -0.175 -0.035 output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.402\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.111 0.019 -0.21 -0.03 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.704\n",
            "output tensor([[2, 8]]) selected_action: tensor([1]) rew: -5.75284317231314\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 751 lasted for 28 time steps with total reward of 5.7170460624112405\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.003 0.021 -0.006 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.898\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.007 0.015 -0.012 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.755\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.011 0.003 -0.017 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.64\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.007 -0.014 -0.011 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 0.552\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.003 -0.026 -0.006 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.007 -0.031 -0.012 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.735\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.011 -0.043 -0.018 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.557\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.015 -0.061 -0.024 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.35\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 0.019 -0.085 -0.03 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.112\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.123 0.023 -0.115 -0.036 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.158\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.146 0.027 -0.151 -0.043 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.46\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.173 0.031 -0.194 -0.05 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.797\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.203 0.035 -0.244 -0.057 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: -1.17\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: -6.580374709030692\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 752 lasted for 14 time steps with total reward of -3.922030110904141\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.003 0.029 -0.005 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.853\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.007 0.023 -0.011 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.736\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.003 0.013 -0.005 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.618\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.001 0.008 0.001 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.82\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.003 0.009 -0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.934\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.001 0.004 0.001 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.841\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.003 0.006 -0.005 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.948\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.007 0.001 -0.01 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.859\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.011 -0.009 -0.016 output tensor([[ 6, 11]]) selected_action: tensor([1]) rew: 0.735\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.015 -0.026 -0.022 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 0.549\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.019 -0.048 -0.028 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.323\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.015 -0.076 -0.023 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.067\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.019 -0.099 -0.029 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.071\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.023 -0.128 -0.035 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: -0.195\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.136 0.027 -0.163 -0.042 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.494\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.162 0.023 -0.205 -0.037 output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.827\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.185 0.019 -0.242 -0.033 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.909\n",
            "output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: -5.974895986280111\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 753 lasted for 18 time steps with total reward of -0.04513855517794596\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.004 -0.0 0.005 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.98\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.008 0.005 0.011 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.87\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.004 0.016 0.005 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.703\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.0 0.021 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.796\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.004 0.02 -0.006 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.886\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.0 0.014 -0.0 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.745\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.004 0.014 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.925\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.0 0.007 -0.0 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.783\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.004 0.007 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.96\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.008 0.013 0.012 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 0.822\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.012 0.025 0.018 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.647\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.008 0.042 0.012 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.443\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.004 0.054 0.006 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.501\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.0 0.06 0.001 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.584\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.004 0.061 -0.005 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.692\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 0.0 0.057 0.002 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.591\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 0.004 0.058 -0.004 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.691\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.0 0.054 0.002 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.624\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.004 0.056 0.008 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.684\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.0 0.065 0.003 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.519\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.004 0.068 0.009 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.616\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.008 0.077 0.015 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.445\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.012 0.093 0.022 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.244\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.008 0.114 0.017 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.012\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.012 0.131 0.023 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.039\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.016 0.154 0.03 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.203\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.02 0.184 0.036 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.478\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.016 0.22 0.032 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.789\n",
            "output tensor([[7, 3]]) selected_action: tensor([0]) rew: -5.847416252742886\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 754 lasted for 29 time steps with total reward of 8.484884984000107\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.004 -0.024 0.007 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.86\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.007 -0.017 0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.722\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.004 -0.005 0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.611\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.007 0.001 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.011 0.013 0.018 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.691\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.015 0.031 0.024 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 0.487\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.011 0.055 0.018 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.253\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.015 0.073 0.024 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.28\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.019 0.098 0.031 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.038\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.023 0.129 0.037 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.236\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.027 0.166 0.044 output tensor([[9, 0]]) selected_action: tensor([0]) rew: -0.544\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -0.031 0.21 0.051 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.886\n",
            "output tensor([[4, 7]]) selected_action: tensor([1]) rew: -6.264939463579289\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 755 lasted for 13 time steps with total reward of -3.1736183468767503\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.005 0.024 0.005 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.857\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.001 0.029 -0.001 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.758\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.005 0.029 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.845\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.008 0.034 0.011 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.728\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.012 0.045 0.017 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.552\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.016 0.063 0.024 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.346\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.012 0.087 0.018 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.109\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.009 0.105 0.013 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.132\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.012 0.118 0.019 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.178\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.009 0.137 0.014 output tensor([[7, 8]]) selected_action: tensor([1]) rew: -0.046\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.005 0.151 0.009 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.012\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.009 0.161 0.016 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.043\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.013 0.177 0.023 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.17\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.017 0.2 0.03 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.021 0.229 0.037 output tensor([[14,  0]]) selected_action: tensor([0]) rew: -0.696\n",
            "output tensor([[10,  1]]) selected_action: tensor([0]) rew: -6.011937183387765\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 756 lasted for 16 time steps with total reward of -2.80328239999698\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.003 -0.038 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.808\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.007 -0.044 -0.013 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.652\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.003 -0.057 -0.007 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.468\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.007 -0.064 -0.013 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.003 -0.078 -0.008 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.357\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.007 -0.086 -0.014 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.429\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.011 -0.1 -0.021 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.232\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.007 -0.121 -0.015 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.005\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.011 -0.136 -0.022 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.036\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.008 -0.158 -0.017 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.202\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.004 -0.175 -0.012 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.184\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.008 -0.188 -0.019 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.146\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.004 -0.207 -0.015 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.376\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 -0.0 -0.222 -0.01 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.353\n",
            "output tensor([[6, 7]]) selected_action: tensor([1]) rew: -5.312589402669281\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 757 lasted for 15 time steps with total reward of -3.0395573906984827\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.004 0.008 0.006 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.954\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.008 0.014 0.012 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.804\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.012 0.026 0.018 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.627\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.016 0.045 0.024 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.42\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.012 0.069 0.019 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.183\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.008 0.087 0.013 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.012 0.101 0.02 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.253\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.016 0.12 0.026 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.03\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.012 0.146 0.021 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.225\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.016 0.167 0.028 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.222\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.012 0.195 0.023 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.49\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.016 0.218 0.03 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.504\n",
            "output tensor([[3, 7]]) selected_action: tensor([1]) rew: -5.787088255273123\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 758 lasted for 13 time steps with total reward of -3.7526910753543454\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.004 -0.021 0.006 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.892\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.0 -0.015 -0.0 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.75\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.004 -0.015 0.006 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.926\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.008 -0.01 0.011 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.784\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.012 0.002 0.017 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.667\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.016 0.019 0.023 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.559\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.02 0.042 0.029 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.33\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.016 0.071 0.024 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.071\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.012 0.095 0.018 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.071\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.008 0.113 0.013 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.094\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.004 0.126 0.008 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.138\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.008 0.134 0.014 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.204\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.012 0.148 0.021 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.002\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.008 0.169 0.016 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.233\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.012 0.185 0.023 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.212\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.016 0.208 0.03 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.459\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.012 0.238 0.025 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.741\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: -5.77347282354069\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 759 lasted for 18 time steps with total reward of -1.9302660901612523\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.003 -0.024 0.006 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.885\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.007 -0.018 0.011 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.743\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.011 -0.007 0.017 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.007 0.01 0.011 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.539\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.003 0.022 0.005 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.001 0.027 -0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.76\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.004 0.027 -0.006 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.864\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.0 0.021 0.0 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.724\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.003 0.021 0.006 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 0.897\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.0 0.027 0.0 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.747\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.003 0.028 0.006 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 0.861\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.007 0.034 0.012 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.709\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.003 0.046 0.007 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 0.528\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 0.0 0.053 0.001 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 0.004 0.054 -0.004 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.716\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.008 0.05 -0.01 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.633\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.012 0.04 -0.015 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.516\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.008 0.025 -0.009 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.425\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.004 0.016 -0.003 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.65\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.0 0.013 0.003 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.845\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.004 0.015 0.009 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.871\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.0 0.024 0.003 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.71\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.004 0.027 -0.003 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.812\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.0 0.024 0.003 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.802\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.004 0.027 0.009 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.804\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.0 0.037 0.004 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.638\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.004 0.04 -0.002 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.736\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.0 0.038 0.004 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.757\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.004 0.042 -0.002 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.717\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.0 0.041 0.005 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.76\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.004 0.045 0.011 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.692\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.0 0.056 0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.518\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.004 0.061 0.011 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.606\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.008 0.072 0.018 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.427\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.011 0.09 0.024 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.217\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.015 0.114 0.03 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.024\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.012 0.144 0.025 output tensor([[1, 6]]) selected_action: tensor([1]) rew: -0.297\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.015 0.169 0.032 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.314\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.012 0.201 0.027 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.601\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.008 0.228 0.023 output tensor([[2, 8]]) selected_action: tensor([1]) rew: -0.635\n",
            "output tensor([[7, 3]]) selected_action: tensor([0]) rew: -5.651809768256202\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 760 lasted for 41 time steps with total reward of 16.492250845938287\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.005 0.031 0.006 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.841\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.009 0.036 0.012 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 0.712\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.005 0.048 0.006 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.534\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.001 0.054 0.001 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.618\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.003 0.055 -0.005 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.728\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.007 0.05 -0.01 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.614\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.003 0.039 -0.004 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.501\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.001 0.035 0.002 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.705\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.005 0.037 0.008 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.788\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.009 0.045 0.014 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.013 0.059 0.02 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 0.439\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.009 0.079 0.015 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.219\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.005 0.093 0.009 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.26\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.001 0.102 0.004 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.324\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.003 0.106 -0.001 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.411\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.001 0.105 0.005 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.463\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.005 0.111 0.012 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.364\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.009 0.122 0.018 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.177\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.013 0.141 0.025 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.042\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.017 0.166 0.032 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.294\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.021 0.197 0.038 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.579\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.025 0.235 0.045 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.899\n",
            "output tensor([[3, 6]]) selected_action: tensor([1]) rew: -6.255811705545579\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 761 lasted for 23 time steps with total reward of 1.2559279587843966\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.003 0.014 0.005 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 0.915\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.007 0.019 0.011 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.801\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.011 0.03 0.017 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.015 0.048 0.023 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.425\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.019 0.071 0.029 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 0.191\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.015 0.1 0.024 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.073\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.011 0.124 0.019 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.079\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -0.015 0.143 0.025 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.064\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.019 0.168 0.032 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.023 0.201 0.039 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.606\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.15 -0.019 0.239 0.034 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.929\n",
            "output tensor([[3, 7]]) selected_action: tensor([1]) rew: -6.002777577800043\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 762 lasted for 12 time steps with total reward of -5.111643427252411\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.005 0.047 0.006 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.775\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.001 0.053 0.001 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.003 0.054 -0.005 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.001 0.049 0.001 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.623\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.003 0.05 -0.004 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.733\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.001 0.046 0.002 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.005 0.048 0.008 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.001 0.056 0.003 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.003 0.059 -0.003 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.667\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.007 0.056 -0.008 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.647\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.003 0.047 -0.002 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.524\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.001 0.045 0.004 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.719\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.003 0.049 -0.002 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.685\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.007 0.048 -0.007 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.725\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.003 0.04 -0.001 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.594\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.001 0.04 0.005 output tensor([[18,  3]]) selected_action: tensor([0]) rew: 0.782\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.005 0.045 0.011 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.684\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.009 0.056 0.017 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.507\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.013 0.073 0.024 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.3\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.017 0.097 0.03 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.062\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.021 0.127 0.036 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.207\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.025 0.163 0.043 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.51\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.021 0.206 0.038 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.848\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.017 0.244 0.034 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.934\n",
            "output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: -6.004074732415287\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 763 lasted for 25 time steps with total reward of 3.826101195319101\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.005 -0.004 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.982\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.001 0.002 -0.0 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.837\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.003 0.002 -0.006 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.988\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.001 -0.004 -0.0 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.842\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.005 -0.004 0.006 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.981\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.008 0.002 0.012 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 0.837\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.012 0.014 0.017 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.701\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.008 0.031 0.012 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.498\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.012 0.043 0.018 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.559\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.008 0.06 0.012 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.352\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.005 0.073 0.007 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.407\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.008 0.079 0.013 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.485\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.012 0.092 0.019 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.296\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.009 0.112 0.014 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.012 0.126 0.021 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.114\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.016 0.146 0.027 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.116\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.02 0.174 0.034 output tensor([[8, 5]]) selected_action: tensor([0]) rew: -0.379\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.016 0.208 0.029 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.676\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.16 -0.013 0.237 0.025 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.721\n",
            "output tensor([[4, 2]]) selected_action: tensor([0]) rew: -5.750028015387973\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 764 lasted for 20 time steps with total reward of 1.3115242029102827\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.005 0.01 0.005 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.93\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.009 0.015 0.011 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.825\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.012 0.026 0.017 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.653\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.016 0.043 0.023 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.451\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.012 0.066 0.017 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.22\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.009 0.083 0.012 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.249\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.012 0.095 0.018 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.302\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.016 0.114 0.025 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.086\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.013 0.139 0.02 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.163\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.016 0.158 0.026 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.154\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.02 0.185 0.033 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.414\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.017 0.218 0.028 output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.709\n",
            "output tensor([[2, 3]]) selected_action: tensor([1]) rew: -5.751593444929578\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 765 lasted for 13 time steps with total reward of -3.4745084297256383\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.004 -0.016 -0.007 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.91\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.0 -0.023 -0.001 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.759\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.004 -0.024 0.005 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 0.871\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.007 -0.019 0.011 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.764\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.011 -0.008 0.016 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.007 0.008 0.01 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.552\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.004 0.019 0.005 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.699\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.007 0.023 0.011 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.794\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.011 0.034 0.017 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.007 0.051 0.011 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.422\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.004 0.062 0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 0.0 0.067 0.0 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.569\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.004 0.067 0.006 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.679\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.008 0.073 0.013 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.522\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.011 0.086 0.019 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.336\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.008 0.105 0.014 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.119\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.011 0.118 0.02 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.161\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.015 0.138 0.027 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.066\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.019 0.165 0.033 output tensor([[7, 3]]) selected_action: tensor([0]) rew: -0.325\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -0.023 0.198 0.04 output tensor([[7, 1]]) selected_action: tensor([0]) rew: -0.618\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.157 -0.027 0.238 0.047 output tensor([[10,  1]]) selected_action: tensor([0]) rew: -0.946\n",
            "output tensor([[16,  0]]) selected_action: tensor([0]) rew: -6.310699076194293\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 766 lasted for 22 time steps with total reward of 1.6431670106372458\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.003 0.025 0.007 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.863\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.007 0.032 0.013 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.709\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.011 0.045 0.019 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.526\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.015 0.064 0.025 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.313\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.011 0.089 0.02 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.07\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.015 0.108 0.026 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.086\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.011 0.134 0.021 output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.167\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.007 0.155 0.016 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.163\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.011 0.171 0.023 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.139\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.015 0.194 0.029 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.384\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.011 0.223 0.025 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.662\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: -5.690029963049431\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 767 lasted for 12 time steps with total reward of -4.638575452020008\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.003 -0.022 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.878\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.007 -0.017 0.011 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.776\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.011 -0.006 0.016 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.007 0.01 0.01 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.563\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.003 0.02 0.005 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.693\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.007 0.025 0.011 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.788\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.011 0.036 0.017 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.616\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.015 0.052 0.023 output tensor([[19,  1]]) selected_action: tensor([0]) rew: 0.416\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.019 0.075 0.029 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.185\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.023 0.104 0.035 output tensor([[13,  2]]) selected_action: tensor([0]) rew: -0.078\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.019 0.139 0.03 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.373\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.163 -0.023 0.169 0.037 output tensor([[8, 2]]) selected_action: tensor([0]) rew: -0.411\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.186 -0.027 0.205 0.043 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.721\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.213 -0.023 0.249 0.039 output tensor([[0, 9]]) selected_action: tensor([1]) rew: -1.066\n",
            "output tensor([[3, 3]]) selected_action: tensor([0]) rew: -6.1613676170876985\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 768 lasted for 15 time steps with total reward of -3.2395556885053347\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.005 0.006 0.006 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.967\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.009 0.012 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.819\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.012 0.024 0.018 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.642\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.009 0.042 0.012 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.005 0.054 0.007 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.493\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.009 0.061 0.013 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.574\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.005 0.074 0.007 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.387\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.009 0.081 0.014 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.462\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.013 0.095 0.02 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.269\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.016 0.115 0.026 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.045\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.02 0.142 0.033 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.212\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.024 0.175 0.04 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.501\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.02 0.214 0.035 output tensor([[7, 8]]) selected_action: tensor([1]) rew: -0.826\n",
            "output tensor([[2, 5]]) selected_action: tensor([1]) rew: -5.898085931536868\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 769 lasted for 14 time steps with total reward of -2.3429207880567917\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.005 -0.031 -0.006 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.847\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.001 -0.037 -0.0 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.704\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.005 -0.037 -0.006 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.009 -0.044 -0.012 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.665\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.013 -0.056 -0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.482\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.017 -0.074 -0.025 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.27\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.021 -0.099 -0.031 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.027\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.024 -0.13 -0.037 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.249\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.021 -0.168 -0.032 output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.558\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.017 -0.2 -0.028 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.612\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.013 -0.228 -0.023 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.649\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: -5.66903691187555\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 770 lasted for 12 time steps with total reward of -3.9231820301543365\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.004 0.024 -0.007 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.856\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.007 0.018 -0.012 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.718\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.004 0.005 -0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.607\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.007 -0.001 -0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.815\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.004 -0.013 -0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.692\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.007 -0.019 -0.012 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.004 -0.032 -0.007 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.601\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.0 -0.038 -0.001 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.686\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.004 -0.039 -0.007 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.796\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.007 -0.046 -0.013 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.64\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.004 -0.059 -0.007 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.454\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.007 -0.067 -0.014 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.531\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.011 -0.08 -0.02 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.34\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.008 -0.1 -0.015 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.118\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.004 -0.115 -0.009 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.156\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.008 -0.124 -0.016 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.215\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.004 -0.14 -0.011 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 0.008 -0.151 -0.018 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.056\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.111 0.004 -0.169 -0.013 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.163\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 -0.0 -0.182 -0.008 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.126\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 -0.004 -0.19 -0.003 output tensor([[9, 1]]) selected_action: tensor([0]) rew: -0.07\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.111 0.0 -0.193 -0.01 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.111 -0.004 -0.204 -0.006 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.183\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 -0.008 -0.21 -0.001 output tensor([[10,  1]]) selected_action: tensor([0]) rew: -0.119\n",
            "output tensor([[7, 7]]) selected_action: tensor([0]) rew: -5.036432343073268\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 771 lasted for 25 time steps with total reward of 3.376831776558892\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.004 -0.006 -0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.959\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.007 -0.013 -0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.809\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.011 -0.025 -0.018 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.631\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.015 -0.043 -0.024 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.424\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.011 -0.068 -0.019 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.187\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.008 -0.086 -0.013 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.21\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.011 -0.099 -0.02 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.257\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.008 -0.119 -0.014 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.034\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.012 -0.134 -0.021 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.07\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.015 -0.155 -0.028 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: -0.163\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.11 0.012 -0.182 -0.023 output tensor([[9, 6]]) selected_action: tensor([0]) rew: -0.429\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 0.008 -0.205 -0.018 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.44\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.129 0.004 -0.223 -0.014 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.434\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: -5.409910766834817\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 772 lasted for 14 time steps with total reward of -3.292742497827807\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.004 0.012 0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.933\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.008 0.018 0.012 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 0.803\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.012 0.029 0.017 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.016 0.047 0.024 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.424\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.02 0.07 0.03 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.19\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.016 0.1 0.024 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.076\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.02 0.124 0.031 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.083\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.016 0.155 0.026 output tensor([[1, 4]]) selected_action: tensor([1]) rew: -0.359\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.146 -0.02 0.18 0.032 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.38\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.166 -0.024 0.213 0.039 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.671\n",
            "output tensor([[8, 3]]) selected_action: tensor([0]) rew: -5.997453423190725\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 773 lasted for 11 time steps with total reward of -4.5883527781922675\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.004 0.015 0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.927\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.0 0.021 0.0 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.779\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.004 0.021 -0.006 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.894\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.008 0.016 -0.011 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.761\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.004 0.005 -0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.644\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.008 -0.001 -0.011 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.847\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.012 -0.012 -0.017 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.719\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.015 -0.029 -0.023 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.52\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.019 -0.052 -0.029 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.291\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.015 -0.081 -0.023 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.031\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.012 -0.104 -0.018 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.031\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.124 0.016 -0.122 -0.025 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.052\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.139 0.019 -0.147 -0.031 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: -0.196\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.159 0.016 -0.178 -0.026 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.174 0.02 -0.204 -0.033 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.504\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.194 0.016 -0.237 -0.029 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.8\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: -5.84658219763296\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 774 lasted for 17 time steps with total reward of -1.3276628421718968\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.005 -0.024 -0.006 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.878\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.001 -0.03 -0.0 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.74\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.003 -0.03 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.855\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.001 -0.024 -0.0 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.717\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.005 -0.025 -0.006 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.871\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.009 -0.031 -0.012 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 0.719\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.005 -0.044 -0.007 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.538\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.009 -0.051 -0.013 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 0.619\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.013 -0.064 -0.019 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.434\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.009 -0.083 -0.014 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.218\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.013 -0.097 -0.02 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.262\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.017 -0.117 -0.026 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.038\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.02 -0.143 -0.033 output tensor([[1, 4]]) selected_action: tensor([1]) rew: -0.218\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 0.024 -0.176 -0.04 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: -0.508\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.161 0.028 -0.216 -0.047 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.832\n",
            "output tensor([[3, 7]]) selected_action: tensor([1]) rew: -6.192476558205117\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 775 lasted for 16 time steps with total reward of -0.8621625305490133\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.005 -0.027 -0.006 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.862\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.008 -0.034 -0.012 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.71\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.012 -0.046 -0.018 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.529\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.016 -0.065 -0.025 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.012 -0.089 -0.019 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.077\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.016 -0.108 -0.026 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.095\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.012 -0.134 -0.02 output tensor([[8, 7]]) selected_action: tensor([0]) rew: -0.156\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.009 -0.154 -0.015 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.149\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.013 -0.17 -0.022 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.123\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.016 -0.192 -0.029 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: -0.365\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.013 -0.221 -0.024 output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.641\n",
            "output tensor([[6, 9]]) selected_action: tensor([1]) rew: -5.666053788899292\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 776 lasted for 12 time steps with total reward of -4.508290948474853\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.004 0.044 -0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.775\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.0 0.039 0.001 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.673\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.004 0.04 0.007 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 0.78\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.0 0.048 0.002 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.004 0.05 0.008 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.725\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.0 0.058 0.002 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.563\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.004 0.06 -0.003 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.663\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.008 0.057 -0.008 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.638\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.012 0.049 -0.014 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.516\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.016 0.035 -0.019 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.419\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.012 0.015 -0.013 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.347\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.016 0.002 -0.019 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.592\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.012 -0.017 -0.013 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.512\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.008 -0.031 -0.008 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.004 -0.038 -0.002 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.665\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.008 -0.04 -0.008 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.771\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.004 -0.048 -0.002 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 0.0 -0.05 0.003 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.711\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 -0.004 -0.047 0.009 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.681\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 -0.008 -0.039 0.014 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.557\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 -0.004 -0.024 0.008 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.46\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 -0.008 -0.016 0.014 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.68\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 -0.004 -0.002 0.008 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.577\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.0 0.006 0.002 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.792\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.004 0.008 -0.004 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.923\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 0.008 0.004 -0.01 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.869\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 0.004 -0.006 -0.004 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.742\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.107 0.008 -0.01 -0.01 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.879\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.004 -0.019 -0.004 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.714\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.119 0.008 -0.023 -0.01 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.813\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 0.012 -0.033 -0.016 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.139 0.016 -0.049 -0.022 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.449\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.155 0.02 -0.07 -0.028 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 0.222\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.175 0.016 -0.098 -0.023 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.036\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.19 0.02 -0.121 -0.029 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: -0.035\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.21 0.016 -0.15 -0.024 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.304\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.226 0.02 -0.174 -0.031 output tensor([[6, 8]]) selected_action: tensor([1]) rew: -0.316\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.246 0.016 -0.205 -0.026 output tensor([[8, 8]]) selected_action: tensor([0]) rew: -0.599\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.262 0.02 -0.231 -0.033 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.629\n",
            "output tensor([[3, 4]]) selected_action: tensor([1]) rew: -5.927436953578167\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 777 lasted for 40 time steps with total reward of 13.320910001370471\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.004 -0.028 -0.006 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.86\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.0 -0.034 -0.0 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.723\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.004 -0.034 0.006 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.837\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.007 -0.028 0.011 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.701\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.004 -0.017 0.005 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.586\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.007 -0.012 0.011 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.79\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.011 -0.001 0.017 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.671\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.007 0.016 0.011 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.579\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.004 0.027 0.005 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.654\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.007 0.032 0.011 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.746\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.011 0.043 0.017 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 0.571\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.015 0.06 0.023 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.367\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.011 0.083 0.018 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.132\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.008 0.101 0.012 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.158\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.004 0.113 0.007 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.206\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.0 0.121 0.002 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.276\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.004 0.123 0.009 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.369\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.008 0.132 0.015 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.194\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.004 0.147 0.01 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.012\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.008 0.157 0.017 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.039\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.004 0.174 0.012 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.178\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.008 0.187 0.019 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.14\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.004 0.206 0.015 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.37\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.008 0.22 0.022 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.347\n",
            "output tensor([[3, 5]]) selected_action: tensor([1]) rew: -5.591276929540421\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 778 lasted for 25 time steps with total reward of 2.820858904752879\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.004 0.027 -0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.868\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.008 0.022 -0.011 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.727\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.004 0.01 -0.005 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.612\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.0 0.005 0.0 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.004 0.005 -0.005 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.965\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.0 -0.0 0.001 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.842\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.004 0.0 0.006 output tensor([[8, 8]]) selected_action: tensor([0]) rew: 0.985\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.0 0.007 0.001 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 0.838\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.004 0.007 0.006 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.954\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.008 0.014 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.804\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.004 0.026 0.007 output tensor([[7, 8]]) selected_action: tensor([1]) rew: 0.625\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.008 0.033 0.013 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.711\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.012 0.045 0.019 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.529\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.016 0.064 0.025 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.317\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.012 0.089 0.019 output tensor([[7, 9]]) selected_action: tensor([1]) rew: 0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.016 0.108 0.026 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.093\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.012 0.134 0.021 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.159\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.008 0.154 0.016 output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.154\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.012 0.17 0.022 output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.128\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.016 0.192 0.029 output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.371\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.02 0.222 0.036 output tensor([[9, 1]]) selected_action: tensor([0]) rew: -0.649\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -5.9610404529618926\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 779 lasted for 22 time steps with total reward of 3.338329258315536\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.003 0.014 -0.006 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.93\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.001 0.008 0.0 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.786\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.005 0.009 0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.957\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.009 0.015 0.012 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 0.809\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.005 0.026 0.006 output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.632\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.009 0.033 0.012 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.72\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.013 0.045 0.018 output tensor([[9, 5]]) selected_action: tensor([0]) rew: 0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.017 0.063 0.024 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.33\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.02 0.087 0.031 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.09\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.024 0.118 0.037 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.182\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.028 0.155 0.044 output tensor([[4, 1]]) selected_action: tensor([0]) rew: -0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.032 0.198 0.05 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.828\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.028 0.249 0.046 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -1.204\n",
            "output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: -6.330452690497369\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 780 lasted for 14 time steps with total reward of -3.23883793988185\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.004 -0.026 0.005 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.863\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.0 -0.021 -0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.747\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.004 -0.022 0.005 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.877\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.008 -0.017 0.011 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.775\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.004 -0.007 0.005 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.655\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.008 -0.002 0.01 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.854\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.012 0.008 0.016 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 0.731\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.008 0.025 0.01 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.554\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.004 0.035 0.005 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.622\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.008 0.04 0.011 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.714\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.004 0.051 0.005 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.54\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.0 0.056 -0.0 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.004 0.055 0.006 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.726\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.008 0.061 0.012 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.588\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.004 0.073 0.007 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.406\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.0 0.08 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.484\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.004 0.081 -0.004 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.587\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.0 0.077 0.002 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.51\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.004 0.079 0.009 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.575\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.0 0.088 0.003 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.407\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.004 0.091 0.01 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.499\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.0 0.101 0.004 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.324\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.004 0.105 0.011 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.409\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.008 0.116 0.017 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.227\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.012 0.133 0.024 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.013\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.008 0.157 0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.233\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.012 0.176 0.026 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.222\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.008 0.202 0.021 output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.481\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.004 0.223 0.017 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -0.487\n",
            "output tensor([[6, 6]]) selected_action: tensor([0]) rew: -5.475785648047029\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 781 lasted for 30 time steps with total reward of 7.416976227129388\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.004 -0.032 0.005 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.833\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.008 -0.027 0.011 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.721\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.004 -0.017 0.005 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.603\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.008 -0.012 0.01 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.805\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.012 -0.002 0.016 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.683\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.015 0.015 0.022 output tensor([[21,  1]]) selected_action: tensor([0]) rew: 0.589\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.019 0.037 0.028 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.38\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.023 0.065 0.034 output tensor([[13,  4]]) selected_action: tensor([0]) rew: 0.126\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.019 0.099 0.029 output tensor([[1, 2]]) selected_action: tensor([1]) rew: -0.16\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.023 0.127 0.035 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.186\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.027 0.162 0.042 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.484\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -0.023 0.204 0.037 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.816\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.196 -0.019 0.241 0.032 output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.895\n",
            "output tensor([[7, 9]]) selected_action: tensor([1]) rew: -5.959700295172249\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 782 lasted for 14 time steps with total reward of -3.7604304460309708\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.004 0.011 -0.006 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.936\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.0 0.005 -0.0 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.793\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.004 0.005 0.006 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.97\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.008 0.011 0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.834\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.004 0.022 0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.66\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.0 0.028 0.0 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.749\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.004 0.028 0.006 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.865\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.008 0.034 0.012 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.714\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.012 0.046 0.018 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.534\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.015 0.064 0.024 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.324\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.012 0.089 0.019 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.084\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.008 0.108 0.014 output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.104\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.004 0.121 0.008 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.145\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 0.0 0.13 0.003 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.209\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 0.004 0.133 -0.002 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.294\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 0.0 0.132 0.005 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.325\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 0.004 0.137 0.0 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.244\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.0 0.137 0.007 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.344\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.004 0.144 0.013 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.177\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.008 0.157 0.02 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.022\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.004 0.177 0.015 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.253\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.008 0.193 0.022 output tensor([[5, 2]]) selected_action: tensor([0]) rew: -0.229\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.012 0.215 0.029 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.474\n",
            "output tensor([[9, 4]]) selected_action: tensor([0]) rew: -5.752723715208191\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 783 lasted for 24 time steps with total reward of 2.5747033387973914\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.005 0.039 0.006 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.81\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.001 0.045 0.001 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.005 0.046 0.007 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.766\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.001 0.053 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.003 0.054 -0.004 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.001 0.05 0.002 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.637\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.005 0.052 0.008 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.712\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.009 0.06 0.014 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.548\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.005 0.074 0.009 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.355\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.001 0.083 0.003 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.423\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 0.003 0.087 -0.002 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.515\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.001 0.085 0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.541\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.005 0.089 0.011 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.481\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.001 0.1 0.006 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 0.003 0.106 0.0 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.379\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 0.007 0.107 -0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.482\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 0.011 0.102 -0.01 output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.374\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.007 0.092 -0.003 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.268\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.003 0.089 0.003 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.001 0.092 0.009 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.5\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.005 0.101 0.016 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.326\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.009 0.117 0.022 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.121\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.013 0.139 0.029 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.116\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.017 0.168 0.035 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.386\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.021 0.204 0.042 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.69\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.017 0.246 0.038 output tensor([[1, 6]]) selected_action: tensor([1]) rew: -1.029\n",
            "output tensor([[6, 4]]) selected_action: tensor([0]) rew: -6.119259820232325\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 784 lasted for 27 time steps with total reward of 2.655665541676867\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.003 -0.029 -0.007 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.853\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.0 -0.035 -0.001 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.7\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.004 -0.036 0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.811\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.0 -0.031 -0.001 output tensor([[ 1, 19]]) selected_action: tensor([1]) rew: 0.708\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.004 -0.033 0.004 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.818\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.008 -0.028 0.01 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.735\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.012 -0.018 0.016 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.615\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.008 -0.003 0.01 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.521\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.012 0.007 0.016 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.745\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.016 0.023 0.021 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.577\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.02 0.044 0.027 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.355\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.024 0.072 0.034 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.103\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.02 0.105 0.028 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.181\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -0.016 0.133 0.023 output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.207\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.15 -0.02 0.156 0.03 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.212\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.17 -0.024 0.186 0.036 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.194 -0.02 0.222 0.032 output tensor([[3, 8]]) selected_action: tensor([1]) rew: -0.798\n",
            "output tensor([[7, 3]]) selected_action: tensor([0]) rew: -5.856651416989584\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 785 lasted for 18 time steps with total reward of -0.20163194910558424\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.004 0.025 0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.856\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.0 0.031 -0.0 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.747\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.004 0.03 -0.006 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.842\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.0 0.024 -0.0 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.703\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.004 0.024 -0.006 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.883\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.008 0.018 -0.011 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 0.741\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.004 0.007 -0.005 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.626\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.0 0.001 0.0 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.83\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.004 0.002 0.006 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.983\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.008 0.008 0.012 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.834\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.004 0.02 0.006 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.658\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.008 0.027 0.012 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.745\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.004 0.039 0.007 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.565\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.008 0.046 0.013 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.649\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.012 0.058 0.019 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.465\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.008 0.077 0.013 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.25\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.012 0.09 0.02 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.297\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.008 0.11 0.014 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.004 0.125 0.009 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.113\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.008 0.134 0.016 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.172\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.004 0.15 0.011 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.037\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.0 0.161 0.006 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.011\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 0.003 0.167 0.001 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.079\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 0.007 0.168 -0.003 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.169\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 0.003 0.165 0.003 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.113\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 0.007 0.168 -0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.126\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 0.003 0.167 0.006 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.164\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.001 0.173 0.012 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.064\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 0.003 0.185 0.008 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.133\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.001 0.193 0.015 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.075\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.005 0.207 0.021 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.284\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.009 0.229 0.028 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.527\n",
            "output tensor([[5, 8]]) selected_action: tensor([1]) rew: -5.80428587035395\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 786 lasted for 33 time steps with total reward of 5.9014755589860135\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.004 0.034 0.006 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 0.836\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.008 0.04 0.012 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.685\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.012 0.052 0.018 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.504\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.008 0.071 0.013 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.294\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.004 0.083 0.007 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.345\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.008 0.091 0.014 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.419\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.012 0.104 0.02 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.225\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.016 0.124 0.027 output tensor([[6, 3]]) selected_action: tensor([0]) rew: -0.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.012 0.151 0.022 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.258\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.008 0.173 0.017 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.259\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.012 0.189 0.024 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.241\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.008 0.213 0.019 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.491\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: -5.489236654599678\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 787 lasted for 13 time steps with total reward of -3.4296697614274043\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.003 0.031 -0.005 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.844\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.007 0.026 -0.011 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.722\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 0.011 0.015 -0.017 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 0.606\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.015 -0.002 -0.022 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 0.515\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.011 -0.024 -0.016 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.433\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.015 -0.041 -0.022 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.473\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.019 -0.063 -0.029 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.244\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.023 -0.092 -0.035 output tensor([[2, 9]]) selected_action: tensor([1]) rew: -0.016\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.019 -0.126 -0.03 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.308\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.015 -0.156 -0.025 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.343\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.011 -0.181 -0.02 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.359\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.007 -0.2 -0.015 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.356\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 0.011 -0.215 -0.022 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.335\n",
            "output tensor([[6, 7]]) selected_action: tensor([1]) rew: -5.58137969799345\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 788 lasted for 14 time steps with total reward of -3.461090353506942\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.004 0.018 -0.006 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 0.9\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.007 0.012 -0.012 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.759\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.011 -0.0 -0.018 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.007 -0.018 -0.012 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.555\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.011 -0.03 -0.018 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.617\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.007 -0.048 -0.012 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.411\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.011 -0.06 -0.018 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.467\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.008 -0.078 -0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.255\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.004 -0.091 -0.008 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.304\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.008 -0.099 -0.014 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.376\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.004 -0.113 -0.009 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.18\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.008 -0.121 -0.015 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.244\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 0.012 -0.137 -0.022 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.039\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.117 0.015 -0.158 -0.028 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.198\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.132 0.012 -0.187 -0.024 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.468\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.144 0.016 -0.211 -0.031 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.485\n",
            "output tensor([[5, 4]]) selected_action: tensor([0]) rew: -5.77017016540479\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 789 lasted for 17 time steps with total reward of -1.1696458233704998\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.004 0.016 0.007 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.908\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.008 0.022 0.013 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.755\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.004 0.035 0.007 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.573\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.008 0.042 0.013 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.655\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.004 0.055 0.008 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.469\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.008 0.063 0.014 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.546\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.004 0.077 0.008 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.355\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.0 0.085 0.003 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.426\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.004 0.088 0.009 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 0.52\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.008 0.097 0.016 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.346\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.012 0.113 0.022 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.142\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.008 0.135 0.017 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.093\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.012 0.152 0.024 output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.072\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.008 0.176 0.019 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.319\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.012 0.195 0.026 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.311\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.016 0.22 0.033 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.573\n",
            "output tensor([[6, 2]]) selected_action: tensor([0]) rew: -5.868633707037449\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 790 lasted for 17 time steps with total reward of -1.5410358980348589\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.004 0.023 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.874\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.008 0.017 -0.012 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.734\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.004 0.005 -0.006 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.621\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.0 -0.001 -0.0 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.827\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.004 -0.001 0.006 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.994\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.008 0.005 0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.851\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.012 0.017 0.018 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.686\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.016 0.034 0.023 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.012 0.058 0.018 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.251\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.016 0.075 0.024 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.279\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.02 0.099 0.03 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.039\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.016 0.13 0.025 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.233\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.119 -0.02 0.155 0.032 output tensor([[4, 4]]) selected_action: tensor([0]) rew: -0.248\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -0.024 0.187 0.038 output tensor([[6, 4]]) selected_action: tensor([0]) rew: -0.534\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -0.02 0.225 0.034 output tensor([[1, 3]]) selected_action: tensor([1]) rew: -0.853\n",
            "output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: -5.922532021564421\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 791 lasted for 16 time steps with total reward of -1.1515061366880488\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.004 0.023 -0.005 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.878\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.0 0.018 0.001 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.764\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.004 0.019 0.007 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.892\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.008 0.026 0.013 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.739\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.012 0.038 0.019 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.557\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.016 0.057 0.025 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.345\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.012 0.082 0.019 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.103\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.008 0.102 0.014 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.121\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.012 0.116 0.021 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.16\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.008 0.136 0.016 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.069\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.012 0.152 0.022 output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.04\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.008 0.174 0.017 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.28\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.004 0.192 0.013 output tensor([[2, 3]]) selected_action: tensor([1]) rew: -0.265\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.008 0.204 0.02 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.232\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.004 0.224 0.015 output tensor([[4, 7]]) selected_action: tensor([1]) rew: -0.465\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: -5.448014639548069\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 792 lasted for 16 time steps with total reward of -2.2401733827621664\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.004 -0.004 0.007 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 0.959\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.0 0.003 0.001 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.817\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.004 0.004 0.007 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.969\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.0 0.01 0.001 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.819\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.004 0.011 -0.005 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.933\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.008 0.006 -0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.822\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.004 -0.005 -0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.702\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.0 -0.01 0.001 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.853\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.004 -0.009 0.007 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.93\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.008 -0.002 0.012 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.79\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.012 0.01 0.018 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.677\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.016 0.028 0.024 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 0.494\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.02 0.053 0.03 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.259\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.024 0.083 0.036 output tensor([[11,  3]]) selected_action: tensor([0]) rew: -0.008\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.028 0.119 0.043 output tensor([[17,  1]]) selected_action: tensor([0]) rew: -0.306\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.032 0.162 0.049 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.639\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.035 0.211 0.056 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -1.007\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: -6.411020793144624\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 793 lasted for 18 time steps with total reward of 1.6525654958183207\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.003 -0.037 -0.006 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.824\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.001 -0.043 -0.0 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.675\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.005 -0.043 0.005 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.786\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.001 -0.038 -0.001 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.665\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.005 -0.039 0.005 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.796\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.008 -0.034 0.01 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.698\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.012 -0.024 0.016 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.58\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.008 -0.008 0.01 output tensor([[7, 9]]) selected_action: tensor([1]) rew: 0.489\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.004 0.002 0.004 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.001 0.006 -0.002 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.891\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.005 0.004 0.004 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.926\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.001 0.008 -0.002 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.88\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.005 0.006 0.004 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.917\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.008 0.01 0.01 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.867\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.005 0.02 0.004 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.7\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.001 0.025 -0.001 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.797\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 0.003 0.023 -0.007 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.845\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.001 0.016 -0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.709\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.005 0.015 0.005 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.894\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -0.008 0.02 0.011 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.81\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -0.005 0.03 0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.639\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.008 0.035 0.011 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 0.731\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 -0.012 0.046 0.017 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.556\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -0.016 0.063 0.023 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.352\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -0.02 0.087 0.029 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.117\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.187 -0.016 0.116 0.024 output tensor([[4, 8]]) selected_action: tensor([1]) rew: -0.15\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.203 -0.012 0.14 0.019 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.158\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.216 -0.009 0.159 0.014 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.146\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.224 -0.005 0.173 0.009 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.115\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.229 -0.009 0.183 0.016 output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.063\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.238 -0.005 0.199 0.012 output tensor([[3, 7]]) selected_action: tensor([1]) rew: -0.279\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.243 -0.009 0.211 0.019 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.242\n",
            "output tensor([[4, 7]]) selected_action: tensor([1]) rew: -5.471263882284435\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 794 lasted for 33 time steps with total reward of 11.234300905087775\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.005 -0.008 0.005 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.953\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.009 -0.002 0.011 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.005 0.009 0.005 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.709\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.001 0.014 -0.001 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.826\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.003 0.013 -0.006 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.921\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.007 0.007 -0.012 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.779\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 0.011 -0.005 -0.018 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.665\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.015 -0.023 -0.024 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.53\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.011 -0.046 -0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.298\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.007 -0.064 -0.012 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.327\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.003 -0.077 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.38\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.001 -0.084 -0.002 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.456\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.003 -0.086 -0.008 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.557\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.001 -0.094 -0.003 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.39\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.005 -0.097 0.002 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.009 -0.094 0.008 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.005 -0.086 0.001 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.359\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.001 -0.085 -0.005 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.556\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.005 -0.09 0.0 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.466\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.001 -0.09 -0.006 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.565\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.003 -0.096 -0.013 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.415\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.007 -0.109 -0.019 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.226\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.003 -0.128 -0.014 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.005\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.001 -0.142 -0.009 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.043\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.003 -0.151 -0.016 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.101\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.0 -0.166 -0.011 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.108\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.003 -0.177 -0.018 output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.062\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.007 -0.194 -0.024 output tensor([[2, 6]]) selected_action: tensor([1]) rew: -0.283\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.011 -0.219 -0.031 output tensor([[5, 8]]) selected_action: tensor([1]) rew: -0.538\n",
            "output tensor([[5, 6]]) selected_action: tensor([1]) rew: -5.827814834061435\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 795 lasted for 30 time steps with total reward of 5.497782791266628\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.004 -0.012 -0.007 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.924\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.0 -0.019 -0.001 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.772\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.004 -0.02 -0.007 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.884\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.008 -0.027 -0.013 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.73\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.004 -0.04 -0.007 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.547\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.008 -0.047 -0.013 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.627\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.004 -0.061 -0.008 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.44\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.0 -0.068 -0.002 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.515\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.004 -0.071 -0.009 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.614\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.008 -0.079 -0.015 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 0.446\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.004 -0.094 -0.01 output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.248\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.0 -0.104 -0.004 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 0.31\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.004 -0.108 -0.011 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.395\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.008 -0.119 -0.017 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.213\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.012 -0.136 -0.024 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.001\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.008 -0.16 -0.019 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.248\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 0.012 -0.179 -0.026 output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.238\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.117 0.008 -0.205 -0.021 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.498\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.125 0.012 -0.226 -0.028 output tensor([[2, 4]]) selected_action: tensor([1]) rew: -0.505\n",
            "output tensor([[7, 7]]) selected_action: tensor([0]) rew: -5.779835704862044\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 796 lasted for 20 time steps with total reward of 0.39574826441547817\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.003 -0.036 0.006 output tensor([[18,  3]]) selected_action: tensor([0]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.007 -0.03 0.011 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.69\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.011 -0.019 0.017 output tensor([[20,  1]]) selected_action: tensor([0]) rew: 0.576\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.015 -0.002 0.023 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.488\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.011 0.021 0.017 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 0.426\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.015 0.037 0.023 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.483\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.019 0.06 0.029 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.254\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.015 0.089 0.023 output tensor([[5, 7]]) selected_action: tensor([1]) rew: -0.007\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 -0.019 0.112 0.03 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.008\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.157 -0.023 0.142 0.036 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.279\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.179 -0.027 0.178 0.043 output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.583\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.206 -0.03 0.221 0.05 output tensor([[7, 5]]) selected_action: tensor([0]) rew: -0.923\n",
            "output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -6.298875469448831\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 797 lasted for 13 time steps with total reward of -4.352332261092206\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.004 0.033 0.006 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 0.825\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.008 0.039 0.012 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 0.702\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.004 0.05 0.006 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.524\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.0 0.056 0.0 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.004 0.057 -0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.718\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.008 0.052 -0.01 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.603\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.004 0.041 -0.004 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.489\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.008 0.037 -0.01 output tensor([[8, 9]]) selected_action: tensor([1]) rew: 0.694\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.004 0.027 -0.004 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.575\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.0 0.023 0.002 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.774\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.004 0.026 0.008 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.834\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.008 0.034 0.014 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.673\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.004 0.048 0.009 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.484\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.0 0.057 0.003 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.557\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.004 0.06 0.009 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.654\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.008 0.069 0.015 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.485\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.012 0.084 0.022 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.286\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.016 0.106 0.028 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.055\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.02 0.134 0.035 output tensor([[14,  0]]) selected_action: tensor([0]) rew: -0.207\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.024 0.169 0.041 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.503\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.028 0.21 0.048 output tensor([[7, 4]]) selected_action: tensor([0]) rew: -0.834\n",
            "output tensor([[5, 4]]) selected_action: tensor([0]) rew: -6.200332846961162\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 798 lasted for 22 time steps with total reward of 2.7967082889197723\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.004 -0.005 0.005 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.97\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.008 0.001 0.011 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.839\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.012 0.012 0.017 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.715\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.016 0.029 0.023 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.514\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.02 0.052 0.029 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.284\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.016 0.081 0.024 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.023\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.119 -0.012 0.105 0.018 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.022\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.016 0.123 0.025 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.042\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.02 0.148 0.031 output tensor([[8, 4]]) selected_action: tensor([0]) rew: -0.207\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -0.024 0.179 0.038 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.489\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.191 -0.028 0.217 0.045 output tensor([[6, 1]]) selected_action: tensor([0]) rew: -0.806\n",
            "output tensor([[17,  2]]) selected_action: tensor([0]) rew: -6.159113416461123\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 799 lasted for 12 time steps with total reward of -4.251910354738969\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.004 -0.019 0.005 output tensor([[9, 5]]) selected_action: tensor([0]) rew: 0.906\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.008 -0.013 0.011 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 0.774\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.012 -0.002 0.017 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 0.657\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.016 0.015 0.023 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.566\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.012 0.038 0.017 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.357\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.008 0.055 0.011 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.392\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.004 0.066 0.006 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.451\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.008 0.072 0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.534\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.012 0.085 0.019 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.349\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.016 0.103 0.025 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.133\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.012 0.128 0.02 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.114\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.016 0.148 0.026 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.104\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.02 0.174 0.033 output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.363\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.024 0.207 0.04 output tensor([[7, 2]]) selected_action: tensor([0]) rew: -0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.02 0.247 0.035 output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.984\n",
            "output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: -6.063224190809258\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 800 lasted for 16 time steps with total reward of -3.1643623679402904\n",
            "\n",
            "\n",
            " Model saved!\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.004 -0.049 -0.006 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.748\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.0 -0.054 0.0 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.628\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.004 -0.054 -0.006 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 0.74\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.0 -0.061 -0.001 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.586\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.004 -0.061 -0.007 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.694\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.0 -0.068 -0.001 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.535\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.004 -0.07 0.004 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.639\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.008 -0.066 0.009 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.568\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.011 -0.056 0.015 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.452\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.015 -0.041 0.02 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.362\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.011 -0.021 0.014 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.295\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.007 -0.007 0.008 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.545\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.011 0.001 0.014 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.762\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.007 0.015 0.008 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.644\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.004 0.023 0.002 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.724\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.007 0.026 0.008 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.829\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.011 0.034 0.014 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.667\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.015 0.049 0.02 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.011 0.069 0.015 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.256\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -0.008 0.084 0.01 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.297\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.004 0.093 0.004 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.361\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.008 0.098 0.011 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.448\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.004 0.108 0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.267\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.148 -0.008 0.114 0.012 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.347\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.012 0.126 0.018 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.159\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -0.008 0.144 0.013 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.062\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.174 -0.012 0.158 0.02 output tensor([[4, 2]]) selected_action: tensor([0]) rew: -0.024\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.186 -0.016 0.178 0.027 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.256\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.202 -0.02 0.205 0.034 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.521\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.221 -0.016 0.238 0.029 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.82\n",
            "output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: -5.870090340969897\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 801 lasted for 31 time steps with total reward of 5.476108560664633\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.004 0.019 0.006 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 0.905\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.0 0.025 0.0 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.764\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.004 0.025 0.006 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 0.879\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.008 0.031 0.012 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.729\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.004 0.043 0.006 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 0.549\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.0 0.05 0.001 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.633\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.004 0.05 0.007 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.742\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.0 0.057 0.002 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.583\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 0.004 0.059 -0.004 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.688\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.0 0.055 0.002 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.619\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 0.004 0.057 -0.003 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.681\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.0 0.054 0.003 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 0.645\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 0.004 0.057 -0.003 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.669\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.0 0.054 0.004 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.664\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.004 0.058 0.01 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.649\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.008 0.068 0.016 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.477\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.012 0.084 0.022 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.275\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.008 0.106 0.017 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.042\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.012 0.123 0.023 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.068\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.016 0.147 0.03 output tensor([[8, 6]]) selected_action: tensor([0]) rew: -0.175\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.012 0.177 0.025 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.452\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.016 0.202 0.032 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.473\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.02 0.234 0.039 output tensor([[6, 2]]) selected_action: tensor([0]) rew: -0.765\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: -6.091975539998247\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 802 lasted for 24 time steps with total reward of 3.30676377005269\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.004 0.038 -0.006 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.806\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.0 0.031 -0.0 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.668\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.004 0.031 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.849\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.0 0.026 0.0 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.708\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 -0.004 0.026 0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.868\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.0 0.033 0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.716\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.004 0.033 -0.005 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.827\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.008 0.028 -0.011 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.718\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.004 0.018 -0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.6\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 -0.0 0.013 0.001 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.802\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 -0.004 0.015 0.007 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.902\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 -0.0 0.022 0.002 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.747\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.004 0.023 -0.004 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.856\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.008 0.019 -0.01 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.785\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.012 0.009 -0.016 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.662\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.016 -0.006 -0.021 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.566\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 0.012 -0.027 -0.016 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.437\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.016 -0.043 -0.022 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.48\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.128 0.019 -0.065 -0.028 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.255\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.148 0.016 -0.092 -0.022 output tensor([[3, 3]]) selected_action: tensor([0]) rew: -0.001\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.163 0.012 -0.115 -0.017 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.003\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.175 0.008 -0.132 -0.012 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.028\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.183 0.004 -0.143 -0.007 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.074\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.187 0.008 -0.15 -0.014 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.142\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.195 0.012 -0.164 -0.02 output tensor([[1, 7]]) selected_action: tensor([1]) rew: -0.059\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.206 0.016 -0.184 -0.027 output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.292\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.222 0.012 -0.211 -0.023 output tensor([[8, 2]]) selected_action: tensor([0]) rew: -0.559\n",
            "output tensor([[7, 3]]) selected_action: tensor([0]) rew: -5.573170723018584\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 803 lasted for 28 time steps with total reward of 7.0162820941481625\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.004 0.026 -0.006 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.857\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.008 0.02 -0.012 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.718\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.004 0.008 -0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.606\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.008 0.002 -0.012 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.813\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.012 -0.01 -0.018 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 0.696\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.016 -0.028 -0.024 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 0.511\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.02 -0.051 -0.03 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.278\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.024 -0.081 -0.036 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.015\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.028 -0.117 -0.042 output tensor([[ 4, 12]]) selected_action: tensor([1]) rew: -0.28\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.11 0.032 -0.159 -0.049 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.609\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 0.028 -0.207 -0.044 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.973\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.17 0.032 -0.251 -0.051 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: -1.086\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: -6.469273956625483\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 804 lasted for 13 time steps with total reward of -4.923590783727208\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.004 -0.036 0.005 output tensor([[8, 7]]) selected_action: tensor([0]) rew: 0.821\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.0 -0.031 -0.001 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.693\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.004 -0.032 0.005 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.834\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.008 -0.027 0.011 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.726\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.012 -0.016 0.016 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.608\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.008 0.0 0.01 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.517\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.004 0.01 0.004 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.742\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.008 0.015 0.01 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 0.84\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.012 0.025 0.016 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.67\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.016 0.041 0.022 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.472\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.02 0.064 0.028 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.244\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.024 0.092 0.035 output tensor([[15,  2]]) selected_action: tensor([0]) rew: -0.015\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.02 0.127 0.029 output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.307\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.15 -0.024 0.156 0.036 output tensor([[3, 2]]) selected_action: tensor([0]) rew: -0.341\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -0.028 0.192 0.043 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.646\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.201 -0.032 0.235 0.05 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.986\n",
            "output tensor([[5, 8]]) selected_action: tensor([1]) rew: -6.363071010272426\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 805 lasted for 17 time steps with total reward of -1.4900868714035473\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.004 0.02 -0.006 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.887\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.008 0.013 -0.012 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 0.747\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.012 0.001 -0.018 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.634\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.008 -0.017 -0.012 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 0.548\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.004 -0.029 -0.006 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.619\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.008 -0.035 -0.012 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.705\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.004 -0.047 -0.007 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.525\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.008 -0.054 -0.013 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.607\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.004 -0.067 -0.007 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.421\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.008 -0.074 -0.014 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.497\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.004 -0.088 -0.008 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.306\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.008 -0.096 -0.015 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.375\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.12 0.004 -0.111 -0.009 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.177\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.123 0.008 -0.12 -0.016 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.237\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 0.004 -0.136 -0.011 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.03\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 0.0 -0.147 -0.006 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.08\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 -0.004 -0.153 -0.001 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.152\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 0.0 -0.154 -0.008 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.246\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 -0.004 -0.161 -0.003 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.072\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.128 -0.008 -0.164 0.002 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.155\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.12 -0.011 -0.163 0.007 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 0.17\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 -0.007 -0.156 -0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.06\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 -0.011 -0.156 0.005 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.249\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 -0.015 -0.152 0.009 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.14\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 -0.019 -0.142 0.014 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.042\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 -0.023 -0.128 0.019 output tensor([[6, 6]]) selected_action: tensor([0]) rew: -0.034\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.019 -0.109 0.012 output tensor([[1, 5]]) selected_action: tensor([1]) rew: -0.089\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.015 -0.097 0.006 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.167\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.011 -0.091 -0.0 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.389\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.015 -0.091 0.005 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.555\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.019 -0.086 0.01 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.445\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.015 -0.076 0.004 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.336\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.019 -0.073 0.009 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.544\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.023 -0.064 0.014 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.427\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.019 -0.049 0.008 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.336\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.015 -0.041 0.002 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.56\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.011 -0.039 -0.004 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.753\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.007 -0.043 -0.01 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 0.709\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.154 -0.003 -0.054 -0.016 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.537\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.157 -0.007 -0.07 -0.011 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.336\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -0.003 -0.081 -0.017 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.395\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -0.007 -0.098 -0.012 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.187\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.174 -0.011 -0.11 -0.007 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 0.238\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.185 -0.007 -0.116 -0.013 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.312\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.192 -0.011 -0.129 -0.008 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.118\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.203 -0.007 -0.137 -0.015 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.182\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.209 -0.011 -0.152 -0.01 output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.021\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.22 -0.007 -0.162 -0.016 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.032\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.227 -0.003 -0.178 -0.023 output tensor([[5, 6]]) selected_action: tensor([1]) rew: -0.182\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.23 -0.007 -0.201 -0.019 output tensor([[8, 1]]) selected_action: tensor([0]) rew: -0.43\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.236 -0.01 -0.22 -0.014 output tensor([[5, 1]]) selected_action: tensor([0]) rew: -0.424\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: -5.4013951304728725\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 806 lasted for 52 time steps with total reward of 9.66056709357165\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.004 0.017 0.006 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.909\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.008 0.023 0.012 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.778\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.004 0.034 0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 0.602\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.008 0.04 0.012 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.689\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.012 0.052 0.018 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.509\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.008 0.07 0.013 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.3\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.012 0.083 0.019 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.351\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.016 0.102 0.025 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 0.135\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.02 0.127 0.032 output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.114\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.024 0.158 0.038 output tensor([[3, 1]]) selected_action: tensor([0]) rew: -0.395\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.168 -0.028 0.197 0.045 output tensor([[5, 5]]) selected_action: tensor([0]) rew: -0.711\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.196 -0.032 0.241 0.052 output tensor([[6, 5]]) selected_action: tensor([0]) rew: -1.062\n",
            "output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: -6.449987129509359\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 807 lasted for 13 time steps with total reward of -4.459836785087965\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.004 -0.037 0.006 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.819\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.0 -0.031 -0.0 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.68\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.004 -0.031 0.005 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.848\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.0 -0.025 -0.001 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.717\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.004 -0.026 0.005 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.863\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.0 -0.021 -0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.749\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.004 -0.022 -0.007 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.876\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.008 -0.029 -0.013 output tensor([[6, 9]]) selected_action: tensor([1]) rew: 0.722\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.004 -0.042 -0.007 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.539\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.008 -0.049 -0.013 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.619\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.004 -0.062 -0.008 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.431\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.0 -0.07 -0.002 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.506\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.004 -0.073 -0.009 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.605\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.0 -0.081 -0.003 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.436\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.004 -0.085 0.002 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.529\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.0 -0.083 -0.004 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.545\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.004 -0.087 -0.011 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.497\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.008 -0.098 -0.017 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.012 -0.115 -0.023 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.107\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.008 -0.138 -0.018 output tensor([[7, 6]]) selected_action: tensor([0]) rew: -0.135\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.004 -0.157 -0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: -0.12\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.008 -0.17 -0.02 output tensor([[3, 6]]) selected_action: tensor([1]) rew: -0.085\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.004 -0.19 -0.016 output tensor([[2, 2]]) selected_action: tensor([0]) rew: -0.318\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.008 -0.206 -0.022 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.297\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.005 -0.228 -0.018 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.544\n",
            "output tensor([[9, 1]]) selected_action: tensor([0]) rew: -5.540303415062204\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 808 lasted for 26 time steps with total reward of 4.3654120608067375\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.004 0.016 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.916\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.0 0.022 0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.765\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.004 0.023 -0.005 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.879\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.0 0.018 0.001 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.765\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.004 0.019 0.007 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.893\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.008 0.025 0.013 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.739\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.004 0.038 0.007 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 0.557\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.008 0.045 0.013 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.638\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.004 0.059 0.008 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.451\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.0 0.066 0.002 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.527\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.004 0.069 -0.003 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.627\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.008 0.065 -0.009 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 0.592\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.004 0.057 -0.002 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 0.472\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.0 0.055 0.004 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.67\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.004 0.058 0.01 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.643\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.008 0.068 0.016 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.47\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.012 0.085 0.023 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.267\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.016 0.107 0.029 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.033\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.012 0.136 0.024 output tensor([[4, 5]]) selected_action: tensor([1]) rew: -0.234\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.008 0.16 0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: -0.243\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.012 0.179 0.026 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.233\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.016 0.204 0.032 output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.492\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.012 0.237 0.028 output tensor([[3, 4]]) selected_action: tensor([1]) rew: -0.785\n",
            "output tensor([[3, 7]]) selected_action: tensor([1]) rew: -5.828619903844852\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 809 lasted for 24 time steps with total reward of 3.0888222060010166\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.004 0.05 0.007 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 0.744\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.0 0.057 0.002 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.585\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.004 0.058 -0.004 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.69\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.008 0.054 -0.009 output tensor([[ 6, 11]]) selected_action: tensor([1]) rew: 0.624\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.004 0.045 -0.003 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.505\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.0 0.042 0.003 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.705\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.004 0.045 0.009 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.727\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.0 0.054 0.003 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.56\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.004 0.057 -0.002 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.656\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.0 0.055 0.004 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.676\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.004 0.059 -0.001 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.632\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.0 0.058 0.005 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.683\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.004 0.063 0.011 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 0.6\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.008 0.074 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.421\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.012 0.092 0.024 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.212\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.008 0.115 0.018 output tensor([[6, 7]]) selected_action: tensor([1]) rew: -0.028\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.004 0.134 0.013 output tensor([[2, 5]]) selected_action: tensor([1]) rew: -0.01\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.0 0.147 0.008 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.03\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.004 0.155 0.015 output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.09\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.008 0.17 0.022 output tensor([[5, 4]]) selected_action: tensor([0]) rew: -0.117\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.004 0.192 0.017 output tensor([[2, 7]]) selected_action: tensor([1]) rew: -0.358\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.008 0.209 0.024 output tensor([[8, 3]]) selected_action: tensor([0]) rew: -0.345\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.012 0.233 0.031 output tensor([[7, 7]]) selected_action: tensor([0]) rew: -0.599\n",
            "output tensor([[6, 6]]) selected_action: tensor([0]) rew: -5.889324442636629\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 810 lasted for 24 time steps with total reward of 1.7922077884239593\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.005 0.027 0.007 output tensor([[16,  3]]) selected_action: tensor([0]) rew: 0.859\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.001 0.033 0.001 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 0.706\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f501a3100de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BioLCNet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'singularbrain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLCNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRLTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_active\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWANDB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m plot_locally_connected_weights(net.connections[('input','main1')].w, net.n_channels1, net.filter_size1,\n\u001b[1;32m      8\u001b[0m                                                 \u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_size1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'main1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-a627f588c45e>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, hparams, online_validate, running_window_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                         \u001b[0mfailure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfailure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m                         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m                         )\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/network.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, time, one_step, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/nodes.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_state_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/nodes.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "manual_seed(SEED)\n",
        "\n",
        "if WANDB:\n",
        "    wandb.init(project='BioLCNet', entity='singularbrain', config=network_hparams)\n",
        "net = LCNet(time,**network_hparams, reward_fn=RLTasks('CartPole-v0'), wandb_active = WANDB)\n",
        "net.learn(**network_hparams)\n",
        "plot_locally_connected_weights(net.connections[('input','main1')].w, net.n_channels1, net.filter_size1,\n",
        "                                                compute_size(net.crop_size, net.filter_size1, net.stride1), net.connections[('input','main1')].locations,\n",
        "                                                net.crop_size ** 2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_locally_connected_weights(net.connections[('input','main1')].w, net.n_channels1, net.filter_size1,\n",
        "                                                compute_size(net.crop_size, net.filter_size1, net.stride1), net.connections[('input','main1')].locations,\n",
        "                                                net.crop_size ** 2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KmwqostVRPJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "874251c6-1e20-43ee-c82a-7ba1a6b40106"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gym Playground"
      ],
      "metadata": {
        "id": "9M7NvCJlr0E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "intensity = 400\n",
        "crop_size = 90\n",
        "time = 100+300+50\n",
        "def frame_process(x):\n",
        "        x[x<0.99] = 2.0\n",
        "        x[x<=1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "screen = env.render(mode='rgb_array')\n",
        "screen = screen.transpose((2, 0, 1))\n",
        "_, screen_height, screen_width = screen.shape\n",
        "screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "view_width = int(screen_width * 0.6)\n",
        "world_width = env.x_threshold * 2\n",
        "scale = screen_width / world_width\n",
        "cart_location = int(env.state[0] * scale + screen_width / 2.0)\n",
        "if cart_location < view_width // 2:\n",
        "    slice_range = slice(view_width)\n",
        "elif cart_location > (screen_width - view_width // 2):\n",
        "    slice_range = slice(-view_width, None)\n",
        "else:\n",
        "    slice_range = slice(cart_location - view_width // 2,\n",
        "                        cart_location + view_width // 2)\n",
        "# Strip off the edges, so that we have a square image centered on a cart\n",
        "screen = screen[:, :, slice_range]\n",
        "# Convert to float, rescale, convert to torch tensor\n",
        "screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "screen = torch.from_numpy(screen)\n",
        "print(screen.shape)\n",
        "plt.imshow(screen.cpu().permute(\n",
        "        1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "h = screen.shape[1]\n",
        "w = screen.shape[2]\n",
        "print(h*0.75)\n",
        "resize = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize([80, 180]),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "            #transforms.Resize([80, 80]),\n",
        "            #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "            # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "            #transforms.CenterCrop((crop_size, crop_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "            transforms.Lambda(frame_process),\n",
        "            #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "            transforms.Lambda(lambda x: x * intensity),\n",
        "            transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "device = 'cuda'\n",
        "\n",
        "# Resize, and add a batch dimension (BCHW)\n",
        "screen = resize(screen)\n",
        "# print(screen.shape)\n",
        "print(int(h*0.8), int(w*0.8))\n",
        "print(screen.shape)\n",
        "# screen = screen.to(device)\n",
        "# plt.imshow(screen.cpu().permute(\n",
        "#         1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "\n",
        "# print(a.shape)\n",
        "a = screen.sum(axis=0)\n",
        "\n",
        "a = a.to(device)\n",
        "plt.imshow(a.cpu().numpy().squeeze(), cmap='gray')\n",
        "obs, reward, done, info = env.step(0)\n",
        "obs, reward, done, info "
      ],
      "metadata": {
        "id": "JDFJB9GeldHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done, _ = env.step(0)   \n",
        "obs"
      ],
      "metadata": {
        "id": "9VmVGl3wPRBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zt5WXE2TgMzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(1)"
      ],
      "metadata": {
        "id": "fmN6aPHyidbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, x_dot, theta, theta_dot = env.state\n",
        "x, x_dot, theta, theta_dot"
      ],
      "metadata": {
        "id": "1MAMB52EiOMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.theta_threshold_radians"
      ],
      "metadata": {
        "id": "K83DiktzRLGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BindsNet Breakout"
      ],
      "metadata": {
        "id": "9jFzOzYIFdGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/BindsNET/bindsnet.git\n"
      ],
      "metadata": {
        "id": "9Qi4v29mDF3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ],
      "metadata": {
        "id": "6ROgQ2ZWA-VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install matplotlib==2.1.1\n",
        "\n",
        "### Restart your runtime after this"
      ],
      "metadata": {
        "id": "_K7CuZmqFTny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "wONJ1L1uEGMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bindsnet.encoding import bernoulli\n",
        "from bindsnet.environment import GymEnvironment\n",
        "from bindsnet.learning import MSTDP\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes\n",
        "from bindsnet.network.topology import Connection\n",
        "from bindsnet.pipeline import EnvironmentPipeline\n",
        "from bindsnet.pipeline.action import select_softmax\n",
        "import time\n",
        "\n",
        "# Build network.\n",
        "network = Network(dt=1.0)\n",
        "\n",
        "# Layers of neurons.\n",
        "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
        "middle = LIFNodes(n=100, traces=True)\n",
        "out = LIFNodes(n=4, refrac=0, traces=True)\n",
        "\n",
        "# Connections between layers.\n",
        "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
        "middle_out = Connection(\n",
        "    source=middle,\n",
        "    target=out,\n",
        "    wmin=0,\n",
        "    wmax=1,\n",
        "    update_rule=MSTDP,\n",
        "    nu=1e-1,\n",
        "    norm=0.5 * middle.n,\n",
        ")\n",
        "\n",
        "# Add all layers and connections to the network.\n",
        "network.add_layer(inpt, name=\"Input Layer\")\n",
        "network.add_layer(middle, name=\"Hidden Layer\")\n",
        "network.add_layer(out, name=\"Output Layer\")\n",
        "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
        "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
        "\n",
        "# Load the Breakout environment.\n",
        "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
        "environment.reset()\n",
        "\n",
        "# Build pipeline from specified components.\n",
        "environment_pipeline = EnvironmentPipeline(\n",
        "    network,\n",
        "    environment,\n",
        "    encoding=bernoulli,\n",
        "    action_function=select_softmax,\n",
        "    output=\"Output Layer\",\n",
        "    time=100,\n",
        "    history_length=1,\n",
        "    delta=1,\n",
        "    plot_interval=None,\n",
        "    render_interval=None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(pipeline, episode_count):\n",
        "    for i in range(episode_count):\n",
        "        total_reward = 0\n",
        "        pipeline.reset_state_variables()\n",
        "        is_done = False\n",
        "        render_counter = 0\n",
        "        while not is_done:\n",
        "            # if render_counter % 10 == 0:\n",
        "            #     # plt.title(\"Game image\")\n",
        "            #     # plt.imshow(pipeline.env.render())\n",
        "            #     # plt.show()\n",
        "            #     a = pipeline.env.render()\n",
        "            #     print(a)\n",
        "            #     time.sleep(0.1)\n",
        "                \n",
        "            render_counter += 1\n",
        "            result = pipeline.env_step()\n",
        "            pipeline.step(result)\n",
        "\n",
        "            reward = result[1]\n",
        "            total_reward += reward\n",
        "            \n",
        "            is_done = result[2]\n",
        "        print(f\"Episode {i} total reward:{total_reward}\")\n",
        "    pipeline.env.close()\n",
        "\n",
        "print(\"Training: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)\n",
        "\n",
        "# stop MSTDP\n",
        "environment_pipeline.network.learning = False\n",
        "\n",
        "print(\"Testing: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)"
      ],
      "metadata": {
        "id": "y_jx13I5-VTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PLVXCK0Djg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7fTSvrK3T_GA",
        "ULGGHW43UksI",
        "MBKedMpIleMr",
        "8clxN_npa1WY",
        "sCXSLZZoGS4z"
      ],
      "machine_shape": "hm",
      "name": "BioLCNet_CartPole.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "87ae7d1e75b14a98f2d7b99b6b39b40721989d38e6517f9dbec64ca4d8e3011b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2022e92497c45bb9f467e121924b024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_43012881fd414e41942d734f7721a223",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac49951d9e8c4a4db5fbb502e386a507",
              "IPY_MODEL_53574f384f734bdba6a8cdbdd0959fa9",
              "IPY_MODEL_15151c976c404a379f20e679bf70f15e"
            ]
          }
        },
        "43012881fd414e41942d734f7721a223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac49951d9e8c4a4db5fbb502e386a507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c2cbeac26956419cb6cffad8959eca0a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Episode: 810, Number of steps: 24, Episode Total Reward: 1.79:   5%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_487cd8c6a5894f85adfffea8c7392892"
          }
        },
        "53574f384f734bdba6a8cdbdd0959fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b378ef99e49a4894b364c1e589158afa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 510,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02c60c9b619c47fdbf4c6ba4f0c20f92"
          }
        },
        "15151c976c404a379f20e679bf70f15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51cdd6210ab24a6e98bb54f4c11164f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 510/10000 [2:04:32&lt;45:24:44, 17.23s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_283723e2216443f782894fffefd2684e"
          }
        },
        "c2cbeac26956419cb6cffad8959eca0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "487cd8c6a5894f85adfffea8c7392892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b378ef99e49a4894b364c1e589158afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02c60c9b619c47fdbf4c6ba4f0c20f92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51cdd6210ab24a6e98bb54f4c11164f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "283723e2216443f782894fffefd2684e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}