{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/DeepBioLCNet/blob/main/BioLCNet_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTSvrK3T_GA"
      },
      "source": [
        "#Notebook setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lXtgP_iEPE0G"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/Singular-Brain/DeepBioLCNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KXcXvvsXcOlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab851dc-3eaa-4e62-cdcc-a50b8bd8e482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepBioLCNet' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "0QFC6xL2uAaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49acf6c1-dfa9-4b09-a101-3492d0531b97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fbec9763ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K4l3AVRbGS4Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.image import AxesImage\n",
        "from torch.nn.modules.utils import _pair\n",
        "from matplotlib.collections import PathCollection\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from typing import Tuple, List, Optional, Sized, Dict, Union\n",
        "import math\n",
        "import random\n",
        "from torchvision.transforms.functional import crop\n",
        "# from ..utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights\n",
        "\n",
        "import gym\n",
        "import tkinter\n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BFGNAecpT-Lj"
      },
      "outputs": [],
      "source": [
        "from bindsnet.network.nodes import Nodes\n",
        "import os\n",
        "### import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, Tuple, Optional, Sequence\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "from bindsnet.datasets import MNIST\n",
        "from bindsnet.encoding import PoissonEncoder\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes, IFNodes\n",
        "from bindsnet.network.topology import Connection, MaxPool2dLocalConnection\n",
        "from bindsnet.network.topology import LocalConnection, LocalConnectionOrig\n",
        "from bindsnet.network.monitors import Monitor, AbstractMonitor, TensorBoardMonitor\n",
        "from bindsnet.learning import PostPre, MSTDP, MSTDPET, WeightDependentPostPre, Hebbian\n",
        "from bindsnet.learning.reward import DynamicDopamineInjection, DopaminergicRPE, AbstractReward\n",
        "from bindsnet.analysis.plotting import plot_locally_connected_weights,plot_locally_connected_weights_meh,plot_spikes,\\\n",
        "plot_LC_timepoint_spikes,plot_locally_connected_weights_meh2,plot_convergence_and_histogram,plot_locally_connected_weights_meh3\n",
        "from bindsnet.analysis.visualization import plot_weights_movie, plot_spike_trains_for_example,summary, plot_voltage\n",
        "from bindsnet.utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RLTasks(AbstractReward):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Computes the reward for a given RL task in the current state\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_id,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if env_id == \"CartPole-v0\":\n",
        "            self.compute = self._cartPole_compute\n",
        "        elif env_id == \"MountainCar-v0-v0\":\n",
        "            self.compute = self._mountainCar_compute\n",
        "        elif env_id == \"BreakoutDeterministic-v4\":\n",
        "            self.compute = self._breakout_compute\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"This rl environment is not currently supported.\"\n",
        "            )\n",
        "    \n",
        "    def _cartPole_compute(self, **kwargs):\n",
        "        success = kwargs['success']\n",
        "        failure = kwargs['failure']\n",
        "        if kwargs['classic_reward']:\n",
        "            if failure:\n",
        "                return torch.tensor([0.])\n",
        "            else:\n",
        "                return torch.tensor([1.])\n",
        "\n",
        "        env = kwargs['env']\n",
        "        state = env.state\n",
        "        success = kwargs['success']\n",
        "        failure = kwargs['failure']\n",
        "        x, x_dot, theta, theta_dot = state\n",
        "        # r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "        # r2 = ((env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5)#*2\n",
        "        #reward = r1 + r2\n",
        "        #reward = ((env.theta_threshold_radians - abs(theta)) - env.tau*abs(theta_dot))/env.theta_threshold_radians-0.5\n",
        "        reward = 1.\n",
        "        if success:\n",
        "            reward += 0\n",
        "        elif failure:\n",
        "            reward -= 1\n",
        "        reward = torch.tensor([reward])\n",
        "        return reward\n",
        "\n",
        "    def _mountainCar_compute(self):\n",
        "        pass\n",
        "\n",
        "    def _breakout_compute(self):\n",
        "        pass\n",
        "        \n",
        "    def update(self):\n",
        "        pass\n",
        "\n",
        "    def online_compute(self,):\n",
        "        pass"
      ],
      "metadata": {
        "id": "WqFD8yro6WCb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGGHW43UksI"
      },
      "source": [
        "## Sets up Gpu use and manual seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LiUmFrpcUfmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10eaca65-fc1a-497e-9975-eddfef7279cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Device =  cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device =  torch.device(\"cuda\")\n",
        "    gpu = True\n",
        "else:\n",
        "    device =  torch.device(\"cpu\")\n",
        "    gpu = False\n",
        "\n",
        "def manual_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SEED = 2045 # The Singularity is Near!\n",
        "manual_seed(SEED)\n",
        "WANDB = False\n",
        "\n",
        "torch.set_num_threads(os.cpu_count() - 1)\n",
        "print(\"Running on Device = \", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKedMpIleMr"
      },
      "source": [
        "# Custom Monitors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqpsr2a1WV"
      },
      "source": [
        "## Reward Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M44GJ65GleMs"
      },
      "outputs": [],
      "source": [
        "class RewardMonitor(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: None,\n",
        "        batch_size: int = 1,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.time = time\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # if time is not specified the monitor variable accumulate the logs\n",
        "        if self.time is None:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        self.recording = []\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if \"reward\" in kwargs:\n",
        "            self.recording.append(kwargs[\"reward\"])\n",
        "        # remove the oldest element (first in the list)\n",
        "        # if self.time is not None:\n",
        "        #     self.recording.pop(0)\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clxN_npa1WY"
      },
      "source": [
        "## Plot Eligibility trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SshGlRwpa1WZ"
      },
      "outputs": [],
      "source": [
        "class PlotET(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        i,\n",
        "        j,\n",
        "        source,\n",
        "        target,\n",
        "        connection,\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.connection = connection\n",
        "\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if hasattr(self.connection.update_rule, 'p_plus'):\n",
        "            self.recording['spikes_i'].append(self.source.s.ravel()[self.i].item())\n",
        "            self.recording['spikes_j'].append(self.target.s.ravel()[self.j].item())\n",
        "            self.recording['p_plus'].append(self.connection.update_rule.p_plus[self.i].item())\n",
        "            self.recording['p_minus'].append(self.connection.update_rule.p_minus[self.j].item())\n",
        "            self.recording['eligibility'].append(self.connection.update_rule.eligibility[self.i,self.j].item())\n",
        "            self.recording['eligibility_trace'].append(self.connection.update_rule.eligibility_trace[self.i,self.j].item())\n",
        "            self.recording['w'].append(self.connection.w[self.i,self.j].item())\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        fig, axs  = plt.subplots(7)\n",
        "        fig.set_size_inches(10, 20)\n",
        "        for i, (name, p) in enumerate(self.recording.items()):\n",
        "            axs[i].plot(p[-250:])\n",
        "            axs[i].set_title(name)\n",
        "    \n",
        "        fig.show()\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = {\n",
        "        'spikes_i': [],\n",
        "        'spikes_j': [],\n",
        "        'p_plus':[],\n",
        "        'p_minus':[],\n",
        "        'eligibility':[],\n",
        "        'eligibility_trace':[],\n",
        "        'w': [],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YGE1XjvIkZ"
      },
      "source": [
        "## Kernel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-4hp2V46vOUv"
      },
      "outputs": [],
      "source": [
        "class AbstractKernel(ABC):\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Base class for generating image filter kernels such as Gabor, DoG, etc. Each subclass should override :attr:`__call__` function.\n",
        "        Instantiates a ``Filter Kernel`` object.\n",
        "        :param window_size : The size of the kernel (int)\n",
        "        \"\"\"\n",
        "        self.window_size = kernel_size\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PL2L6_ABwBH4"
      },
      "outputs": [],
      "source": [
        "class DoGKernel(AbstractKernel):\n",
        "    def __init__(self, kernel_size: Union[int, Tuple[int, int]], sigma1 : float, sigma2 : float):\n",
        "        \"\"\"\n",
        "        Generates DoG filter kernels.\n",
        "        :param kernel_size: Horizontal and vertical size of DOG kernels.(If pass int, we consider it as a square filter) \n",
        "        :param sigma1 : The sigma parameter for the first Gaussian function.\n",
        "        :param sigma2 : The sigma parameter for the second Gaussian function.\n",
        "        \"\"\"\n",
        "        super(DoGKernel, self).__init__(kernel_size)\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        \n",
        "    def __call__(self):\n",
        "        k = self.window_size//2\n",
        "        x, y = np.mgrid[-k:k+1:1, -k:k+1:1]\n",
        "        a = 1.0 / (2 * math.pi)\n",
        "        prod = x*x + y*y\n",
        "        f1 = (1/(self.sigma1*self.sigma1)) * np.exp(-0.5 * (1/(self.sigma1*self.sigma1)) * (prod))\n",
        "        f2 = (1/(self.sigma2*self.sigma2)) * np.exp(-0.5 * (1/(self.sigma2*self.sigma2)) * (prod))\n",
        "        dog = a * (f1-f2)\n",
        "        dog_mean = np.mean(dog)\n",
        "        dog = dog - dog_mean\n",
        "        dog_max = np.max(dog)\n",
        "        dog = dog / dog_max\n",
        "        dog_tensor = torch.from_numpy(dog)\n",
        "        # returns a 2d tensor corresponding to the requested DoG filter\n",
        "        return dog_tensor.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zBUT0IUZDXxW"
      },
      "outputs": [],
      "source": [
        "class Filter():\n",
        "    \"\"\"\n",
        "    Applies a filter transform. Each filter contains a sequence of :attr:`FilterKernel` objects.\n",
        "    The result of each filter kernel will be passed through a given threshold (if not :attr:`None`).\n",
        "    Args:\n",
        "        filter_kernels (sequence of FilterKernels): The sequence of filter kernels.\n",
        "        padding (int, optional): The size of the padding for the convolution of filter kernels. Default: 0\n",
        "        thresholds (sequence of floats, optional): The threshold for each filter kernel. Default: None\n",
        "        use_abs (boolean, optional): To compute the absolute value of the outputs or not. Default: False\n",
        "    .. note::\n",
        "        The size of the compund filter kernel tensor (stack of individual filter kernels) will be equal to the \n",
        "        greatest window size among kernels. All other smaller kernels will be zero-padded with an appropriate \n",
        "        amount.\n",
        "    \"\"\"\n",
        "    # filter_kernels must be a list of filter kernels\n",
        "    # thresholds must be a list of thresholds for each kernel\n",
        "    def __init__(self, filter_kernels, padding=0, thresholds=None, use_abs=False):\n",
        "        tensor_list = []\n",
        "        self.max_window_size = 0\n",
        "        for kernel in filter_kernels:\n",
        "            if isinstance(kernel, torch.Tensor):\n",
        "                tensor_list.append(kernel)\n",
        "                self.max_window_size = max(self.max_window_size, kernel.size(-1))\n",
        "            else:\n",
        "                tensor_list.append(kernel().unsqueeze(0))\n",
        "                self.max_window_size = max(self.max_window_size, kernel.window_size)\n",
        "        for i in range(len(tensor_list)):\n",
        "            p = (self.max_window_size - filter_kernels[i].window_size)//2\n",
        "            tensor_list[i] = fn.pad(tensor_list[i], (p,p,p,p))\n",
        "\n",
        "        self.kernels = torch.stack(tensor_list)\n",
        "        self.number_of_kernels = len(filter_kernels)\n",
        "        self.padding = padding\n",
        "        if isinstance(thresholds, list):\n",
        "            self.thresholds = thresholds.clone().detach()\n",
        "            self.thresholds.unsqueeze_(0).unsqueeze_(2).unsqueeze_(3)\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "        self.use_abs = use_abs\n",
        "\n",
        "    # returns a 4d tensor containing the flitered versions of the input image\n",
        "    # input is a 4d tensor. dim: (minibatch=1, filter_kernels, height, width)\n",
        "    def __call__(self, input):\n",
        "\n",
        "        # if input.dim() == 3:\n",
        "        #     input2 = torch.unsqueeze(input, 0)\n",
        "        input.unsqueeze_(0)\n",
        "        output = fn.conv2d(input, self.kernels, padding = self.padding).float()\n",
        "        if not(self.thresholds is None):\n",
        "            output = torch.where(output < self.thresholds, torch.tensor(0.0, device=output.device), output)\n",
        "        if self.use_abs:\n",
        "            torch.abs_(output)\n",
        "        return output.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXSLZZoGS4z"
      },
      "source": [
        "# Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3DIAdG1mGS4z"
      },
      "outputs": [],
      "source": [
        "def plot_LC_timepoint_spikes(spikes: torch.Tensor,\n",
        "    timepoint: int,\n",
        "    n_filters: int,\n",
        "    in_chans: int,\n",
        "    slice_to_plot: int,\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    im: Optional[AxesImage] = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (10, 10),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(n_filters)))\n",
        "    sel_slice = spikes[timepoint].view(in_chans, n_filters, conv_size, conv_size).cpu()\n",
        "    sel_slice = sel_slice[slice_to_plot, ...].view(n_filters, conv_size, conv_size)\n",
        "    spikes_ = np.zeros((n_sqrt*conv_size, n_sqrt*conv_size))\n",
        "    filt_counter = 0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for n1 in range(n_sqrt):\n",
        "        for n2 in range(n_sqrt):\n",
        "            filter_ = sel_slice[filt_counter, :, :].view(conv_size, conv_size)\n",
        "            spikes_[n1 * conv_size : (n1 + 1) * conv_size, n2 * conv_size : (n2 + 1) * conv_size] = filter_\n",
        "            filt_counter += 1\n",
        "            ax.axhline((n1 + 1) * conv_size, color=\"g\", linestyle=\"-\")\n",
        "            ax.axvline((n2 + 1) * conv_size, color=\"g\", linestyle=\"--\")\n",
        "    ax.imshow(spikes_, cmap='Greys')\n",
        "    return spikes_\n",
        "    \n",
        "def plot_FC_response_map(lc: object,\n",
        "    fc: object,\n",
        "    ind_neuron_in_group: int,\n",
        "    label: int,\n",
        "    n_per_action: int,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param fc: FC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input\n",
        "    :param scale_factor: determines intensity of activation map  \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\t\n",
        "    ind_neuron = label * n_per_action + ind_neuron_in_group\n",
        "    w = fc.w[:,ind_neuron].view(reshaped.shape[0]//lc.kernel_size[0],reshaped.shape[1]//lc.kernel_size[1])\n",
        "    w = w.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*w, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_activation_map(lc : object,\n",
        "    spikes: torch.tensor,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot an activation map of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param scale_factor: determines intensity of activation map \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "    spikes = spikes.sum(0).squeeze().view(lc.conv_size[0]*int(np.sqrt(lc.out_channels)),lc.conv_size[1]*int(np.sqrt(lc.out_channels)))\n",
        "    x = scale_factor * spikes / torch.max(spikes)\n",
        "    x = x.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*x, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def reshape_LC_weights(\n",
        "    w: torch.Tensor,\n",
        "    n_filters: int,\n",
        "    kernel_size: Union[int, Tuple[int, int]],\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    input_sqrt: Union[int, Tuple[int, int]],\n",
        ") -> torch.Tensor:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Get the weights from a locally connected layer and reshape them to be two-dimensional and square.\n",
        "    :param w: Weights from a locally connected layer.\n",
        "    :param n_filters: No. of neuron filters.\n",
        "    :param kernel_size: Side length(s) of convolutional kernel.\n",
        "    :param conv_size: Side length(s) of convolution population.\n",
        "    :param input_sqrt: Sides length(s) of input neurons.\n",
        "    :return: Locally connected weights reshaped as a collection of spatially ordered square grids.\n",
        "    \"\"\"\n",
        "    k1, k2 = kernel_size\n",
        "    c1, c2 = conv_size\n",
        "    i1, i2 = input_sqrt\n",
        "    c1sqrt, c2sqrt = int(math.ceil(math.sqrt(c1))), int(math.ceil(math.sqrt(c2)))\n",
        "    fs = int(math.ceil(math.sqrt(n_filters)))\n",
        "\n",
        "    w_ = torch.zeros((n_filters * k1, k2 * c1 * c2))\n",
        "\n",
        "    for n1 in range(c1):\n",
        "        for n2 in range(c2):\n",
        "            for feature in range(n_filters):\n",
        "                n = n1 * c2 + n2\n",
        "                filter_ = w[feature, n1, n2, :, :\n",
        "                ].view(k1, k2)\n",
        "                w_[feature * k1 : (feature + 1) * k1, n * k2 : (n + 1) * k2] = filter_\n",
        "\n",
        "    if c1 == 1 and c2 == 1:\n",
        "        square = torch.zeros((i1 * fs, i2 * fs))\n",
        "\n",
        "        for n in range(n_filters):\n",
        "            square[\n",
        "                (n // fs) * i1 : ((n // fs) + 1) * i2,\n",
        "                (n % fs) * i2 : ((n % fs) + 1) * i2,\n",
        "            ] = w_[n * i1 : (n + 1) * i2]\n",
        "\n",
        "        return square\n",
        "    else:\n",
        "        square = torch.zeros((k1 * fs * c1, k2 * fs * c2))\n",
        "\n",
        "        for n1 in range(c1):\n",
        "            for n2 in range(c2):\n",
        "                for f1 in range(fs):\n",
        "                    for f2 in range(fs):\n",
        "                        if f1 * fs + f2 < n_filters:\n",
        "                            square[\n",
        "                                k1 * (n1 * fs + f1) : k1 * (n1 * fs + f1 + 1),\n",
        "                                k2 * (n2 * fs + f2) : k2 * (n2 * fs + f2 + 1),\n",
        "                            ] = w_[\n",
        "                                (f1 * fs + f2) * k1 : (f1 * fs + f2 + 1) * k1,\n",
        "                                (n1 * c2 + n2) * k2 : (n1 * c2 + n2 + 1) * k2,\n",
        "                            ]\n",
        "\n",
        "        return square\n",
        "\n",
        "def plot_semantic_pooling(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_weights(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        #print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            lc.kernel_size[0],#n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt*lc.conv_size[0] * lc.kernel_size[0],#n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            lc.kernel_size[0],#,n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt*lc.conv_size[1] * lc.kernel_size[1],#n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color=color, linestyle=\"--\")\n",
        "            \n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywXyWP0I83Au"
      },
      "source": [
        "# Design network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PcU9FSsVi4Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f42a33-6e9a-4d80-cd8a-2a6eb95b469e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ],
      "source": [
        "WANDB = True\n",
        "if WANDB:\n",
        "    !pip install -q wandb\n",
        "    !wandb login\n",
        "    import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8bZpJmlrJDa9"
      },
      "outputs": [],
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "def convergence(c):\n",
        "    if c.norm is None:\n",
        "        return 1-torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2\n",
        "    else:\n",
        "        mean_norm_factor = c.norm / c.w.shape[-1]\n",
        "        return  1-(torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2)\n",
        "\n",
        "        \n",
        "class LCNet(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: int,\n",
        "        n_actions: int,\n",
        "        neuron_per_action: int,\n",
        "        in_channels : int,\n",
        "        n_channels1: int,\n",
        "        n_channels2: int,\n",
        "        filter_size1: int,\n",
        "        filter_size2: int,\n",
        "        stride1: int,\n",
        "        stride2: int,\n",
        "        maxPool1: bool,\n",
        "        maxPool2: bool,\n",
        "        online: bool,\n",
        "        deep: bool,\n",
        "        reward_fn,\n",
        "        n_neurons: int,\n",
        "        pre_observation: bool,\n",
        "        has_decision_period: bool,\n",
        "        local_rewarding: bool,\n",
        "        nu_LC: Union[float, Tuple[float, float]],\n",
        "        nu_LC2: Union[float, Tuple[float, float]],\n",
        "        nu_Output: float,\n",
        "        dt: float,\n",
        "        crop_size:int ,\n",
        "        nu_inh_LC: float,\n",
        "        nu_inh: float,\n",
        "        inh_type,\n",
        "        inh_LC: bool,\n",
        "        inh_LC2: bool,\n",
        "        inh_factor_LC: float,\n",
        "        inh_factor_LC2: float,\n",
        "        inh_factor:float,\n",
        "        single_output_layer:bool,\n",
        "        NodesType_LC,\n",
        "        NodesType_Output, \n",
        "        update_rule_LC,\n",
        "        update_rule_LC2,\n",
        "        update_rule_Output,\n",
        "        update_rule_inh,\n",
        "        update_rule_inh_LC,\n",
        "        wmin: float,\n",
        "        wmax: float ,\n",
        "        soft_bound,\n",
        "        theta_plus: float,\n",
        "        tc_theta_decay: float,\n",
        "        tc_trace:int,\n",
        "        normal_init:bool,\n",
        "        mu: float,\n",
        "        std:float,\n",
        "        norm_factor_fc,\n",
        "        norm_factor_inh_LC: bool,\n",
        "        norm_factor_LC,\n",
        "        norm_factor_LC2,\n",
        "        norm_factor_out,\n",
        "        norm_factor_inh,\n",
        "        trace_additive,\n",
        "        load_path,\n",
        "        save_path,\n",
        "        LC_weights_path,\n",
        "        LC2_weights_path,\n",
        "        confusion_matrix,\n",
        "        lc_weights_vis,\n",
        "        out_weights_vis,\n",
        "        lc_convergence_vis,\n",
        "        out_convergence_vis,\n",
        "        thresh_LC,\n",
        "        thresh_FC,\n",
        "        num_episodes,\n",
        "        max_steps,\n",
        "        NodeType_FC,\n",
        "        add_layer_fc,\n",
        "        num_neurons_in_fc,\n",
        "        nu_fc,\n",
        "        update_rule_fc,\n",
        "        inh_fc,\n",
        "        inh_factor_fc,\n",
        "        classic_reward,\n",
        "        wandb_active = False,\n",
        "        batch_size=1,\n",
        "        save_interval = 100,\n",
        "\n",
        "\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructor for class ``BioLCNet``.\n",
        "\n",
        "        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n",
        "        :param n_neurons: Number of excitatory, inhibitory neurons.\n",
        "        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n",
        "        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n",
        "        :param dt: Simulation time step.\n",
        "        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n",
        "            respectively.\n",
        "        :param reduction: Method for reducing parameter updates along the minibatch\n",
        "            dimension.\n",
        "        :param wmin: Minimum allowed weight on input to excitatory synapses.\n",
        "        :param wmax: Maximum allowed weight on input to excitatory synapses.\n",
        "        :param norm: Input to excitatory layer connection weights normalization\n",
        "            constant.\n",
        "        :param theta_plus: On-spike increment of ``(adaptive)LIFNodes`` membrane\n",
        "            threshold potential.\n",
        "        :param tc_theta_decay: Time constant of ``(adaptive)LIFNodes`` threshold\n",
        "            potential decay.\n",
        "        :param inpt_shape: The dimensionality of the input layer.\n",
        "        \"\"\"\n",
        "        manual_seed(SEED)\n",
        "        super().__init__(dt=dt, reward_fn = None, online=online)\n",
        "        kwargs['single_output_layer'] = single_output_layer\n",
        "        kwargs['dt'] = dt\n",
        "        kwargs['n_labels'] = n_actions\n",
        "        kwargs['neuron_per_actionn'] = neuron_per_action\n",
        "        self.true_label = 0\n",
        "        self.dt = dt\n",
        "        self.intensity = kwargs['intensity']\n",
        "        self.reward_fn = reward_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_fn.network = self\n",
        "        self.reward_fn.dt = self.dt\n",
        "        self.n_actions = n_actions\n",
        "        self.neuron_per_action = neuron_per_action\n",
        "        self.n_classes = n_actions\n",
        "        self.classic_reward = classic_reward\n",
        "        self.add_layer_fc = add_layer_fc\n",
        "        self.num_neurons_in_fc = num_neurons_in_fc\n",
        "        self.neuron_per_class = neuron_per_action\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "        self.save_interval = save_interval\n",
        "        self.deep = deep\n",
        "        self.maxPool1 = maxPool1\n",
        "        self.maxPool2 = maxPool2\n",
        "        self.time = time\n",
        "        self.crop_size = crop_size\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.clamp_intensity = kwargs.get('clamp_intensity',None)\n",
        "        self.single_output_layer = single_output_layer\n",
        "        self.pre_observation = pre_observation\n",
        "        self.has_decision_period = has_decision_period\n",
        "        self.local_rewarding = local_rewarding\n",
        "        self.soft_bound = soft_bound\n",
        "        self.confusion_matrix = confusion_matrix\n",
        "        self.lc_weights_vis = lc_weights_vis\n",
        "        self.out_weights_vis = out_weights_vis\n",
        "        self.lc_convergence_vis = lc_convergence_vis\n",
        "        self.out_convergence_vis = out_convergence_vis\n",
        "        self.frame_analysis = frame_analysis\n",
        "        self.in_channels = in_channels\n",
        "        self.n_channels1 = n_channels1\n",
        "        self.n_channels2 = n_channels2\n",
        "        self.stride1 = stride1\n",
        "        self.stride2 = stride2\n",
        "        self.convergences = {}\n",
        "        self.norm_factor_LC = norm_factor_LC\n",
        "        self.norm_factor_LC2 = norm_factor_LC2\n",
        "        self.norm_factor_out = norm_factor_out\n",
        "        self.wmin = wmin \n",
        "        self.wmax = wmax\n",
        "        self.wandb_active = wandb_active\n",
        "        self.epochs_trained = 0\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_steps= max_steps\n",
        "        self.rew = 0.0\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.env.reset()\n",
        "\n",
        "        self.time_analysis = kwargs.get('time_analysis', False)\n",
        "        if kwargs['variant'] == 'scalar':\n",
        "            assert self.has_decision_period == True, ''\n",
        "\n",
        "        if self.online == False:\n",
        "            assert self.has_decision_period == True, ''\n",
        "        \n",
        "        if self.has_decision_period == True:\n",
        "            assert self.online == False, \"Decision period is not compatible with online learning.\"\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.decision_period = kwargs['decision_period']\n",
        "            assert self.decision_period > 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period - self.decision_period\n",
        "\n",
        "        elif self.pre_observation == True:\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period\n",
        "            self.decision_period = self.time - self.observation_period\n",
        "\n",
        "        else:\n",
        "            self.observation_period = 0\n",
        "            self.decision_period = self.time\n",
        "            self.learning_period = self.time\n",
        "\n",
        "        ### nodes\n",
        "        inp = Input(shape= [in_channels,crop_size,crop_size], traces=True, tc_trace=tc_trace,traces_additive = trace_additive)\n",
        "        self.add_layer(inp, name=\"input\")\n",
        "\n",
        "        ## First hidden layer\n",
        "        conv_size1 = compute_size(crop_size, filter_size1, stride1)\n",
        "        main1 = NodesType_LC(shape= [n_channels1, conv_size1, conv_size1], thresh = thresh_LC, traces=True, tc_trace=tc_trace,\n",
        "                             traces_additive = trace_additive,tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "        \n",
        "        self.add_layer(main1, name=\"main1\")\n",
        "\n",
        "        ### connections \n",
        "        LC1 = LocalConnectionOrig(inp, main1, filter_size1, stride1, n_channels1,\\\n",
        "                              nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, norm = norm_factor_LC)\n",
        "\n",
        "        # LC1 = LocalConnection(inp, main1, filter_size1, stride1, in_channels, n_channels1,input_shape=(crop_size,crop_size),\\\n",
        "        #                      nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC)\n",
        "\n",
        "\n",
        "        if LC_weights_path:\n",
        "            a = torch.load(LC_weights_path)\n",
        "            LC1.w.data = a['state_dict']['input_to_main1.w']\n",
        "            print(\"Weights loaded ...\")\n",
        "        \n",
        "        elif normal_init:\n",
        "            w_lc_init = torch.normal(mu,std, size = (in_channels, n_channels1 * compute_size(crop_size, filter_size1, stride1)**2, filter_size1**2))\n",
        "            LC1.w.data = w_lc_init\n",
        "       \n",
        "        self.add_connection(LC1, \"input\", \"main1\")\n",
        "        self.convergences['lc1'] = []\n",
        "\n",
        "        if inh_LC:\n",
        "            main_width = compute_size(crop_size, filter_size1, stride1)\n",
        "            w_inh_LC = torch.zeros(n_channels1,main_width,main_width,n_channels1,main_width,main_width)\n",
        "            for c in range(n_channels1):\n",
        "                for w1 in range(main_width):\n",
        "                    for w2 in range(main_width):\n",
        "                        w_inh_LC[c,w1,w2,:,w1,w2] = - inh_factor_LC\n",
        "                        w_inh_LC[c,w1,w2,c,w1,w2] = 0\n",
        "        \n",
        "            w_inh_LC = w_inh_LC.reshape(main1.n,main1.n)\n",
        "                                                             \n",
        "            LC_recurrent_inhibition = Connection(\n",
        "                source=main1,\n",
        "                target=main1,\n",
        "                w=w_inh_LC,\n",
        "            )\n",
        "            self.add_connection(LC_recurrent_inhibition, \"main1\", \"main1\")\n",
        "        \n",
        "        self.final_connection_source_name = 'main1'\n",
        "        self.final_connection_source = main1\n",
        "        \n",
        "        if self.add_layer_fc:\n",
        "            FC1 = NodeType_FC(n= self.num_neurons_in_fc, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "            self.add_layer(FC1, \"fc1\")\n",
        "\n",
        "            main_fc = Connection(main1, FC1, nu = nu_fc, update_rule = update_rule_fc, wmin = wmin, wmax= wmax, norm = norm_factor_fc)\n",
        "            \n",
        "            self.add_connection(main_fc, \"main1\", \"fc1\")\n",
        "            self.final_connection_source_name = 'fc1'\n",
        "            self.final_connection_source = FC1\n",
        "\n",
        "            if inh_fc:\n",
        "                w = -inh_factor_fc * (torch.ones(FC1.n, FC1.n) - torch.eye(FC1.n, FC1.n))\n",
        "                fc_recurrent_inhibition = Connection(\n",
        "                    source=FC1,\n",
        "                    target=FC1,\n",
        "                    w=w,\n",
        "                    update_rule = None,\n",
        "                    wmin=-inh_factor_fc,\n",
        "                    wmax=0,\n",
        "                )\n",
        "                self.add_connection(fc_recurrent_inhibition, \"fc1\", \"fc1\")\n",
        "\n",
        "        self.hidden2 = main1\n",
        "        self.hidden2_name = 'main1'\n",
        "        if maxPool1:\n",
        "            maxPool_kernel = 2\n",
        "            maxPool_stride = 2\n",
        "            \n",
        "            conv_size1 =compute_size(conv_size1, maxPool_kernel, maxPool_stride)\n",
        "            self.final_connection_source_name = 'maxpool1'\n",
        "            \n",
        "            maxpool1 = LIFNodes(shape= [self.n_channels1, conv_size1, conv_size1], refrac = 0)\n",
        "            self.add_layer(maxpool1, name=\"maxpool1\")\n",
        "            self.final_connection_source = maxpool1\n",
        "            \n",
        "            maxPoolConnection = MaxPool2dLocalConnection(main1, maxpool1, maxPool_kernel, maxPool_stride)\n",
        "            self.add_connection(maxPoolConnection, \"main1\", 'maxpool1')\n",
        "            \n",
        "            self.hidden2 = maxpool1\n",
        "            self.hidden2_name = 'maxpool1'\n",
        "\n",
        "        if deep:\n",
        "            # # Second hidden layer\n",
        "            conv_size2 = compute_size(conv_size1, filter_size2, stride2)\n",
        "\n",
        "            main2 = NodesType_LC(shape= [n_channels2, conv_size2, conv_size2],traces=True, tc_trace=tc_trace,traces_additive = trace_additive,\n",
        "                                            tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "            \n",
        "            self.add_layer(main2, name=\"main2\")\n",
        "\n",
        "            ### connections \n",
        "            lc2_input_shape = (conv_size1,conv_size1)\n",
        "            LC2 = LocalConnection(self.hidden2, main2, filter_size2, stride2, n_channels1, n_channels2, input_shape= lc2_input_shape,\n",
        "            nu = _pair(nu_LC2), update_rule = update_rule_LC2, wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC2)\n",
        "\n",
        "            self.add_connection(LC2,  self.hidden2_name, \"main2\")\n",
        "            self.convergences['lc2'] = []\n",
        "            if LC2_weights_path:\n",
        "                a = torch.load(LC2_weights_path)\n",
        "                LC2.w.data = a['state_dict']['main1_to_main2.w']\n",
        "                print(\"Weights loaded ...\")\n",
        "                \n",
        "            \n",
        "            elif normal_init:\n",
        "                w_lc_init = torch.normal(mu,std, size = (n_channels1, n_channels2 * compute_size(conv_size1, filter_size2, stride2)**2, filter_size2**2))\n",
        "                LC2.w.data = w_lc_init\n",
        "\n",
        "            self.final_connection_source_name = 'main2'\n",
        "            self.final_connection_source = main2\n",
        "\n",
        "            if inh_LC2:\n",
        "                main_width = conv_size2\n",
        "                w_inh_LC2 = torch.zeros(n_channels2,main_width,main_width,n_channels2,main_width,main_width)\n",
        "                for c in range(n_channels2):\n",
        "                    for w1 in range(main_width):\n",
        "                        for w2 in range(main_width):\n",
        "                            w_inh_LC2[c,w1,w2,:,w1,w2] = - inh_factor_LC2\n",
        "                            w_inh_LC2[c,w1,w2,c,w1,w2] = 0\n",
        "            \n",
        "                w_inh_LC2 = w_inh_LC2.reshape(main2.n,main2.n)\n",
        "                                                                \n",
        "                LC_recurrent_inhibition2 = Connection(\n",
        "                    source=main2,\n",
        "                    target=main2,\n",
        "                    w=w_inh_LC2,\n",
        "                )\n",
        "                self.add_connection(LC_recurrent_inhibition2, \"main2\", \"main2\")\n",
        "\n",
        "\n",
        "            if maxPool2:\n",
        "                maxPool_kernel = 2\n",
        "                maxPool_stride = 2\n",
        "                conv_size2 =compute_size(conv_size2, maxPool_kernel, maxPool_stride)\n",
        "                self.final_connection_source_name = 'maxpool2'\n",
        "                maxpool2 = LIFNodes(shape= [self.n_channels2, conv_size2, conv_size2], refrac = 0)\n",
        "                self.final_connection_source = maxpool2\n",
        "                maxPoolConnection2 = MaxPool2dLocalConnection(main2, maxpool2, maxPool_kernel, maxPool_stride)\n",
        "\n",
        "                self.add_layer(maxpool2, name=\"maxpool2\")\n",
        "                self.add_connection(maxPoolConnection2, \"main2\", 'maxpool2')\n",
        "\n",
        "\n",
        "        ### main2 to output\n",
        "        out = NodesType_Output(n= n_neurons, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "        self.add_layer(out, \"output\")\n",
        "\n",
        "        last_main_out = Connection(self.final_connection_source, out, nu = nu_Output, update_rule = update_rule_Output, wmin = wmin, wmax= wmax, norm = norm_factor_out)\n",
        "\n",
        "        if normal_init:\n",
        "            w_last_main_init = torch.normal(mu,std,size = (self.final_connection_source.n,out.n)) \n",
        "            last_main_out.w.data = w_last_main_init\n",
        "\n",
        "        self.add_connection(last_main_out, self.final_connection_source_name, \"output\")\n",
        "        self.convergences['last_main_out'] = []\n",
        "        ### Inhibitory:\n",
        "        if inh_type == 'between_layers':\n",
        "            w = -inh_factor * torch.ones(out.n, out.n)\n",
        "            for c in range(n_actions):\n",
        "                ind = slice(c*neuron_per_action,(c+1)*neuron_per_action)\n",
        "                w[ind, ind] = 0\n",
        "\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        elif inh_type == 'one_2_all':\n",
        "            w = -inh_factor * (torch.ones(out.n, out.n) - torch.eye(out.n, out.n))\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        # Diehl and Cook\n",
        "        elif inh_type == 'DC':\n",
        "            raise NotImplementedError('Diehl and cook not implemented yet fo r 10 classes')\n",
        "        elif inh_type == None:\n",
        "            pass\n",
        "        # Directs network to GPU\n",
        "        if gpu:\n",
        "            self.to(\"cuda\")\n",
        "\n",
        "    def frame_process(self, x):\n",
        "        x[x<1.0] = 2.0\n",
        "        x[x==1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "\n",
        "\n",
        "    def get_state_spiking(self):\n",
        "        intensity = self.intensity\n",
        "        screen = self.env.render(mode='rgb_array')\n",
        "        screen = screen.transpose((2, 0, 1))\n",
        "        _, screen_height, screen_width = screen.shape\n",
        "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "        view_width = int(screen_width * 0.6)\n",
        "        world_width = self.env.x_threshold * 2\n",
        "        scale = screen_width / world_width\n",
        "        cart_location = int(self.env.state[0] * scale + screen_width / 2.0)\n",
        "        if cart_location < view_width // 2:\n",
        "            slice_range = slice(view_width)\n",
        "        elif cart_location > (screen_width - view_width // 2):\n",
        "            slice_range = slice(-view_width, None)\n",
        "        else:\n",
        "            slice_range = slice(cart_location - view_width // 2,\n",
        "                                cart_location + view_width // 2)\n",
        "            \n",
        "        # Strip off the edges, so that we have a square image centered on a cart\n",
        "        screen = screen[:, :, slice_range]\n",
        "\n",
        "        # Convert to float, rescale, convert to torch tensor\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        h = screen.shape[1]\n",
        "        w = screen.shape[2]\n",
        "        resize = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize([80, 180]),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "                    # transforms.Resize([80, 80]),\n",
        "                    #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "                    # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "                    #transforms.CenterCrop((crop_size, crop_size)),\n",
        "                    transforms.Grayscale(),\n",
        "                    transforms.ToTensor(),\n",
        "                    #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "                    transforms.Lambda(self.frame_process),\n",
        "                    #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "                    transforms.Lambda(lambda x: x * intensity),\n",
        "                    transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "        screen = resize(screen)\n",
        "        if self.frame_analysis:\n",
        "            f = screen.sum(axis=0)\n",
        "            f = f.to(device)\n",
        "            plt.imshow(f.cpu().numpy().squeeze(), cmap='gray')\n",
        "            plt.show()\n",
        "        return screen\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        hparams = None,\n",
        "        online_validate = True,\n",
        "        running_window_length = 250,\n",
        "        verbose = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        manual_seed(SEED)\n",
        "        if self.wandb_active:\n",
        "            wandb.watch(self)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "\n",
        "        self.add_monitor(reward_monitor, name=\"reward\")\n",
        "\n",
        "        reward_hist = []\n",
        "\n",
        "        self.spikes = {}\n",
        "        for layer in set(self.layers):\n",
        "            self.spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=None)\n",
        "            self.add_monitor(self.spikes[layer], name=\"%s_spikes\" % layer)\n",
        "            self.dopaminergic_layers = self.layers[\"output\"]\n",
        "       \n",
        "        self.episode = 0\n",
        "        rew = 0.0\n",
        "        tot_rew = 0.0\n",
        "\n",
        "\n",
        "        if self.load_path:\n",
        "            \n",
        "            self.model_params = torch.load(self.load_path)\n",
        "            self.load_state_dict(torch.load(self.load_path)['state_dict'])\n",
        "            self.episode =  self.model_params['episode']\n",
        "            hparams = self.model_params['hparams']\n",
        "            reward_hist = self.model_params['reward_hist']\n",
        "            print(f'Previous model loaded! Resuming training from episode {self.episode}...\\n')\n",
        "        else:\n",
        "            self.env = gym.make('CartPole-v0')\n",
        "            self.env.reset()\n",
        "            print(f'Previous model not found! Training from the beginning...\\n')\n",
        "\n",
        "        pbar = tqdm(total=self.num_episodes)\n",
        "        \n",
        "        if self.time_analysis:\n",
        "            self.sample_spikes = {'input': [], 'main1': [], 'output': []}\n",
        "\n",
        "        for ep in range(self.num_episodes):\n",
        "            # print(f\"episode: {ep+1}\")\n",
        "            self.reset_state_variables()\n",
        "            self.env.reset()\n",
        "            done = False\n",
        "            tot_rew = 0.0\n",
        "            success = False\n",
        "            failure = False\n",
        "            num_steps = 0\n",
        "            for t in count():\n",
        "                \n",
        "                if t != 0:\n",
        "                    print(\"state (x, x_dot, theta (rad), theta_dot):\", round(x,3), round(x_dot*0.02, 3), round(theta, 3), round(theta_dot*0.02,3), \"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', round(rew,3))\n",
        "                    if self.time_analysis:\n",
        "                        self.sample_spikes['input'].append(self.spikes['input'].get('s'))\n",
        "                        self.sample_spikes['main1'].append(self.spikes['main1'].get('s'))\n",
        "                        self.sample_spikes['output'].append(self.spikes['output'].get('s').view(self.time, self.batch_size, n_actions, neuron_per_action))\n",
        "\n",
        "                        # w_lc1 = self.connections[('input', 'main1')].w\n",
        "                        # w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "                \n",
        "                image = self.get_state_spiking()\n",
        "                if gpu:\n",
        "                    inputs = {\"input\": image.cuda().view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "                else:\n",
        "                    inputs = {\"input\": image.view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "\n",
        "                clamp = {}\n",
        "                if self.clamp_intensity is not None:\n",
        "                    encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "                    clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "                # if done:\n",
        "                #     failure = True\n",
        "\n",
        "                if t >= self.max_steps:\n",
        "                    success = True\n",
        "                    done = True\n",
        "                elif done:\n",
        "                    failure = True\n",
        "\n",
        "\n",
        "                self.run(inputs=inputs, \n",
        "                        time = self.time,\n",
        "                        one_step=False,\n",
        "                        clamp = clamp,\n",
        "                        env = self.env,\n",
        "                        success = success,\n",
        "                        failure = failure,\n",
        "                        **kwargs,\n",
        "                        )\n",
        "                \n",
        "                rew = float(reward_monitor.get()[0])\n",
        "                tot_rew += rew\n",
        "                \n",
        "                lc_spikes1 = self.spikes['main1'].get('s')\n",
        "                #lc_spikes2 = self.spikes['main2'].get('s')\n",
        "                out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, self.n_actions, self.neuron_per_action)\n",
        "                sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "                selected_action = torch.argmax(sum_spikes, dim=1)\n",
        "\n",
        "                self.spikes['main1'].reset_state_variables()\n",
        "                self.spikes[\"output\"].reset_state_variables()\n",
        "                reward_monitor.reset_state_variables()\n",
        "                #self.reset_state_variables()\n",
        "\n",
        "                if done:\n",
        "                    if success == True:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\Successful episode!')\n",
        "                        num_steps = t+1\n",
        "                    else:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\nEpisode not successful!')\n",
        "                        num_steps = t+1\n",
        "                    break\n",
        "\n",
        "                obs, reward, done, _ = self.env.step(int(selected_action[0]))     \n",
        "                x = obs[0]\n",
        "                x_dot = obs[1]\n",
        "                theta = obs[2]\n",
        "                theta_dot = obs[3]\n",
        "\n",
        "                # Get voltage recording.\n",
        "                #main_voltage = main_monitor.get(\"v\")\n",
        "\n",
        "                #tensorboard.update(step= i)\n",
        "\n",
        "\n",
        "            if self.lc_weights_vis:\n",
        "                plot_locally_connected_weights(self.connections[('input','main1')].w, self.n_channels1, self.filter_size1,\n",
        "                                                compute_size(self.crop_size, self.filter_size1, self.stride1), self.connections[('input','main1')].locations,\n",
        "                                                self.crop_size ** 2)\n",
        "                plt.show()\n",
        "\n",
        "            if self.wandb_active:\n",
        "                wandb.log({\n",
        "                        **{'reward': tot_rew},\n",
        "                        **{' to '.join(name) + ' std': c.w.std().item() for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                        #**{name + ' spikes': monitor.get('s').sum().item() for name, monitor in self.spikes.items()},\n",
        "                        **{' to '.join(name) + \" gradients\": wandb.Histogram(c.w.cpu()) for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                    },\n",
        "                    step = self.episode)\n",
        "\n",
        "\n",
        "            #self.reward_fn.update() \n",
        "            #Plot_et.plot()    \n",
        "            # self.reset_state_variables()  # Reset state variables.\n",
        "            \n",
        "            self.episode += 1\n",
        "            reward_hist.append(tot_rew)\n",
        "            print(f'\\nEpisode {self.episode} lasted for {num_steps} time steps with total reward of {tot_rew}\\n')\n",
        "            if self.episode % self.save_interval == 0 and self.save_path:\n",
        "                model_params = {'state_dict': self.state_dict(), 'hparams': network_hparams, 'episode': self.episode, 'reward_hist': reward_hist}\n",
        "                torch.save(model_params, self.save_path)\n",
        "                print(\"\\n Model saved!\\n\")\n",
        "\n",
        "            pbar.set_description_str(\"Episode: \"+str(self.episode)+ \", Number of steps: \" + str(num_steps) +\", Episode Total Reward: \" + \"{:.2f}\".format(tot_rew))\n",
        "            pbar.update()\n",
        "\n",
        "    \n",
        "    def single_trial(self):\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        image = self.get_state_spiking()\n",
        "\n",
        "        if gpu:\n",
        "            inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "        else:\n",
        "            inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step = False,\n",
        "                mode = 'RL',\n",
        "                env = 'CartPole'\n",
        "                )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqAFucAUDb8"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3TerGeJoFdzg"
      },
      "outputs": [],
      "source": [
        "n_neurons = 2 #100\n",
        "n_actions = 2\n",
        "neuron_per_action = int(n_neurons/n_actions)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "crop_size = 40\n",
        "intensity = 200\n",
        "frame_analysis = False\n",
        "lc_weights_vis = False\n",
        "\n",
        "obs = 100\n",
        "dec = 200\n",
        "learn = 100\n",
        "time = obs+dec+learn\n",
        "\n",
        "max_steps = 100\n",
        "num_episodes = 10000\n",
        "\n",
        "filter_size1 = 24\n",
        "stride1 = 8\n",
        "n_channels1 = 100\n",
        "\n",
        "save_interval = 100\n",
        "\n",
        "learning_rate = 0.1\n",
        "norm_factor = 0.25\n",
        "\n",
        "add_layer_fc = False\n",
        "num_neurons_in_fc = 10\n",
        "NodeTypeFC = LIFNodes\n",
        "\n",
        "nu_fc = (0.0001,0.01)\n",
        "update_rule_fc = PostPre\n",
        "\n",
        "classic_reward = False\n",
        "inh_type = 'between_layers'\n",
        "\n",
        "clamp_intensity = 32\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': crop_size,\n",
        "    'intensity': intensity,\n",
        "    'round_input': False,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': n_channels1,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': filter_size1,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': stride1,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_actions': n_actions,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'max_steps': max_steps,\n",
        "    'num_episodes': num_episodes,\n",
        "    'save_interval': save_interval,\n",
        "    'add_layer_fc': add_layer_fc,\n",
        "    'num_neurons_in_fc': num_neurons_in_fc,\n",
        "    'classic_reward': classic_reward,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': True,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': obs,\n",
        "    'decision_period': dec,\n",
        "    'time_analysis': False,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'NodeType_FC': NodeTypeFC,\n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': None,#PostPre,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDPET,\n",
        "    'update_rule_fc': update_rule_fc,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.0001,0.01),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output': learning_rate,\n",
        "    'nu_fc': nu_fc,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': True,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': inh_type,\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    'inh_fc': False,\n",
        "    'inh_factor_fc': 100,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': norm_factor*filter_size1*filter_size1,\n",
        "    'norm_factor_fc' : None,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': clamp_intensity,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': '/content/drive/My Drive/LCNet/BioLCNet_cartpole_100tr.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_cartpole1.pth',#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'LC_weights_path': '/content/drive/My Drive/LCNet/BioLCNet_cartpole1.pth',#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25_weights.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': lc_weights_vis,\n",
        "    'out_weights_vis': False,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': False,\n",
        "    'frame_analysis': False,\n",
        "\n",
        "    ## reward\n",
        "    'n_labels': n_actions,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': 1.,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0005,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokdidkrV2Z5"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Venb2KhSYrT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70948d5-cf8c-4e90-cd29-199c0f3cb1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oThYyYvHJzeP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "69c8e26557ec4ff1bad490a064e5b7a7",
            "7b670731fdad4787bc2030b316daaf72",
            "c0109f360f404fb39b4e4cbdb66c416a",
            "9b0581c2937a42489254106d72918e8b",
            "5ca6a3795181445cbd5690b3afe65f72",
            "c91e809d389341229fb005b933649c4d",
            "5ecd142a68eb4dd983fadd2d53bb4ffe",
            "7397cf9abc7b45bc95107fccee11d9b0",
            "3235166c5d3e45d78da4d67dca961897",
            "6612ea4b33504514bbc1e60141101757",
            "734471b7292747999ca38a7f929332df"
          ]
        },
        "outputId": "1e7a22d9-6cf1-451e-9f92-7839392374a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/singularbrain/biolcnet/runs/1du5zihj\" target=\"_blank\">sage-cloud-265</a></strong> to <a href=\"https://wandb.ai/singularbrain/biolcnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights loaded ...\n",
            "Previous model not found! Training from the beginning...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69c8e26557ec4ff1bad490a064e5b7a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 -0.532 -0.167 0.001 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.725 -0.166 0.005 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.917 -0.161 0.01 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -1.11 -0.151 0.015 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -1.302 -0.136 0.02 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -1.495 -0.116 0.025 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -1.299 -0.092 0.018 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -1.493 -0.074 0.023 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -1.687 -0.05 0.029 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -1.881 -0.022 0.034 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.177 -2.076 0.012 0.04 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.218 -1.881 0.052 0.034 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.256 -1.687 0.086 0.029 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.29 -1.493 0.115 0.023 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.32 -1.689 0.138 0.03 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.353 -1.496 0.168 0.025 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.383 -1.693 0.193 0.032 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.417 -1.89 0.225 0.039 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 14 lasted for 43 time steps with total reward of 42.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.162 -0.011 0.006 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.357 -0.005 0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.552 0.006 0.017 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.748 0.024 0.023 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.553 0.047 0.018 output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.749 0.064 0.024 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.554 0.088 0.018 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.751 0.106 0.025 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.947 0.131 0.031 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.754 0.162 0.026 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.129 -0.561 0.188 0.021 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.14 -0.369 0.21 0.017 output tensor([[ 3, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 15 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.176 -0.032 0.005 output tensor([[13,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.37 -0.027 0.011 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.565 -0.016 0.016 output tensor([[16,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.37 0.001 0.01 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.565 0.011 0.016 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.76 0.027 0.022 output tensor([[22,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.956 0.05 0.028 output tensor([[25,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -1.151 0.078 0.034 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.148 -1.347 0.112 0.041 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.175 -1.543 0.153 0.047 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.206 -1.35 0.201 0.042 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.233 -1.157 0.243 0.038 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 16 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.205 0.047 0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.011 0.053 0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.184 0.054 -0.005 output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.378 0.05 -0.01 output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.182 0.039 -0.004 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.377 0.036 -0.01 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.181 0.026 -0.003 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.014 0.023 0.003 output tensor([[23,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.21 0.025 0.009 output tensor([[22,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.405 0.034 0.015 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.211 0.048 0.009 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.016 0.057 0.003 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.178 0.061 -0.002 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.372 0.059 -0.008 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.566 0.051 -0.013 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.371 0.038 -0.007 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.175 0.031 -0.001 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.021 0.03 0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.174 0.036 -0.0 output tensor([[ 1, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.369 0.035 -0.006 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.173 0.029 0.0 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.368 0.029 -0.006 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.172 0.024 0.0 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 -0.023 0.024 0.006 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 -0.219 0.031 0.012 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 -0.414 0.043 0.018 output tensor([[21,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.61 0.062 0.025 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.416 0.086 0.019 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.222 0.105 0.014 output tensor([[ 1, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.028 0.119 0.009 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.165 0.128 0.004 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.032 0.131 0.01 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.228 0.142 0.017 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.035 0.159 0.012 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.232 0.171 0.019 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.429 0.189 0.026 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.237 0.215 0.021 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 17 lasted for 38 time steps with total reward of 37.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.21 -0.026 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.015 -0.02 -0.0 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.21 -0.02 0.005 output tensor([[22,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.014 -0.015 -0.0 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.209 -0.015 0.005 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.404 -0.01 0.011 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.599 0.001 0.017 output tensor([[22,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.794 0.018 0.023 output tensor([[28,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.599 0.04 0.017 output tensor([[ 1, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.405 0.057 0.011 output tensor([[ 0, 24]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.211 0.069 0.006 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.016 0.075 0.0 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 0.177 0.075 -0.005 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 0.371 0.07 -0.01 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 0.175 0.06 -0.004 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 0.37 0.056 -0.009 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 0.174 0.046 -0.003 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.022 0.043 0.003 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.218 0.046 0.009 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.023 0.055 0.003 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 0.171 0.059 -0.002 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 0.365 0.056 -0.008 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 0.169 0.049 -0.001 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 0.364 0.048 -0.007 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 0.558 0.041 -0.012 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 0.363 0.028 -0.006 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.167 0.022 -0.0 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.028 0.022 0.006 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.224 0.027 0.012 output tensor([[21,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.029 0.039 0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 0.165 0.045 0.0 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.36 0.046 -0.005 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.164 0.041 0.001 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 0.359 0.042 -0.005 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.163 0.037 0.002 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.033 0.038 0.008 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.162 0.046 0.002 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.356 0.048 -0.004 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.161 0.044 0.003 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.355 0.047 -0.003 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.549 0.044 -0.009 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.744 0.036 -0.014 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.548 0.021 -0.008 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.353 0.013 -0.002 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.548 0.011 -0.008 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.743 0.004 -0.014 output tensor([[ 5, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.938 -0.01 -0.019 output tensor([[ 2, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 1.133 -0.029 -0.025 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 1.329 -0.055 -0.031 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 1.134 -0.086 -0.026 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.15 0.94 -0.112 -0.021 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.169 0.747 -0.133 -0.015 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.184 0.554 -0.148 -0.011 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.195 0.361 -0.159 -0.006 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.202 0.558 -0.164 -0.012 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.213 0.365 -0.177 -0.008 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.22 0.173 -0.184 -0.003 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.224 0.37 -0.187 -0.01 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.231 0.568 -0.197 -0.017 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.243 0.765 -0.214 -0.024 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 18 lasted for 61 time steps with total reward of 60.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.231 -0.032 0.005 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.036 -0.027 -0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.16 -0.028 -0.007 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.035 -0.035 -0.001 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.229 -0.036 0.004 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.034 -0.032 -0.002 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.229 -0.033 0.004 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.033 -0.029 -0.002 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.228 -0.031 0.004 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.422 -0.028 0.009 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.227 -0.018 0.003 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.031 -0.015 -0.003 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.226 -0.018 0.003 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.421 -0.015 0.009 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.616 -0.006 0.015 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.811 0.008 0.02 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.616 0.029 0.015 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.812 0.043 0.021 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -1.007 0.064 0.027 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.813 0.09 0.021 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.154 -1.009 0.112 0.028 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.174 -1.206 0.139 0.034 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.198 -1.402 0.174 0.041 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.226 -1.209 0.214 0.036 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 19 lasted for 25 time steps with total reward of 24.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.181 -0.01 0.005 output tensor([[12,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.014 -0.005 -0.0 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.181 -0.005 0.005 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.376 0.0 0.011 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.571 0.011 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.376 0.028 0.011 output tensor([[ 4, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.182 0.04 0.006 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.377 0.045 0.012 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.183 0.057 0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 0.011 0.063 0.001 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 0.206 0.064 -0.005 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 0.01 0.059 0.001 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.186 0.06 0.008 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.382 0.068 0.014 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.578 0.082 0.02 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.774 0.102 0.026 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.971 0.128 0.033 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.777 0.161 0.028 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.584 0.189 0.023 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.148 -0.392 0.212 0.019 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 20 lasted for 21 time steps with total reward of 20.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.2 -0.039 -0.005 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.395 -0.044 -0.011 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.591 -0.056 -0.017 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.787 -0.073 -0.024 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.983 -0.097 -0.03 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 0.789 -0.126 -0.025 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.986 -0.151 -0.031 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 1.182 -0.182 -0.038 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.158 0.989 -0.22 -0.033 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 21 lasted for 10 time steps with total reward of 9.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.197 0.032 -0.006 output tensor([[29, 30]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.392 0.026 -0.011 output tensor([[ 6, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.586 0.015 -0.017 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.391 -0.002 -0.011 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.196 -0.014 -0.005 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.001 -0.019 0.0 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 -0.194 -0.019 0.006 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.002 -0.012 0.0 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 -0.193 -0.012 0.006 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.388 -0.006 0.012 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.583 0.005 0.018 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.779 0.023 0.023 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.974 0.046 0.029 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -1.17 0.076 0.036 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -1.366 0.111 0.042 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -1.172 0.153 0.037 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.979 0.19 0.032 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -1.175 0.222 0.039 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[21,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 22 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.155 0.043 0.007 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.351 0.05 0.013 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.157 0.063 0.007 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.353 0.07 0.014 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.549 0.084 0.02 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.745 0.104 0.026 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.941 0.13 0.033 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -1.137 0.163 0.039 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -1.334 0.202 0.046 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -1.53 0.248 0.053 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 23 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.229 0.016 -0.006 output tensor([[ 1, 31]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.424 0.01 -0.012 output tensor([[ 0, 24]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.619 -0.002 -0.018 output tensor([[ 2, 28]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.814 -0.02 -0.023 output tensor([[ 4, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 1.009 -0.043 -0.029 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.815 -0.072 -0.024 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.621 -0.096 -0.018 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.427 -0.115 -0.013 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 0.624 -0.128 -0.02 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 0.43 -0.148 -0.015 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.627 -0.162 -0.021 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.125 0.435 -0.184 -0.017 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.632 -0.2 -0.024 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.147 0.829 -0.224 -0.031 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 24 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.199 -0.032 0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.394 -0.027 0.011 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.589 -0.016 0.017 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.784 0.0 0.022 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.589 0.022 0.016 output tensor([[ 2, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.394 0.039 0.011 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.199 0.049 0.005 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.005 0.055 -0.0 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.201 0.054 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.006 0.06 0.0 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.202 0.06 0.006 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.398 0.067 0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.204 0.079 0.007 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.01 0.087 0.002 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.206 0.088 0.008 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.403 0.097 0.015 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.209 0.111 0.009 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.016 0.121 0.004 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.212 0.125 0.011 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.019 0.136 0.006 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 0.174 0.142 0.001 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.023 0.143 0.008 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.22 0.151 0.014 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.417 0.165 0.021 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.614 0.186 0.028 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.421 0.214 0.023 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 25 lasted for 27 time steps with total reward of 26.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.209 -0.002 -0.005 output tensor([[ 4, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.404 -0.007 -0.011 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.209 -0.018 -0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.405 -0.023 -0.011 output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.6 -0.034 -0.017 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.405 -0.051 -0.011 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.211 -0.062 -0.006 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.407 -0.068 -0.012 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.603 -0.08 -0.018 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.409 -0.098 -0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.605 -0.111 -0.019 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.412 -0.131 -0.014 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 0.609 -0.145 -0.021 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.416 -0.166 -0.016 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.223 -0.182 -0.011 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 0.031 -0.193 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 0.228 -0.2 -0.014 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 0.426 -0.214 -0.021 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 26 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.185 -0.024 0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.379 -0.019 0.011 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.574 -0.008 0.017 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.769 0.009 0.023 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.964 0.031 0.028 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.77 0.06 0.023 output tensor([[ 4, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.575 0.083 0.017 output tensor([[ 2, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.382 0.1 0.012 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.188 0.112 0.007 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.384 0.119 0.013 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.581 0.132 0.02 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.778 0.152 0.027 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.974 0.179 0.033 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.782 0.212 0.029 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 27 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.146 0.046 -0.005 output tensor([[ 4, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.34 0.041 -0.01 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 0.535 0.031 -0.016 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.73 0.015 -0.022 output tensor([[ 5, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.924 -0.007 -0.027 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 1.12 -0.034 -0.033 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 1.315 -0.068 -0.039 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 1.121 -0.107 -0.034 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.927 -0.141 -0.029 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 0.734 -0.17 -0.024 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.119 0.931 -0.194 -0.031 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.138 0.739 -0.224 -0.026 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 28 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.156 -0.004 0.006 output tensor([[20,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.351 0.002 0.012 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.547 0.014 0.018 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.742 0.032 0.024 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.547 0.056 0.018 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.353 0.074 0.013 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.549 0.087 0.019 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.745 0.106 0.025 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.941 0.131 0.032 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.748 0.163 0.027 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.555 0.19 0.022 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.752 0.212 0.029 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 29 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.245 -0.01 0.006 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.049 -0.004 -0.0 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.146 -0.005 -0.006 output tensor([[ 3, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.341 -0.011 -0.012 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.146 -0.023 -0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 0.342 -0.029 -0.012 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.147 -0.041 -0.007 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.048 -0.048 -0.001 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.242 -0.049 0.005 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.436 -0.044 0.01 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.631 -0.034 0.016 output tensor([[18,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.826 -0.018 0.021 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -1.02 0.003 0.027 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -1.216 0.03 0.033 output tensor([[15,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -1.021 0.063 0.027 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -1.217 0.09 0.034 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.152 -1.413 0.124 0.04 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.181 -1.609 0.164 0.046 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.213 -1.805 0.21 0.053 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 30 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.157 -0.001 0.007 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.352 0.006 0.013 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.547 0.019 0.019 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.743 0.037 0.025 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.938 0.062 0.031 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -1.134 0.092 0.037 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.116 -0.94 0.129 0.032 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -1.137 0.161 0.038 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.157 -0.944 0.199 0.033 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.176 -1.14 0.232 0.04 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 31 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.15 0.008 0.005 output tensor([[29,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.345 0.013 0.011 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.54 0.025 0.017 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.346 0.042 0.012 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.541 0.053 0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.347 0.071 0.012 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -0.153 0.083 0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.349 0.09 0.013 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.545 0.103 0.02 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.742 0.123 0.026 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.938 0.149 0.033 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.745 0.181 0.028 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -0.553 0.209 0.023 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -0.361 0.232 0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 32 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.195 0.032 0.006 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.391 0.038 0.012 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.196 0.05 0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.002 0.056 0.001 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.198 0.056 0.007 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.394 0.063 0.013 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.59 0.076 0.019 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.396 0.095 0.014 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.202 0.109 0.009 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.009 0.118 0.003 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.205 0.121 0.01 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.402 0.131 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.599 0.148 0.023 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.795 0.171 0.03 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.603 0.201 0.025 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.41 0.226 0.021 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 33 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.215 0.001 -0.006 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.41 -0.006 -0.012 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.215 -0.018 -0.006 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.021 -0.024 -0.001 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.174 -0.025 0.005 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.369 -0.02 0.011 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.564 -0.009 0.016 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.759 0.008 0.022 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.564 0.03 0.016 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.369 0.046 0.011 output tensor([[ 4, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.565 0.057 0.017 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.761 0.074 0.023 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -0.957 0.097 0.029 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.763 0.127 0.024 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.57 0.151 0.019 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -0.377 0.17 0.014 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.574 0.185 0.021 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.771 0.206 0.028 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.159 -0.968 0.234 0.035 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 34 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.236 -0.047 0.005 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.43 -0.042 0.01 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.625 -0.032 0.016 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.82 -0.016 0.022 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -1.015 0.006 0.027 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.82 0.033 0.022 output tensor([[ 2, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.625 0.055 0.016 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.431 0.071 0.01 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.236 0.081 0.005 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.043 0.086 -0.0 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.239 0.086 0.006 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.435 0.092 0.012 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.241 0.104 0.007 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.048 0.112 0.002 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 0.146 0.114 -0.003 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.051 0.11 0.003 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.248 0.114 0.01 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.444 0.124 0.016 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.641 0.14 0.023 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -0.837 0.163 0.03 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.174 -0.645 0.193 0.025 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.187 -0.452 0.218 0.02 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 35 lasted for 23 time steps with total reward of 22.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.224 0.001 0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.419 0.006 0.011 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.614 0.018 0.017 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.81 0.035 0.023 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -1.005 0.057 0.029 output tensor([[24,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -1.201 0.086 0.035 output tensor([[20,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -1.397 0.122 0.042 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -1.593 0.163 0.048 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.177 -1.4 0.211 0.043 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 36 lasted for 10 time steps with total reward of 9.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.172 0.009 0.007 output tensor([[25,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.367 0.016 0.012 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.562 0.028 0.018 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.367 0.047 0.013 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.173 0.059 0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.369 0.066 0.013 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.565 0.08 0.02 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.761 0.099 0.026 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.567 0.125 0.021 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.374 0.146 0.016 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.181 0.162 0.011 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 0.011 0.173 0.006 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 0.204 0.179 0.001 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 0.007 0.181 0.008 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.191 0.189 0.015 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 0.001 0.204 0.011 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 0.193 0.215 0.006 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 37 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.184 -0.012 0.007 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.379 -0.005 0.012 output tensor([[13,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.184 0.007 0.006 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.011 0.014 0.001 output tensor([[ 2, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.184 0.014 0.007 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.011 0.021 0.001 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.206 0.022 -0.005 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.01 0.017 0.001 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.185 0.018 0.007 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.38 0.025 0.013 output tensor([[20,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.576 0.038 0.019 output tensor([[23,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.381 0.057 0.013 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.187 0.07 0.008 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 0.007 0.078 0.003 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.189 0.081 0.009 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 0.005 0.09 0.004 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.191 0.093 0.01 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.388 0.103 0.016 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.194 0.12 0.011 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.001 0.131 0.006 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 0.192 0.137 0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.005 0.138 0.008 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.188 0.146 0.003 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 0.381 0.149 -0.002 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 0.574 0.147 -0.007 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.377 0.14 -0.0 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.57 0.14 -0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.763 0.135 -0.01 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.566 0.125 -0.003 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.369 0.121 0.003 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.173 0.125 0.01 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.024 0.134 0.016 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.221 0.151 0.023 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.418 0.174 0.03 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.225 0.204 0.025 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.033 0.229 0.021 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 38 lasted for 37 time steps with total reward of 36.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.162 0.003 0.006 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.357 0.01 0.012 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.553 0.022 0.018 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.748 0.04 0.024 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.553 0.064 0.018 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.749 0.082 0.025 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.555 0.107 0.019 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.752 0.126 0.026 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -0.948 0.152 0.032 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -1.145 0.184 0.039 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.952 0.223 0.035 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 39 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.222 -0.028 0.006 output tensor([[18,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.416 -0.022 0.012 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.611 -0.01 0.018 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.416 0.007 0.012 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.221 0.019 0.006 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.026 0.025 0.0 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.169 0.025 -0.006 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 0.363 0.02 -0.011 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 0.168 0.009 -0.005 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 0.363 0.003 -0.011 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.168 -0.008 -0.005 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.027 -0.013 0.001 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.222 -0.012 0.006 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.027 -0.006 0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.168 -0.005 -0.005 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.364 -0.011 -0.011 output tensor([[ 4, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.559 -0.022 -0.017 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.754 -0.039 -0.023 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.56 -0.062 -0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.756 -0.08 -0.024 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.952 -0.104 -0.03 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.758 -0.134 -0.025 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.565 -0.159 -0.02 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.762 -0.179 -0.027 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.569 -0.206 -0.022 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.377 -0.228 -0.018 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[7, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 40 lasted for 27 time steps with total reward of 26.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.222 -0.028 -0.006 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.417 -0.034 -0.012 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.613 -0.047 -0.018 output tensor([[ 3, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.418 -0.065 -0.013 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.224 -0.077 -0.007 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.03 -0.084 -0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.164 -0.086 0.004 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.358 -0.083 0.009 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.161 -0.074 0.002 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.355 -0.071 0.008 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.159 -0.063 0.002 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.353 -0.062 0.007 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.548 -0.055 0.012 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.742 -0.042 0.018 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.546 -0.025 0.012 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.741 -0.013 0.018 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.936 0.005 0.023 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -1.131 0.028 0.029 output tensor([[16,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.936 0.057 0.023 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.742 0.081 0.018 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -0.548 0.099 0.013 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -0.744 0.111 0.019 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.177 -0.551 0.131 0.014 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.188 -0.358 0.145 0.009 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.195 -0.555 0.154 0.016 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.206 -0.752 0.169 0.022 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.222 -0.948 0.192 0.029 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.24 -1.145 0.221 0.036 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 41 lasted for 29 time steps with total reward of 28.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.193 -0.05 0.005 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.388 -0.045 0.011 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.582 -0.034 0.016 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.777 -0.018 0.022 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.581 0.004 0.016 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.386 0.02 0.01 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.582 0.03 0.016 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.777 0.046 0.022 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.973 0.069 0.028 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.779 0.097 0.023 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.975 0.12 0.029 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.782 0.149 0.024 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.17 -0.589 0.173 0.019 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.182 -0.396 0.193 0.015 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.19 -0.593 0.208 0.022 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.202 -0.402 0.229 0.017 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 42 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.204 -0.034 -0.005 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.009 -0.039 0.001 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.185 -0.038 0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.01 -0.032 0.0 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.206 -0.032 -0.006 output tensor([[ 2, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.401 -0.038 -0.012 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.207 -0.05 -0.006 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.403 -0.056 -0.013 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.598 -0.069 -0.019 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.404 -0.088 -0.013 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.601 -0.101 -0.02 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.797 -0.121 -0.026 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.604 -0.147 -0.021 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.411 -0.168 -0.016 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.608 -0.185 -0.023 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.415 -0.208 -0.019 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.223 -0.226 -0.014 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[7, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 43 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.16 -0.0 -0.007 output tensor([[ 5, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.355 -0.007 -0.013 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.55 -0.02 -0.019 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.745 -0.038 -0.025 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.941 -0.063 -0.031 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.747 -0.094 -0.025 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.553 -0.119 -0.02 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.36 -0.139 -0.015 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.556 -0.154 -0.022 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.363 -0.175 -0.017 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 0.171 -0.192 -0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 0.368 -0.204 -0.019 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.566 -0.223 -0.026 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 44 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.225 0.015 -0.005 output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.42 0.01 -0.011 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.615 -0.001 -0.016 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.42 -0.017 -0.011 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.615 -0.028 -0.017 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.811 -0.044 -0.023 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.616 -0.067 -0.017 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.422 -0.084 -0.012 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.618 -0.095 -0.018 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.814 -0.113 -0.024 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.621 -0.138 -0.019 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.818 -0.157 -0.026 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 0.625 -0.183 -0.021 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 0.433 -0.204 -0.016 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.241 -0.22 -0.012 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 45 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.235 0.011 0.006 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.431 0.017 0.012 output tensor([[26,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.626 0.029 0.018 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.431 0.047 0.012 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.237 0.059 0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.043 0.065 0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.239 0.066 0.007 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.435 0.074 0.014 output tensor([[16,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.241 0.087 0.008 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.047 0.096 0.003 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 0.147 0.099 -0.002 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.34 0.096 -0.007 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 0.534 0.089 -0.013 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.338 0.076 -0.006 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.142 0.07 0.0 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.054 0.07 0.006 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.25 0.076 0.013 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.056 0.089 0.007 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.137 0.096 0.002 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.059 0.098 0.008 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.255 0.106 0.015 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.452 0.121 0.021 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.258 0.142 0.016 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.455 0.159 0.023 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.262 0.182 0.018 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.459 0.2 0.025 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.267 0.225 0.021 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 46 lasted for 28 time steps with total reward of 27.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.191 -0.04 -0.006 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.003 -0.046 0.0 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.193 -0.045 -0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.388 -0.052 -0.012 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.194 -0.064 -0.007 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.39 -0.07 -0.013 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.586 -0.083 -0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.392 -0.103 -0.014 output tensor([[2, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.199 -0.116 -0.009 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.395 -0.125 -0.015 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.592 -0.14 -0.022 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.399 -0.162 -0.017 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.596 -0.179 -0.024 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.403 -0.203 -0.019 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 0.6 -0.222 -0.026 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 3, 17]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 47 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.239 -0.043 0.006 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.433 -0.037 0.012 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.238 -0.025 0.006 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.042 -0.02 -0.0 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.237 -0.02 0.005 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.432 -0.015 0.011 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.627 -0.004 0.017 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -0.822 0.013 0.023 output tensor([[20,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.627 0.036 0.017 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.432 0.053 0.011 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.238 0.064 0.006 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.434 0.07 0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.63 0.081 0.018 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -0.826 0.1 0.025 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -0.632 0.124 0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.186 -0.439 0.144 0.014 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.194 -0.246 0.158 0.009 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.199 -0.443 0.168 0.016 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.208 -0.251 0.184 0.012 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.213 -0.448 0.195 0.018 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.222 -0.645 0.214 0.025 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[12,  3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 48 lasted for 22 time steps with total reward of 21.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.236 0.014 -0.005 output tensor([[ 3, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.431 0.008 -0.011 output tensor([[ 4, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.626 -0.003 -0.017 output tensor([[ 3, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.821 -0.019 -0.023 output tensor([[ 4, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 1.016 -0.042 -0.029 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 1.212 -0.071 -0.035 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 1.408 -0.105 -0.041 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 1.214 -0.146 -0.036 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 1.021 -0.182 -0.031 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 0.828 -0.213 -0.026 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[17,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 49 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.21 0.001 -0.006 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.406 -0.004 -0.011 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.601 -0.016 -0.017 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.796 -0.033 -0.023 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.992 -0.056 -0.029 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.797 -0.086 -0.024 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.993 -0.109 -0.03 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 1.19 -0.14 -0.037 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.123 0.996 -0.176 -0.032 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.143 1.193 -0.208 -0.039 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.167 1.39 -0.246 -0.046 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 50 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.243 -0.016 0.006 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.047 -0.01 0.0 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.242 -0.01 0.006 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.047 -0.003 0.0 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.148 -0.003 -0.006 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.047 -0.009 0.0 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.242 -0.009 0.006 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.437 -0.003 0.012 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.632 0.009 0.018 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.827 0.026 0.023 output tensor([[19,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -1.023 0.05 0.029 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -1.218 0.079 0.036 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -1.414 0.115 0.042 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.146 -1.61 0.157 0.048 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.178 -1.417 0.205 0.044 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.206 -1.613 0.249 0.051 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 51 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.177 0.017 -0.005 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.018 0.011 0.001 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.213 0.012 0.007 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.409 0.018 0.012 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.604 0.031 0.018 output tensor([[18,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.409 0.049 0.013 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.215 0.062 0.007 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.021 0.069 0.002 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.217 0.071 0.008 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.023 0.079 0.003 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.171 0.082 -0.003 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.025 0.079 0.004 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.221 0.083 0.01 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.417 0.093 0.016 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.224 0.109 0.011 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.03 0.12 0.006 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.227 0.126 0.013 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.034 0.139 0.008 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 0.159 0.147 0.003 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.038 0.149 0.009 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.235 0.159 0.016 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.042 0.175 0.011 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 0.15 0.186 0.007 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 0.342 0.193 0.002 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.145 0.195 0.009 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.337 0.204 0.004 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.529 0.208 0.0 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.331 0.208 0.007 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.134 0.215 0.014 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 52 lasted for 30 time steps with total reward of 29.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.228 -0.002 0.005 output tensor([[16,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.423 0.003 0.011 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.618 0.014 0.017 output tensor([[23,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.813 0.031 0.023 output tensor([[14,  6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -1.009 0.054 0.029 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -1.204 0.082 0.035 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -1.4 0.118 0.041 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -1.597 0.159 0.048 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -1.793 0.207 0.055 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -1.989 0.261 0.062 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 53 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.154 0.007 -0.006 output tensor([[ 1, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.349 0.001 -0.012 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.544 -0.011 -0.018 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.349 -0.028 -0.012 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.545 -0.04 -0.018 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.74 -0.058 -0.024 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.546 -0.082 -0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.352 -0.1 -0.013 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.159 -0.113 -0.008 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.355 -0.121 -0.014 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.162 -0.136 -0.009 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.359 -0.145 -0.016 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.555 -0.161 -0.023 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.752 -0.184 -0.03 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.949 -0.213 -0.036 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 54 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.165 -0.049 0.005 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.031 -0.045 -0.002 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.163 -0.046 0.004 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.358 -0.042 0.01 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.162 -0.033 0.003 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.034 -0.029 -0.003 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.229 -0.032 -0.009 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.425 -0.04 -0.015 output tensor([[ 4, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.23 -0.055 -0.009 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.036 -0.064 -0.004 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.232 -0.068 -0.01 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.428 -0.077 -0.016 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.234 -0.093 -0.011 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.04 -0.104 -0.005 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.237 -0.11 -0.012 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.043 -0.122 -0.007 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 -0.15 -0.129 -0.002 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 -0.343 -0.13 0.003 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 -0.146 -0.127 -0.003 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.34 -0.131 0.002 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.533 -0.129 0.007 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.726 -0.122 0.012 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.919 -0.111 0.017 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.722 -0.094 0.01 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.526 -0.084 0.004 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.33 -0.081 -0.003 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.134 -0.083 -0.009 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.328 -0.092 -0.004 output tensor([[3, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.521 -0.096 0.001 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.325 -0.095 -0.005 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.519 -0.1 0.0 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.322 -0.099 -0.006 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.516 -0.106 -0.001 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.709 -0.107 0.004 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.903 -0.102 0.009 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -1.096 -0.093 0.015 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -1.29 -0.079 0.02 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -1.484 -0.059 0.025 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.191 -1.288 -0.034 0.019 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.217 -1.092 -0.015 0.013 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.239 -1.287 -0.002 0.019 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.265 -1.482 0.017 0.024 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.294 -1.678 0.041 0.03 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.328 -1.873 0.071 0.036 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.365 -2.069 0.108 0.043 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.407 -1.875 0.151 0.038 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.444 -2.072 0.188 0.044 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.486 -1.879 0.233 0.04 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 55 lasted for 49 time steps with total reward of 48.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.153 -0.049 0.005 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.348 -0.044 0.01 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.152 -0.034 0.004 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.346 -0.03 0.01 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.151 -0.021 0.004 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.346 -0.017 0.009 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.541 -0.007 0.015 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.345 0.008 0.009 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.15 0.017 0.003 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 0.044 0.02 -0.002 output tensor([[ 2, 24]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 0.239 0.018 -0.008 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 0.434 0.01 -0.014 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 0.629 -0.004 -0.02 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.824 -0.024 -0.025 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 1.02 -0.049 -0.031 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.825 -0.08 -0.026 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.631 -0.106 -0.021 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.438 -0.127 -0.015 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.634 -0.142 -0.022 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.831 -0.164 -0.029 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 1.028 -0.193 -0.036 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 1.224 -0.229 -0.042 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 56 lasted for 23 time steps with total reward of 22.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.238 -0.04 0.006 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.432 -0.034 0.012 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.627 -0.022 0.017 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.821 -0.005 0.023 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -1.017 0.018 0.029 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -1.212 0.047 0.035 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -1.017 0.082 0.029 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -1.213 0.112 0.036 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -1.41 0.147 0.042 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -1.606 0.19 0.049 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.205 -1.413 0.238 0.044 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 57 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.151 -0.001 -0.006 output tensor([[ 2, 31]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 0.346 -0.006 -0.011 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.542 -0.018 -0.017 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.737 -0.035 -0.023 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.932 -0.058 -0.029 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.738 -0.087 -0.024 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.934 -0.111 -0.03 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.741 -0.141 -0.025 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.548 -0.167 -0.02 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.355 -0.187 -0.015 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.163 -0.202 -0.011 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 -0.029 -0.213 -0.006 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 58 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.158 -0.026 0.005 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.037 -0.021 -0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.158 -0.022 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.038 -0.017 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.157 -0.018 0.005 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.352 -0.013 0.01 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.157 -0.003 0.004 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 0.038 0.002 -0.001 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.157 0.0 0.004 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 0.038 0.005 -0.001 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.157 0.003 0.005 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.352 0.008 0.01 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.547 0.018 0.016 output tensor([[20,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.352 0.035 0.011 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.158 0.045 0.005 output tensor([[ 5, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 0.037 0.05 -0.001 output tensor([[ 4, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.159 0.049 0.006 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.355 0.055 0.012 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.551 0.067 0.018 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.357 0.084 0.012 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.119 -0.553 0.097 0.019 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.749 0.116 0.025 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.945 0.141 0.032 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -0.752 0.173 0.027 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.179 -0.949 0.2 0.034 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.198 -1.146 0.233 0.041 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 59 lasted for 27 time steps with total reward of 26.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.213 0.024 -0.005 output tensor([[ 4, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.408 0.018 -0.011 output tensor([[ 3, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.603 0.007 -0.017 output tensor([[ 4, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.408 -0.01 -0.011 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.603 -0.021 -0.017 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.798 -0.037 -0.023 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.994 -0.06 -0.029 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 1.19 -0.089 -0.035 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.124 0.996 -0.124 -0.03 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.144 1.192 -0.154 -0.036 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.168 0.999 -0.191 -0.032 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.188 0.807 -0.222 -0.027 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 60 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.205 -0.028 0.006 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.4 -0.022 0.012 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.204 -0.01 0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.009 -0.004 -0.0 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.186 -0.005 -0.006 output tensor([[ 4, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.009 -0.011 -0.0 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.204 -0.011 0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.009 -0.006 -0.0 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.186 -0.006 -0.006 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.009 -0.013 -0.001 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.187 -0.013 -0.006 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.382 -0.02 -0.012 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.577 -0.032 -0.018 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.383 -0.05 -0.013 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.579 -0.063 -0.019 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.774 -0.082 -0.025 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.58 -0.107 -0.02 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.387 -0.127 -0.015 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.194 -0.142 -0.01 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.391 -0.151 -0.016 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.198 -0.167 -0.011 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.395 -0.179 -0.018 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.202 -0.197 -0.014 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.011 -0.211 -0.009 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 61 lasted for 25 time steps with total reward of 24.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.218 0.041 -0.006 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.412 0.035 -0.012 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.607 0.023 -0.017 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.802 0.006 -0.023 output tensor([[ 0, 24]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.997 -0.017 -0.029 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 1.192 -0.046 -0.035 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.998 -0.081 -0.029 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 1.194 -0.11 -0.036 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.15 1.0 -0.145 -0.03 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.17 1.197 -0.176 -0.037 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.194 1.004 -0.213 -0.032 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 62 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.196 -0.029 -0.007 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.391 -0.035 -0.013 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.197 -0.048 -0.007 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.002 -0.056 -0.002 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.192 -0.058 0.004 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.004 -0.054 -0.003 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.2 -0.056 -0.009 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.005 -0.065 -0.003 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.189 -0.068 0.002 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.383 -0.066 0.008 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.577 -0.058 0.013 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.771 -0.045 0.019 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.575 -0.027 0.012 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.38 -0.014 0.006 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.575 -0.008 0.012 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.38 0.004 0.006 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.185 0.01 0.0 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 0.01 0.011 -0.005 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 0.205 0.005 -0.011 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 0.4 -0.006 -0.017 output tensor([[ 0, 26]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 0.596 -0.023 -0.023 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.401 -0.046 -0.017 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.206 -0.063 -0.012 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.012 -0.074 -0.006 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.182 -0.081 -0.001 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 0.014 -0.081 -0.007 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.18 -0.088 -0.002 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.373 -0.09 0.003 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.177 -0.087 -0.003 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 0.019 -0.09 -0.009 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 0.216 -0.099 -0.016 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.022 -0.115 -0.01 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.218 -0.125 -0.017 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.415 -0.142 -0.024 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.612 -0.166 -0.03 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.419 -0.196 -0.026 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.227 -0.222 -0.021 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 63 lasted for 38 time steps with total reward of 37.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.168 -0.047 -0.005 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.026 -0.052 0.0 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.17 -0.052 -0.006 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 0.366 -0.058 -0.012 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 0.171 -0.07 -0.007 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.367 -0.077 -0.013 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.564 -0.089 -0.019 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.76 -0.109 -0.026 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.566 -0.134 -0.02 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.373 -0.155 -0.015 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.57 -0.17 -0.022 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.377 -0.192 -0.018 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.575 -0.21 -0.024 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 64 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.165 0.03 -0.006 output tensor([[ 1, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.36 0.024 -0.012 output tensor([[ 4, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.555 0.012 -0.017 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.75 -0.005 -0.023 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.555 -0.028 -0.017 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.36 -0.045 -0.011 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.166 -0.056 -0.006 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 -0.029 -0.062 -0.0 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.167 -0.063 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 -0.027 -0.069 -0.001 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.169 -0.071 -0.007 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 0.365 -0.078 -0.014 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.171 -0.092 -0.008 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 -0.023 -0.1 -0.003 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 -0.216 -0.103 0.002 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 -0.02 -0.101 -0.004 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 -0.213 -0.106 0.001 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 -0.407 -0.105 0.006 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 -0.6 -0.099 0.011 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 -0.794 -0.088 0.016 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 -0.597 -0.072 0.01 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.401 -0.062 0.004 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.205 -0.059 -0.003 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.4 -0.061 0.003 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.594 -0.059 0.008 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.788 -0.05 0.014 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.592 -0.037 0.008 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.787 -0.029 0.013 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.591 -0.016 0.007 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.786 -0.009 0.013 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.981 0.004 0.019 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -1.176 0.023 0.025 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.982 0.047 0.019 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -1.177 0.066 0.025 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -0.983 0.091 0.02 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -0.789 0.111 0.014 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.177 -0.596 0.125 0.009 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.189 -0.403 0.134 0.004 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.197 -0.21 0.139 -0.001 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.201 -0.017 0.138 -0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.201 -0.214 0.132 0.001 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.205 -0.411 0.133 0.008 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.214 -0.607 0.141 0.014 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.226 -0.414 0.155 0.009 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.234 -0.222 0.164 0.005 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.239 -0.419 0.169 0.011 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.247 -0.616 0.18 0.018 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.259 -0.423 0.198 0.014 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.268 -0.232 0.212 0.009 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 65 lasted for 50 time steps with total reward of 49.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.213 -0.026 -0.007 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.409 -0.033 -0.013 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.214 -0.046 -0.007 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.41 -0.053 -0.013 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.606 -0.066 -0.02 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.802 -0.086 -0.026 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.998 -0.112 -0.032 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 1.194 -0.144 -0.039 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 1.001 -0.183 -0.034 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.808 -0.217 -0.029 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[12,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 66 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.229 0.018 -0.006 output tensor([[ 4, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.034 0.012 0.0 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.162 0.012 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.033 0.018 0.0 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.228 0.018 -0.006 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.033 0.013 0.0 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.163 0.013 0.006 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.358 0.019 0.012 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.553 0.032 0.018 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.749 0.05 0.024 output tensor([[13,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.554 0.074 0.019 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.36 0.093 0.013 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.557 0.107 0.02 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.363 0.127 0.015 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.56 0.141 0.021 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.756 0.163 0.028 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.953 0.19 0.035 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -1.15 0.225 0.042 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 67 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.184 0.031 -0.005 output tensor([[ 2, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.378 0.026 -0.011 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.573 0.015 -0.016 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.378 -0.001 -0.01 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.573 -0.012 -0.016 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.378 -0.028 -0.011 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.573 -0.039 -0.017 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.379 -0.055 -0.011 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.184 -0.066 -0.005 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.38 -0.072 -0.012 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.186 -0.083 -0.006 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 -0.007 -0.09 -0.001 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.189 -0.091 -0.007 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.385 -0.098 -0.014 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.582 -0.112 -0.02 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.388 -0.132 -0.015 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.195 -0.147 -0.01 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.002 -0.157 -0.005 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 -0.19 -0.163 -0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 -0.383 -0.163 0.004 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 -0.575 -0.159 0.009 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 -0.768 -0.15 0.014 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 -0.571 -0.136 0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.374 -0.129 0.0 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.567 -0.129 0.005 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.76 -0.124 0.01 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.954 -0.113 0.015 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -1.147 -0.098 0.02 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -1.341 -0.078 0.026 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -1.535 -0.052 0.031 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -1.339 -0.021 0.025 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -1.144 0.004 0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.948 0.023 0.013 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -0.754 0.036 0.007 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.177 -0.949 0.043 0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.196 -1.145 0.056 0.019 output tensor([[15,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.219 -0.951 0.076 0.014 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.238 -1.147 0.09 0.02 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.261 -0.953 0.11 0.015 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.28 -0.759 0.125 0.01 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.295 -0.956 0.135 0.017 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.314 -1.153 0.152 0.023 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.337 -0.96 0.175 0.018 output tensor([[ 3, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.356 -0.768 0.193 0.014 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.372 -0.965 0.207 0.021 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.391 -0.773 0.227 0.016 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 68 lasted for 47 time steps with total reward of 46.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.208 -0.017 0.006 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.402 -0.01 0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.597 0.002 0.018 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.402 0.019 0.012 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.598 0.031 0.018 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.793 0.049 0.024 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.989 0.073 0.03 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.795 0.104 0.025 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.991 0.128 0.031 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -1.188 0.16 0.038 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.994 0.197 0.033 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.146 -1.191 0.23 0.04 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 69 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.207 -0.031 -0.006 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.403 -0.037 -0.012 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.208 -0.048 -0.006 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.014 -0.054 -0.0 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.21 -0.055 -0.007 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.015 -0.061 -0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.179 -0.062 0.004 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.017 -0.058 -0.002 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.213 -0.06 -0.008 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.019 -0.068 -0.003 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.175 -0.071 0.003 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.369 -0.068 0.008 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.563 -0.06 0.014 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.368 -0.046 0.007 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.172 -0.039 0.001 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 0.024 -0.038 -0.005 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.171 -0.043 0.001 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.365 -0.042 0.006 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.56 -0.036 0.012 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.754 -0.024 0.018 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.949 -0.006 0.023 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.754 0.017 0.017 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.559 0.034 0.012 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.755 0.046 0.018 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.95 0.064 0.024 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -1.146 0.087 0.03 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -0.952 0.117 0.025 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.184 -1.149 0.142 0.031 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.207 -0.956 0.173 0.026 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.226 -0.763 0.2 0.022 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.241 -0.96 0.222 0.029 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 70 lasted for 32 time steps with total reward of 31.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.222 0.048 -0.005 output tensor([[ 4, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.417 0.043 -0.011 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.221 0.032 -0.005 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.416 0.027 -0.01 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.22 0.016 -0.004 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.025 0.012 0.002 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.22 0.013 -0.004 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.415 0.009 -0.01 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.219 -0.001 -0.004 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.414 -0.005 -0.01 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.61 -0.015 -0.016 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.415 -0.031 -0.01 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.61 -0.041 -0.016 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.806 -0.057 -0.022 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 1.002 -0.079 -0.028 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.11 0.808 -0.108 -0.023 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 0.614 -0.131 -0.018 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.138 0.421 -0.149 -0.013 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.146 0.618 -0.162 -0.02 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.159 0.425 -0.181 -0.015 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.167 0.233 -0.196 -0.01 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.172 0.041 -0.207 -0.006 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.173 -0.151 -0.212 -0.001 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 71 lasted for 24 time steps with total reward of 23.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.182 0.008 0.006 output tensor([[30,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.377 0.015 0.012 output tensor([[22,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.572 0.027 0.018 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.768 0.045 0.024 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.573 0.069 0.019 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.769 0.088 0.025 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.965 0.112 0.031 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -1.162 0.144 0.038 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -1.358 0.181 0.044 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -1.554 0.226 0.051 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 72 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.239 -0.01 -0.006 output tensor([[ 5, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.434 -0.016 -0.012 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.629 -0.028 -0.018 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.825 -0.046 -0.024 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 1.021 -0.07 -0.03 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 1.216 -0.1 -0.036 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.128 1.412 -0.136 -0.043 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.156 1.609 -0.179 -0.049 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.188 1.805 -0.229 -0.056 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 73 lasted for 10 time steps with total reward of 9.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 0.18 -0.015 -0.007 output tensor([[ 3, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.015 -0.022 -0.001 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.209 -0.023 0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.014 -0.018 -0.001 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.181 -0.019 -0.007 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.377 -0.026 -0.013 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.572 -0.039 -0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.378 -0.058 -0.013 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 0.183 -0.071 -0.008 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.011 -0.079 -0.003 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.186 -0.082 -0.009 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.008 -0.09 -0.004 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.188 -0.094 -0.01 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.384 -0.104 -0.016 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.581 -0.12 -0.023 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.777 -0.143 -0.029 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.584 -0.173 -0.024 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.391 -0.197 -0.02 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.199 -0.217 -0.015 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[1, 4]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 74 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.204 -0.034 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.009 -0.028 -0.0 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.203 -0.028 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.398 -0.023 0.011 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.203 -0.012 0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.398 -0.006 0.011 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.202 0.005 0.005 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.007 0.01 -0.001 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.188 0.009 -0.007 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.008 0.003 -0.001 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.187 0.002 -0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.383 -0.005 -0.012 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.188 -0.017 -0.006 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.383 -0.023 -0.012 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.578 -0.036 -0.018 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.384 -0.054 -0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.58 -0.067 -0.019 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.385 -0.086 -0.014 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.582 -0.099 -0.02 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.388 -0.119 -0.015 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.195 -0.134 -0.01 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.002 -0.144 -0.005 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 -0.191 -0.148 0.0 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.006 -0.148 -0.007 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 -0.187 -0.155 -0.002 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 -0.38 -0.157 0.003 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.572 -0.153 0.008 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.765 -0.146 0.013 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.568 -0.133 0.006 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.761 -0.127 0.011 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.954 -0.116 0.016 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.758 -0.1 0.009 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.561 -0.09 0.003 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.365 -0.087 -0.003 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.169 -0.091 -0.01 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.362 -0.101 -0.005 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.166 -0.105 -0.011 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 0.03 -0.116 -0.017 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.163 -0.134 -0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.034 -0.146 -0.019 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 0.23 -0.165 -0.026 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 0.427 -0.191 -0.032 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 0.235 -0.223 -0.028 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 75 lasted for 44 time steps with total reward of 43.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.238 -0.042 0.005 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.043 -0.038 -0.001 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.237 -0.039 0.004 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.041 -0.035 -0.002 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.154 -0.037 -0.008 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.35 -0.045 -0.014 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.545 -0.059 -0.02 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.351 -0.079 -0.015 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.157 -0.094 -0.009 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.354 -0.103 -0.016 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.55 -0.119 -0.022 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.357 -0.142 -0.017 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.164 -0.159 -0.012 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.361 -0.171 -0.019 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.168 -0.19 -0.014 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 -0.024 -0.205 -0.01 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 -0.216 -0.215 -0.005 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 76 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.15 -0.029 0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.345 -0.024 0.011 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.54 -0.012 0.017 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.735 0.005 0.023 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.54 0.028 0.017 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.345 0.045 0.011 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.541 0.056 0.018 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.736 0.074 0.024 output tensor([[18,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.932 0.098 0.03 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -1.129 0.128 0.036 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.935 0.164 0.031 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.742 0.196 0.027 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.55 0.223 0.022 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 77 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.169 -0.016 -0.005 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.026 -0.021 0.0 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.221 -0.021 0.006 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.026 -0.015 0.0 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 0.17 -0.015 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.025 -0.021 -0.0 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.22 -0.021 0.006 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.415 -0.015 0.011 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.61 -0.004 0.017 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.805 0.013 0.023 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -1.0 0.036 0.029 output tensor([[20,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -1.196 0.065 0.035 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -1.001 0.1 0.029 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -1.198 0.129 0.036 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.171 -1.394 0.165 0.043 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.199 -1.201 0.208 0.038 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.223 -1.008 0.246 0.033 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[8, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 78 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.191 0.009 0.006 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.386 0.015 0.012 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.191 0.028 0.007 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.003 0.034 0.001 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.192 0.035 0.007 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.388 0.042 0.013 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.193 0.055 0.007 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.001 0.063 0.002 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.195 0.065 0.008 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.391 0.073 0.014 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.197 0.087 0.009 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.003 0.097 0.004 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.199 0.1 0.01 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.396 0.111 0.017 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.592 0.127 0.023 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.399 0.15 0.018 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.206 0.169 0.013 output tensor([[ 3, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.014 0.182 0.009 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.211 0.191 0.016 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.019 0.206 0.011 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.216 0.217 0.018 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 79 lasted for 22 time steps with total reward of 21.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.176 0.026 -0.006 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.019 0.02 -0.0 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.175 0.02 -0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.37 0.014 -0.012 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.565 0.002 -0.018 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.76 -0.016 -0.023 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.956 -0.039 -0.029 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.761 -0.068 -0.024 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.567 -0.092 -0.018 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.763 -0.11 -0.025 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 0.569 -0.135 -0.02 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.376 -0.155 -0.015 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.123 0.184 -0.169 -0.01 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 0.381 -0.179 -0.017 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.578 -0.196 -0.024 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.146 0.386 -0.219 -0.019 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 80 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.214 0.025 0.007 output tensor([[23,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.019 0.032 0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.176 0.033 -0.005 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.37 0.028 -0.01 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.175 0.018 -0.004 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.021 0.014 0.002 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.216 0.015 0.008 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.411 0.023 0.014 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.607 0.037 0.02 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.802 0.056 0.026 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.608 0.082 0.02 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.414 0.102 0.015 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.61 0.117 0.021 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.807 0.138 0.028 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.614 0.166 0.023 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.421 0.189 0.018 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.618 0.207 0.025 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.815 0.232 0.032 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 81 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.177 -0.019 0.005 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.372 -0.014 0.011 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.567 -0.004 0.017 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.371 0.013 0.011 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.177 0.024 0.005 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 0.018 0.029 -0.001 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.177 0.028 0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 0.017 0.033 -0.0 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.178 0.032 0.006 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.374 0.038 0.012 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.569 0.05 0.018 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.765 0.067 0.024 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.961 0.091 0.03 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -0.767 0.122 0.025 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -0.574 0.146 0.02 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -0.381 0.166 0.015 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.175 -0.188 0.181 0.01 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.179 -0.386 0.192 0.017 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.186 -0.583 0.209 0.024 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.198 -0.78 0.233 0.031 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 82 lasted for 21 time steps with total reward of 20.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.219 0.046 -0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.414 0.04 -0.012 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.218 0.029 -0.005 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.022 0.023 0.001 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.173 0.024 0.007 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 -0.368 0.03 0.013 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.564 0.043 0.019 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.369 0.062 0.013 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.175 0.075 0.008 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.019 0.082 0.002 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.213 0.084 -0.003 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.016 0.081 0.003 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.18 0.085 0.01 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.014 0.094 0.004 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.208 0.099 -0.001 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.011 0.098 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.205 0.103 0.0 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.008 0.104 0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.188 0.11 0.013 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.384 0.124 0.02 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.191 0.143 0.015 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.388 0.158 0.021 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.585 0.18 0.028 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.392 0.208 0.024 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.2 0.231 0.019 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 83 lasted for 26 time steps with total reward of 25.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.208 0.03 0.006 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.013 0.036 0.0 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.209 0.036 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.014 0.043 0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.18 0.043 -0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.016 0.038 0.001 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.179 0.039 -0.004 output tensor([[ 3, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.374 0.035 -0.01 output tensor([[ 2, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.568 0.025 -0.016 output tensor([[ 3, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.763 0.009 -0.021 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.568 -0.012 -0.015 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.373 -0.028 -0.01 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.568 -0.037 -0.016 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.764 -0.053 -0.022 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.569 -0.075 -0.016 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.119 0.375 -0.091 -0.011 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 0.182 -0.102 -0.006 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 0.378 -0.108 -0.012 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.138 0.185 -0.12 -0.007 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 -0.009 -0.127 -0.002 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 -0.202 -0.129 0.003 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.138 -0.395 -0.126 0.008 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 -0.198 -0.118 0.001 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 -0.391 -0.116 0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 -0.195 -0.11 0.0 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.002 -0.11 -0.007 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 -0.192 -0.116 -0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.11 -0.385 -0.117 0.004 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 -0.578 -0.114 0.009 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 -0.382 -0.105 0.002 output tensor([[0, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 -0.575 -0.103 0.007 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 -0.769 -0.095 0.013 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 -0.962 -0.083 0.018 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.766 -0.065 0.011 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.57 -0.053 0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.374 -0.048 -0.001 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.179 -0.049 -0.007 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.017 -0.056 -0.013 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.177 -0.07 -0.008 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.371 -0.077 -0.002 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.565 -0.08 0.003 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.759 -0.077 0.008 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.953 -0.069 0.014 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -1.147 -0.055 0.019 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.951 -0.036 0.013 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.756 -0.023 0.007 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.95 -0.016 0.012 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -1.145 -0.004 0.018 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -1.34 0.014 0.024 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.183 -1.536 0.039 0.03 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.213 -1.731 0.069 0.036 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.248 -1.927 0.105 0.042 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.286 -1.733 0.147 0.037 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.321 -1.54 0.184 0.032 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.352 -1.347 0.216 0.028 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 84 lasted for 56 time steps with total reward of 55.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.226 -0.024 0.006 output tensor([[21,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.421 -0.019 0.011 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.616 -0.007 0.017 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.811 0.01 0.023 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -1.006 0.033 0.029 output tensor([[15,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -1.202 0.061 0.035 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -1.398 0.096 0.041 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -1.594 0.137 0.047 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 -1.79 0.185 0.054 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -1.596 0.239 0.049 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 85 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.193 0.036 -0.005 output tensor([[ 3, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.003 0.031 0.001 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.198 0.033 0.007 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.394 0.04 0.013 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.59 0.053 0.02 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.785 0.073 0.026 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.981 0.099 0.032 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -1.178 0.131 0.038 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.984 0.169 0.033 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.791 0.203 0.029 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.599 0.231 0.024 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 86 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.204 0.0 0.007 output tensor([[24,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.4 0.007 0.012 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.205 0.019 0.007 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.01 0.026 0.001 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 0.185 0.027 -0.005 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 0.38 0.022 -0.011 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.575 0.011 -0.016 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.77 -0.005 -0.022 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.965 -0.027 -0.028 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.77 -0.055 -0.022 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.966 -0.077 -0.028 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.772 -0.106 -0.023 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.578 -0.129 -0.018 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.385 -0.147 -0.013 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.192 -0.16 -0.008 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 -0.0 -0.168 -0.003 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 -0.193 -0.171 0.001 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 -0.385 -0.17 0.006 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.188 -0.164 -0.001 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.381 -0.164 0.004 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.573 -0.16 0.009 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.765 -0.152 0.013 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.958 -0.138 0.018 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -1.151 -0.12 0.023 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -1.345 -0.097 0.028 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -1.538 -0.068 0.034 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -1.343 -0.035 0.027 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -1.147 -0.008 0.021 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -1.342 0.014 0.027 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -1.537 0.041 0.033 output tensor([[20,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.189 -1.733 0.074 0.039 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.223 -1.929 0.113 0.045 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.262 -1.735 0.158 0.04 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.296 -1.542 0.198 0.035 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.327 -1.738 0.234 0.042 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[17,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 87 lasted for 36 time steps with total reward of 35.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.152 0.008 0.007 output tensor([[24,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.347 0.015 0.013 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.152 0.028 0.007 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.043 0.035 0.001 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.237 0.036 -0.004 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.042 0.032 0.002 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.154 0.034 0.008 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.35 0.041 0.014 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.545 0.055 0.02 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.741 0.075 0.026 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.937 0.102 0.033 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.743 0.134 0.027 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.94 0.161 0.034 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -1.136 0.195 0.041 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -1.333 0.236 0.048 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 88 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.234 0.048 0.006 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.04 0.054 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.236 0.055 0.007 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.041 0.062 0.001 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.153 0.063 -0.004 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.347 0.059 -0.009 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.541 0.05 -0.015 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.736 0.035 -0.02 output tensor([[ 5, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.54 0.014 -0.014 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.345 0.0 -0.008 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.15 -0.008 -0.003 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 -0.045 -0.011 0.003 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.15 -0.008 -0.003 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 -0.045 -0.011 0.003 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 -0.24 -0.008 0.009 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 -0.045 0.001 0.003 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.15 0.004 -0.003 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.345 0.001 -0.009 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.54 -0.007 -0.015 output tensor([[ 3, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.736 -0.022 -0.02 output tensor([[ 1, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.931 -0.043 -0.026 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.736 -0.069 -0.021 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 0.932 -0.09 -0.027 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.149 0.738 -0.117 -0.022 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.164 0.935 -0.139 -0.028 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.183 1.131 -0.167 -0.035 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.206 1.328 -0.202 -0.042 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.232 1.135 -0.244 -0.037 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 89 lasted for 29 time steps with total reward of 28.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.221 -0.038 -0.007 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.417 -0.045 -0.013 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.612 -0.058 -0.019 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.418 -0.076 -0.013 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.614 -0.09 -0.02 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.42 -0.109 -0.014 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.227 -0.124 -0.009 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.423 -0.133 -0.016 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.62 -0.149 -0.022 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.427 -0.171 -0.018 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 0.235 -0.189 -0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 0.432 -0.202 -0.02 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.24 -0.221 -0.015 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 90 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.171 -0.007 0.005 output tensor([[21,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.366 -0.002 0.011 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.17 0.008 0.005 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.025 0.013 -0.001 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.171 0.012 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.366 0.017 0.011 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.561 0.028 0.017 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.367 0.045 0.011 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.172 0.056 0.006 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 0.022 0.061 0.0 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 0.216 0.061 -0.005 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.41 0.056 -0.011 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.215 0.045 -0.005 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.019 0.04 0.001 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.177 0.042 0.008 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.373 0.049 0.014 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.178 0.063 0.008 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.016 0.071 0.003 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.18 0.074 0.009 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.376 0.083 0.015 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.572 0.098 0.022 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.769 0.12 0.028 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.575 0.148 0.023 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.382 0.171 0.018 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.19 0.189 0.013 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.387 0.202 0.02 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.584 0.223 0.027 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 91 lasted for 28 time steps with total reward of 27.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.154 0.044 -0.006 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.349 0.039 -0.011 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.544 0.028 -0.017 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.738 0.011 -0.022 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.933 -0.011 -0.028 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.738 -0.04 -0.022 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.934 -0.062 -0.029 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 1.13 -0.09 -0.035 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.936 -0.125 -0.029 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.742 -0.155 -0.024 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.149 0.549 -0.179 -0.02 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.16 0.357 -0.199 -0.015 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.167 0.554 -0.214 -0.022 output tensor([[0, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 92 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.172 0.031 -0.006 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.367 0.024 -0.012 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.172 0.012 -0.006 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.024 0.006 -0.0 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.219 0.006 0.006 output tensor([[13,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.024 0.011 -0.0 output tensor([[ 4, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.171 0.011 -0.006 output tensor([[ 1, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.366 0.005 -0.012 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.561 -0.006 -0.018 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.756 -0.024 -0.023 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.952 -0.047 -0.029 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.757 -0.077 -0.024 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.563 -0.101 -0.019 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.369 -0.119 -0.013 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.566 -0.132 -0.02 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 0.763 -0.152 -0.026 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.125 0.57 -0.179 -0.022 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.136 0.767 -0.201 -0.029 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.151 0.964 -0.229 -0.035 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 93 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.191 0.009 -0.006 output tensor([[ 1, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.386 0.004 -0.011 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.191 -0.008 -0.005 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.386 -0.013 -0.011 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.581 -0.025 -0.017 output tensor([[ 2, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.387 -0.042 -0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.192 -0.054 -0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.002 -0.06 -0.001 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.197 -0.06 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.001 -0.055 -0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.195 -0.057 -0.007 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.001 -0.064 -0.002 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.193 -0.066 0.003 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.387 -0.063 0.009 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.581 -0.054 0.014 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.776 -0.039 0.02 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.97 -0.02 0.025 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.775 0.006 0.019 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.97 0.025 0.025 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.775 0.051 0.02 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.581 0.07 0.014 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.777 0.084 0.02 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.583 0.105 0.015 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.779 0.12 0.022 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.976 0.141 0.028 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.129 -1.173 0.169 0.035 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.153 -0.98 0.204 0.03 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.172 -1.177 0.234 0.037 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 94 lasted for 29 time steps with total reward of 28.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.152 -0.014 0.006 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.347 -0.008 0.012 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.542 0.004 0.018 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.347 0.021 0.012 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.153 0.033 0.006 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.042 0.039 0.0 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.237 0.039 -0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.431 0.034 -0.011 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.236 0.023 -0.005 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.04 0.018 0.001 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.155 0.019 0.007 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.351 0.026 0.013 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.546 0.04 0.019 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.742 0.059 0.025 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.938 0.084 0.031 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.744 0.115 0.026 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.94 0.142 0.033 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.747 0.174 0.028 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.554 0.202 0.023 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.362 0.225 0.019 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 95 lasted for 21 time steps with total reward of 20.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.148 0.017 0.006 output tensor([[18,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.047 0.023 0.001 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.241 0.024 -0.005 output tensor([[ 4, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.436 0.019 -0.011 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.241 0.008 -0.005 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.436 0.003 -0.011 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.631 -0.008 -0.017 output tensor([[ 5, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.436 -0.024 -0.011 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.241 -0.035 -0.005 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.437 -0.04 -0.011 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.632 -0.051 -0.017 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.107 0.438 -0.068 -0.012 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.115 0.634 -0.08 -0.018 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.128 0.83 -0.098 -0.024 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.145 1.026 -0.122 -0.031 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.165 0.833 -0.153 -0.026 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.182 1.03 -0.178 -0.032 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.202 1.226 -0.211 -0.039 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 96 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.147 0.013 0.006 output tensor([[12,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.342 0.02 0.012 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.537 0.032 0.018 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.733 0.05 0.024 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.929 0.074 0.03 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.735 0.105 0.025 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.541 0.13 0.02 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.348 0.15 0.015 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.088 -0.545 0.165 0.022 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.741 0.187 0.028 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.549 0.215 0.024 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 97 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.189 0.017 -0.007 output tensor([[ 4, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.006 0.01 -0.001 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.201 0.01 0.005 output tensor([[10,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.006 0.015 -0.001 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.189 0.014 -0.006 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.384 0.008 -0.012 output tensor([[ 4, 24]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.579 -0.004 -0.018 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.774 -0.022 -0.024 output tensor([[5, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.579 -0.046 -0.018 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.775 -0.064 -0.024 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.971 -0.088 -0.03 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.777 -0.119 -0.025 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 0.583 -0.144 -0.02 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.39 -0.164 -0.015 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.587 -0.18 -0.022 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 0.784 -0.202 -0.029 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.124 0.592 -0.231 -0.024 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 98 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.216 0.042 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.411 0.048 0.012 output tensor([[22,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.607 0.06 0.018 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.803 0.078 0.024 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.999 0.102 0.031 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.805 0.133 0.025 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.612 0.159 0.021 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.809 0.179 0.027 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -1.006 0.206 0.034 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -1.203 0.241 0.041 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 99 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.21 0.042 -0.005 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.404 0.037 -0.011 output tensor([[ 1, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.208 0.026 -0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.013 0.022 0.001 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.183 0.023 0.007 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.378 0.03 0.013 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.183 0.044 0.008 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.011 0.051 0.002 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.185 0.054 0.008 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.38 0.062 0.014 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.186 0.076 0.009 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.008 0.085 0.004 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.202 0.089 -0.002 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.395 0.087 -0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.199 0.081 -0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.003 0.08 0.006 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.193 0.086 0.012 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.39 0.098 0.019 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.586 0.117 0.025 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.392 0.141 0.02 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.589 0.161 0.027 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.396 0.188 0.022 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.204 0.21 0.017 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 7]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 100 lasted for 24 time steps with total reward of 23.0\n",
            "\n",
            "\n",
            " Model saved!\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.231 0.04 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.425 0.035 -0.011 output tensor([[ 5, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.62 0.024 -0.017 output tensor([[ 2, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.815 0.007 -0.023 output tensor([[ 4, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 1.01 -0.016 -0.028 output tensor([[ 4, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.077 1.205 -0.044 -0.034 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 1.401 -0.078 -0.04 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.129 1.207 -0.119 -0.035 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.153 1.403 -0.154 -0.042 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.181 1.21 -0.195 -0.037 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.205 1.406 -0.232 -0.044 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 101 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.242 0.018 0.006 output tensor([[13,  6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.437 0.023 0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.633 0.035 0.018 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.828 0.053 0.024 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -1.024 0.077 0.03 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -1.22 0.107 0.036 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -1.416 0.143 0.043 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -1.223 0.186 0.038 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -1.419 0.224 0.045 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 4]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 102 lasted for 10 time steps with total reward of 9.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.229 -0.005 -0.006 output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.424 -0.011 -0.012 output tensor([[ 4, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.229 -0.022 -0.006 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.034 -0.028 -0.0 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.23 -0.028 -0.006 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.425 -0.035 -0.012 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.621 -0.047 -0.018 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.816 -0.065 -0.024 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 1.012 -0.09 -0.031 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.818 -0.12 -0.025 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.625 -0.146 -0.02 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.432 -0.166 -0.016 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.1 0.24 -0.182 -0.011 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 0.047 -0.193 -0.006 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 -0.144 -0.199 -0.002 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.053 -0.2 -0.009 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 -0.139 -0.209 -0.004 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 -0.331 -0.213 0.0 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 103 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.178 0.033 -0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.373 0.026 -0.012 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.178 0.014 -0.006 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.018 0.008 -0.0 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.213 0.008 0.006 output tensor([[14,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.408 0.014 0.012 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.604 0.026 0.018 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.799 0.044 0.024 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.605 0.067 0.018 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.41 0.085 0.013 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.607 0.098 0.019 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.413 0.117 0.014 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.609 0.131 0.02 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.806 0.151 0.027 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -1.003 0.178 0.034 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.81 0.212 0.029 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 104 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.203 0.039 -0.006 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.398 0.034 -0.011 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.592 0.022 -0.017 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.787 0.006 -0.023 output tensor([[ 3, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.982 -0.017 -0.028 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.787 -0.045 -0.023 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.593 -0.068 -0.017 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.398 -0.085 -0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.205 -0.097 -0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.011 -0.103 -0.001 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.207 -0.104 -0.008 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.404 -0.112 -0.014 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.21 -0.126 -0.009 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.084 0.017 -0.135 -0.004 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.214 -0.138 -0.011 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.021 -0.149 -0.006 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 -0.172 -0.155 -0.001 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.025 -0.156 -0.008 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.222 -0.163 -0.014 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.419 -0.177 -0.021 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 0.616 -0.198 -0.028 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.813 -0.226 -0.035 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 105 lasted for 23 time steps with total reward of 22.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.174 0.048 -0.006 output tensor([[ 4, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.369 0.042 -0.012 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.173 0.03 -0.006 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.368 0.025 -0.011 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.562 0.013 -0.017 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.757 -0.003 -0.023 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.953 -0.026 -0.029 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 1.148 -0.055 -0.035 output tensor([[6, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 1.344 -0.089 -0.041 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.162 1.54 -0.13 -0.047 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.193 1.346 -0.177 -0.042 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.22 1.153 -0.219 -0.037 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 106 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.151 -0.038 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.346 -0.033 0.011 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.541 -0.021 0.017 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.345 -0.005 0.011 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.54 0.006 0.017 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.735 0.023 0.023 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.541 0.045 0.017 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.736 0.062 0.023 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.542 0.085 0.018 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.738 0.103 0.024 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.935 0.127 0.03 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.166 -1.131 0.157 0.037 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.189 -0.938 0.194 0.032 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.208 -0.745 0.226 0.028 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 107 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.21 -0.025 0.006 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.405 -0.019 0.012 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.6 -0.008 0.017 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.795 0.01 0.023 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.6 0.033 0.017 output tensor([[ 1, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.405 0.05 0.012 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.211 0.061 0.006 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.017 0.068 0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.213 0.068 0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.018 0.075 0.001 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 0.175 0.077 -0.004 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.021 0.073 0.002 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.217 0.075 0.009 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.413 0.084 0.015 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.609 0.099 0.021 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.805 0.12 0.028 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -1.002 0.148 0.034 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -1.198 0.182 0.041 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.186 -1.005 0.224 0.036 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 108 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.187 -0.049 0.006 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.009 -0.043 -0.0 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.186 -0.043 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.01 -0.037 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.185 -0.038 0.005 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.379 -0.033 0.011 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.574 -0.022 0.016 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.379 -0.006 0.01 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.183 0.004 0.004 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.379 0.008 0.01 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.574 0.019 0.016 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.769 0.035 0.022 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.965 0.057 0.028 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -1.16 0.085 0.034 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -1.356 0.119 0.041 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -1.553 0.16 0.047 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -1.359 0.207 0.042 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -1.167 0.25 0.038 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 109 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.155 -0.034 -0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.04 -0.041 -0.001 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.156 -0.043 -0.007 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.351 -0.05 -0.013 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.547 -0.063 -0.02 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 0.743 -0.083 -0.026 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.549 -0.109 -0.021 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.356 -0.129 -0.015 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.552 -0.145 -0.022 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.359 -0.167 -0.017 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.167 -0.184 -0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.364 -0.196 -0.019 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.172 -0.216 -0.015 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 110 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.198 -0.034 0.005 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.392 -0.029 0.01 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.197 -0.019 0.004 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.392 -0.014 0.01 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.587 -0.004 0.016 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.782 0.012 0.022 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.977 0.033 0.028 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.782 0.061 0.022 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.588 0.083 0.017 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.394 0.1 0.011 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.591 0.111 0.018 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.787 0.128 0.024 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.594 0.153 0.019 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -0.401 0.172 0.014 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.209 0.186 0.01 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.406 0.196 0.017 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.603 0.212 0.024 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 111 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.24 -0.001 0.006 output tensor([[11,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.435 0.006 0.012 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.631 0.018 0.018 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.826 0.036 0.024 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.631 0.06 0.018 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.061 -0.437 0.078 0.013 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.633 0.091 0.019 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.439 0.111 0.014 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.246 0.125 0.009 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -0.443 0.134 0.016 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.639 0.149 0.022 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -0.836 0.171 0.029 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -1.033 0.2 0.036 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -1.229 0.236 0.043 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 112 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.231 0.036 -0.007 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.426 0.029 -0.012 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.62 0.017 -0.018 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.815 -0.001 -0.024 output tensor([[ 4, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 1.011 -0.025 -0.03 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 1.206 -0.054 -0.036 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 1.011 -0.09 -0.03 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 0.818 -0.12 -0.025 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.143 1.014 -0.145 -0.031 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.163 0.821 -0.176 -0.026 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.179 0.628 -0.202 -0.022 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.192 0.436 -0.224 -0.017 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 113 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.219 0.042 0.007 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.024 0.049 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.17 0.05 -0.004 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.364 0.045 -0.01 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.559 0.035 -0.016 output tensor([[ 4, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.753 0.02 -0.021 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 0.558 -0.001 -0.015 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.753 -0.017 -0.021 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 0.949 -0.038 -0.027 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.124 0.754 -0.065 -0.021 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.139 0.56 -0.086 -0.016 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.15 0.756 -0.102 -0.022 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.166 0.952 -0.124 -0.029 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.185 0.759 -0.153 -0.024 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.2 0.566 -0.177 -0.019 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.211 0.763 -0.196 -0.026 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.226 0.96 -0.221 -0.033 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 114 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 0.171 0.023 -0.006 output tensor([[ 0, 24]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.025 0.016 -0.0 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.22 0.016 0.006 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.415 0.021 0.011 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.221 0.033 0.006 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.416 0.039 0.012 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.222 0.051 0.006 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.027 0.057 0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.223 0.057 0.007 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.419 0.064 0.013 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.615 0.077 0.019 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.811 0.097 0.026 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.617 0.122 0.02 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.424 0.143 0.015 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 -0.231 0.158 0.011 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.14 -0.428 0.169 0.017 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.148 -0.625 0.186 0.024 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -0.433 0.21 0.02 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 115 lasted for 19 time steps with total reward of 18.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.231 0.036 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.426 0.041 0.012 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.622 0.053 0.018 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.818 0.07 0.024 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -1.014 0.094 0.03 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.82 0.124 0.025 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -1.016 0.149 0.031 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -0.823 0.181 0.027 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -0.631 0.207 0.022 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.171 -0.828 0.229 0.029 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 116 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.187 0.003 0.007 output tensor([[12,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.382 0.01 0.013 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.187 0.023 0.007 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.008 0.029 0.001 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.203 0.03 -0.005 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.007 0.026 0.001 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.202 0.027 -0.004 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.006 0.023 0.002 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.189 0.025 0.008 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.385 0.032 0.014 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.58 0.046 0.02 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.776 0.066 0.026 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.972 0.092 0.032 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -1.168 0.124 0.039 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -1.364 0.163 0.045 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -1.56 0.208 0.052 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -1.756 0.26 0.059 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 117 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.231 0.014 0.007 output tensor([[22,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.427 0.021 0.013 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.622 0.034 0.019 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.818 0.052 0.025 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.623 0.077 0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.819 0.096 0.026 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.625 0.122 0.02 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.822 0.142 0.027 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.14 -0.629 0.169 0.022 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.152 -0.826 0.191 0.029 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.169 -1.023 0.22 0.036 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 118 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.175 0.012 -0.006 output tensor([[ 2, 24]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.37 0.006 -0.012 output tensor([[ 5, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.565 -0.006 -0.017 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.37 -0.023 -0.012 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.566 -0.035 -0.018 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.371 -0.052 -0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.177 -0.064 -0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 -0.018 -0.071 -0.001 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.179 -0.072 -0.007 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 -0.016 -0.079 -0.002 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.181 -0.081 -0.008 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 -0.013 -0.089 -0.003 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.183 -0.092 -0.009 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 0.379 -0.101 -0.016 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 0.186 -0.117 -0.01 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.382 -0.127 -0.017 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.189 -0.144 -0.012 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 -0.004 -0.156 -0.007 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 -0.196 -0.164 -0.002 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 -0.389 -0.166 0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 -0.192 -0.164 -0.004 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 -0.384 -0.168 0.0 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 -0.577 -0.168 0.005 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 -0.769 -0.163 0.01 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 -0.572 -0.153 0.003 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 -0.765 -0.15 0.008 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.957 -0.142 0.013 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -1.15 -0.13 0.017 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -1.343 -0.112 0.022 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -1.147 -0.09 0.016 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.951 -0.074 0.01 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -1.145 -0.064 0.015 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -1.339 -0.049 0.02 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -1.143 -0.029 0.014 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.948 -0.014 0.008 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.162 -1.143 -0.006 0.014 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.184 -0.947 0.008 0.008 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.203 -0.752 0.016 0.002 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.218 -0.948 0.018 0.008 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.237 -0.753 0.026 0.002 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.252 -0.948 0.029 0.009 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.271 -0.754 0.037 0.003 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.286 -0.559 0.04 -0.003 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.298 -0.755 0.037 0.003 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.313 -0.95 0.041 0.009 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.332 -1.146 0.05 0.016 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.355 -1.342 0.066 0.022 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.381 -1.148 0.087 0.016 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.404 -1.344 0.104 0.023 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.431 -1.15 0.126 0.017 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.454 -1.347 0.144 0.024 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.481 -1.154 0.168 0.019 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.504 -0.961 0.187 0.014 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.524 -0.769 0.201 0.01 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.539 -0.966 0.211 0.017 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 119 lasted for 56 time steps with total reward of 55.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.182 -0.005 0.005 output tensor([[19,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.013 -0.0 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.208 -0.001 -0.007 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.404 -0.008 -0.013 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.599 -0.021 -0.019 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.794 -0.039 -0.025 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.6 -0.064 -0.019 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.795 -0.083 -0.025 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.992 -0.108 -0.032 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.798 -0.139 -0.026 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.124 0.994 -0.166 -0.033 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.144 1.191 -0.199 -0.04 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.168 0.999 -0.239 -0.035 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[10,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 120 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.157 -0.046 -0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.037 -0.053 -0.001 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.158 -0.054 -0.007 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.036 -0.061 -0.001 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.23 -0.062 0.004 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.424 -0.058 0.009 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.619 -0.049 0.015 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.813 -0.034 0.02 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.617 -0.013 0.014 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.422 0.001 0.008 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.227 0.009 0.003 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.032 0.012 -0.003 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.227 0.009 0.003 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.423 0.011 0.009 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.228 0.02 0.003 output tensor([[ 1, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.033 0.023 -0.003 output tensor([[ 4, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.162 0.02 -0.009 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.033 0.011 -0.003 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.162 0.009 -0.008 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.357 0.0 -0.014 output tensor([[ 5, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 0.552 -0.014 -0.02 output tensor([[ 4, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.357 -0.034 -0.014 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.162 -0.048 -0.009 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.032 -0.057 -0.003 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.164 -0.06 -0.009 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 0.36 -0.069 -0.016 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 0.556 -0.085 -0.022 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.362 -0.107 -0.016 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.168 -0.123 -0.011 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.025 -0.134 -0.006 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.172 -0.141 -0.013 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.368 -0.154 -0.02 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.176 -0.173 -0.015 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.017 -0.188 -0.01 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.209 -0.198 -0.006 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.011 -0.204 -0.013 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.186 -0.216 -0.02 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 121 lasted for 38 time steps with total reward of 37.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.157 0.047 0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.353 0.053 0.013 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.158 0.066 0.007 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.036 0.073 0.002 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.23 0.074 -0.004 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.424 0.071 -0.009 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.228 0.062 -0.003 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.032 0.059 0.003 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.164 0.062 0.01 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.36 0.072 0.016 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.556 0.088 0.022 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.752 0.11 0.029 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.949 0.138 0.035 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.755 0.173 0.03 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.563 0.204 0.025 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.76 0.229 0.032 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 122 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.168 0.018 -0.007 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.028 0.011 -0.001 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.223 0.01 0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.028 0.015 -0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.167 0.015 -0.006 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.028 0.008 -0.0 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.224 0.008 0.005 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.419 0.013 0.011 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.614 0.025 0.017 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.81 0.042 0.023 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -1.005 0.065 0.029 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -1.201 0.095 0.036 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.096 -1.007 0.13 0.03 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.116 -0.814 0.161 0.025 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.621 0.186 0.021 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.429 0.207 0.016 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.153 -0.237 0.223 0.012 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 123 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.197 0.049 0.006 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.392 0.055 0.012 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.588 0.067 0.018 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.784 0.085 0.025 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.98 0.11 0.031 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -1.177 0.141 0.037 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.983 0.178 0.032 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.791 0.211 0.028 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 124 lasted for 9 time steps with total reward of 8.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.199 -0.031 0.005 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.004 -0.026 -0.001 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.199 -0.026 0.005 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.393 -0.021 0.011 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.588 -0.011 0.016 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.783 0.006 0.022 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.978 0.028 0.028 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.784 0.056 0.023 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.979 0.079 0.029 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -1.175 0.108 0.035 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.982 0.143 0.03 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.788 0.173 0.025 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.985 0.198 0.032 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -1.182 0.229 0.039 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 125 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.204 -0.023 0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.009 -0.017 0.0 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.203 -0.016 0.006 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.008 -0.01 0.0 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.203 -0.01 0.006 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.008 -0.004 0.0 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.187 -0.004 -0.006 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.383 -0.01 -0.012 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.188 -0.022 -0.006 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.007 -0.027 -0.0 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.202 -0.028 0.005 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.397 -0.022 0.011 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.591 -0.011 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.786 0.006 0.023 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.982 0.029 0.029 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.787 0.057 0.023 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -0.983 0.08 0.029 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -1.179 0.109 0.035 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.156 -0.985 0.144 0.03 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.176 -1.181 0.175 0.037 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.2 -1.378 0.212 0.044 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 126 lasted for 22 time steps with total reward of 21.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.155 -0.045 -0.005 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.351 -0.05 -0.011 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.547 -0.062 -0.018 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.352 -0.079 -0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.549 -0.091 -0.018 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.355 -0.11 -0.013 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.551 -0.123 -0.02 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.358 -0.143 -0.015 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.165 -0.157 -0.01 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.027 -0.167 -0.005 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.22 -0.172 -0.0 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.023 -0.172 -0.007 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.215 -0.179 -0.002 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.407 -0.182 0.002 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.599 -0.18 0.007 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.402 -0.173 -0.0 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.594 -0.173 0.005 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.397 -0.168 -0.002 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.59 -0.17 0.002 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.782 -0.168 0.007 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.585 -0.161 0.0 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.777 -0.16 0.005 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.58 -0.155 -0.002 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.773 -0.157 0.003 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.966 -0.154 0.008 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -1.158 -0.146 0.013 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.144 -0.961 -0.133 0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -0.765 -0.127 -0.001 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.179 -0.568 -0.127 -0.007 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.19 -0.761 -0.135 -0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.206 -0.954 -0.137 0.003 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.225 -0.757 -0.134 -0.004 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.24 -0.561 -0.138 -0.011 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.251 -0.364 -0.148 -0.017 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.258 -0.557 -0.166 -0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.269 -0.36 -0.178 -0.019 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.277 -0.163 -0.197 -0.026 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.28 -0.355 -0.223 -0.021 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 127 lasted for 39 time steps with total reward of 38.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.226 0.006 0.006 output tensor([[10,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.421 0.012 0.012 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.226 0.024 0.006 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.421 0.03 0.012 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.617 0.042 0.018 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.813 0.061 0.024 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.618 0.085 0.019 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.424 0.104 0.014 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.621 0.118 0.02 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.427 0.138 0.015 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.624 0.153 0.022 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -0.821 0.174 0.028 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.628 0.203 0.024 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.14 -0.436 0.227 0.019 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 128 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.216 -0.021 0.006 output tensor([[15,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.411 -0.014 0.012 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.606 -0.002 0.018 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.41 0.016 0.012 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.216 0.028 0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.021 0.034 0.001 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.216 0.034 0.007 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.412 0.041 0.013 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.608 0.054 0.019 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.413 0.073 0.013 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.609 0.086 0.02 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.806 0.106 0.026 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -1.002 0.132 0.032 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -1.198 0.164 0.039 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -1.395 0.203 0.046 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.175 -1.591 0.249 0.053 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 129 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.206 0.007 0.005 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.011 0.012 -0.001 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 0.184 0.012 -0.006 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 0.379 0.005 -0.012 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.574 -0.007 -0.018 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.379 -0.025 -0.012 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 0.184 -0.037 -0.006 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.011 -0.043 -0.001 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.205 -0.044 0.005 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.01 -0.039 -0.001 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.204 -0.041 0.004 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.399 -0.037 0.01 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.593 -0.027 0.015 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.788 -0.011 0.021 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.593 0.01 0.015 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.788 0.025 0.021 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.593 0.046 0.015 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.399 0.061 0.01 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.595 0.071 0.016 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.791 0.087 0.022 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.119 -0.597 0.11 0.017 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.793 0.127 0.024 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.146 -0.6 0.15 0.019 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -0.797 0.169 0.025 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.174 -0.994 0.194 0.032 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.194 -0.801 0.226 0.028 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 130 lasted for 27 time steps with total reward of 26.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.19 -0.02 -0.005 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.005 -0.025 0.0 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.19 -0.025 -0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 -0.005 -0.031 0.0 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 -0.199 -0.031 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.394 -0.025 0.011 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.589 -0.014 0.017 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.393 0.003 0.011 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.589 0.014 0.017 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.784 0.031 0.023 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.589 0.054 0.017 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.395 0.072 0.012 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.201 0.083 0.006 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.007 0.09 0.001 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.187 0.091 -0.004 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.381 0.087 -0.009 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.184 0.077 -0.003 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.012 0.074 0.003 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.182 0.077 -0.002 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.014 0.075 0.004 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.18 0.079 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.374 0.078 -0.006 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.178 0.072 -0.0 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.372 0.072 -0.006 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.176 0.066 0.001 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.02 0.067 0.007 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 0.174 0.074 0.002 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.368 0.075 -0.004 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.562 0.071 -0.009 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.366 0.062 -0.003 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.17 0.059 0.003 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.026 0.063 0.01 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.222 0.072 0.016 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.418 0.088 0.022 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.614 0.11 0.028 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.811 0.139 0.035 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.617 0.174 0.03 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.814 0.204 0.037 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -1.011 0.241 0.044 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[11,  3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 131 lasted for 40 time steps with total reward of 39.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.227 -0.047 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 -0.422 -0.04 0.012 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.226 -0.028 0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.421 -0.023 0.012 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.225 -0.011 0.006 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.03 -0.006 -0.0 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.225 -0.006 0.005 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 -0.42 -0.001 0.011 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.615 0.011 0.017 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.81 0.028 0.023 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -1.006 0.051 0.029 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -1.202 0.08 0.035 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.113 -1.398 0.115 0.042 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -1.204 0.157 0.036 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.165 -1.4 0.193 0.043 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.193 -1.597 0.236 0.05 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 132 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.181 0.031 0.006 output tensor([[12,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.376 0.036 0.012 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.572 0.048 0.018 output tensor([[12,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.768 0.065 0.024 output tensor([[19,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.964 0.089 0.03 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -1.16 0.119 0.036 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.966 0.156 0.031 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.773 0.187 0.027 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.97 0.214 0.034 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[13,  4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 133 lasted for 10 time steps with total reward of 9.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.226 -0.03 0.005 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.03 -0.025 -0.001 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.225 -0.025 0.005 output tensor([[7, 7]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.42 -0.02 0.011 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.614 -0.009 0.017 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.419 0.007 0.011 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.224 0.018 0.005 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.42 0.023 0.011 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.615 0.033 0.017 output tensor([[10,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.42 0.05 0.011 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.226 0.061 0.006 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.422 0.067 0.012 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -0.228 0.079 0.006 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.424 0.085 0.013 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.13 -0.23 0.098 0.007 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -0.036 0.105 0.002 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.135 0.157 0.107 -0.003 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 0.35 0.104 -0.008 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.125 0.154 0.096 -0.002 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -0.042 0.095 0.005 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.239 0.1 0.011 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.045 0.111 0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.242 0.117 0.013 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.048 0.129 0.007 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 0.145 0.137 0.003 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -0.052 0.139 0.009 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.132 -0.249 0.149 0.016 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.056 0.164 0.011 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 0.136 0.175 0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.061 0.182 0.013 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.137 -0.258 0.195 0.02 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -0.066 0.215 0.015 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 134 lasted for 33 time steps with total reward of 32.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 0.151 0.012 -0.005 output tensor([[ 2, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.346 0.007 -0.011 output tensor([[ 1, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.541 -0.004 -0.017 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.346 -0.021 -0.011 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.542 -0.032 -0.017 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.737 -0.048 -0.023 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.543 -0.071 -0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.349 -0.088 -0.012 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.545 -0.1 -0.018 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.741 -0.119 -0.025 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.938 -0.143 -0.031 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 1.134 -0.175 -0.038 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 1.331 -0.213 -0.045 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 135 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.193 -0.016 0.006 output tensor([[21,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.003 -0.01 0.0 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.198 -0.01 -0.006 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.393 -0.015 -0.012 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.589 -0.027 -0.018 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.784 -0.045 -0.024 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.98 -0.068 -0.03 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 1.176 -0.098 -0.036 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.982 -0.134 -0.031 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.11 0.789 -0.165 -0.026 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.125 0.596 -0.191 -0.021 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 0.404 -0.212 -0.017 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 136 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.205 -0.006 -0.005 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.01 -0.011 0.001 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.185 -0.01 0.007 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.38 -0.003 0.012 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.575 0.009 0.018 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.77 0.027 0.024 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.966 0.052 0.03 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -1.162 0.082 0.036 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -1.357 0.118 0.043 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -1.164 0.161 0.038 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.115 -0.971 0.199 0.033 output tensor([[3, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -1.167 0.232 0.04 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 137 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.23 0.015 -0.006 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.035 0.009 0.0 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.16 0.01 0.006 output tensor([[11,  6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 -0.355 0.016 0.012 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.551 0.028 0.018 output tensor([[15,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.746 0.046 0.024 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.552 0.07 0.019 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.748 0.089 0.025 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.944 0.114 0.031 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -1.14 0.145 0.038 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -1.337 0.183 0.044 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.079 -1.533 0.227 0.051 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[2, 7]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 138 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.212 0.047 0.005 output tensor([[9, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.018 0.052 -0.0 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 0.177 0.052 -0.006 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.371 0.046 -0.011 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.175 0.035 -0.005 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.37 0.03 -0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.565 0.02 -0.016 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.76 0.003 -0.022 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.955 -0.019 -0.028 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 1.15 -0.046 -0.034 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.955 -0.08 -0.028 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 1.151 -0.109 -0.035 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.136 1.348 -0.143 -0.041 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.163 1.154 -0.184 -0.036 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.186 1.351 -0.22 -0.043 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 139 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.173 -0.047 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.369 -0.054 -0.013 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.175 -0.066 -0.007 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.02 -0.073 -0.002 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.176 -0.075 -0.008 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.373 -0.083 -0.014 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.569 -0.097 -0.021 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.375 -0.117 -0.015 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.572 -0.133 -0.022 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.768 -0.155 -0.028 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.965 -0.183 -0.035 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 1.161 -0.218 -0.042 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 140 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.171 -0.02 -0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.024 -0.025 0.0 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.218 -0.025 0.006 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.413 -0.019 0.012 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.608 -0.007 0.017 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.803 0.011 0.023 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.608 0.034 0.018 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.804 0.051 0.024 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.999 0.075 0.03 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.805 0.105 0.024 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.116 -1.001 0.129 0.031 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -1.198 0.16 0.037 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.16 -1.005 0.197 0.033 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.18 -0.813 0.23 0.028 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 141 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.181 0.043 0.007 output tensor([[15,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.377 0.05 0.013 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.183 0.063 0.008 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.378 0.071 0.014 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.574 0.084 0.02 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.381 0.104 0.015 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.577 0.119 0.021 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.773 0.14 0.028 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.97 0.168 0.034 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -0.777 0.203 0.03 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.974 0.232 0.037 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 142 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.234 0.033 0.006 output tensor([[20,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.429 0.039 0.012 output tensor([[13,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.625 0.051 0.018 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.821 0.069 0.024 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -1.017 0.093 0.03 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.823 0.123 0.025 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.629 0.149 0.02 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.437 0.169 0.015 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.095 -0.634 0.184 0.022 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.831 0.206 0.029 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -1.028 0.235 0.036 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 143 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.182 0.033 0.005 output tensor([[21,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.013 0.038 -0.0 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.207 0.038 -0.006 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.012 0.032 0.0 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.184 0.032 0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.38 0.038 0.012 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.575 0.05 0.018 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.771 0.068 0.024 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.967 0.092 0.031 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.773 0.123 0.025 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.58 0.148 0.02 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.776 0.169 0.027 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.139 -0.973 0.196 0.034 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.159 -0.781 0.23 0.029 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[4, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 144 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.156 0.011 0.005 output tensor([[23,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.352 0.016 0.011 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.547 0.027 0.017 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.352 0.044 0.011 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.158 0.056 0.006 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 0.036 0.061 0.0 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 0.231 0.062 -0.005 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 0.425 0.057 -0.011 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 0.619 0.046 -0.016 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.423 0.03 -0.01 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.618 0.02 -0.016 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.813 0.005 -0.021 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 1.008 -0.017 -0.027 output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.813 -0.044 -0.021 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 1.009 -0.065 -0.027 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.815 -0.093 -0.022 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.621 -0.115 -0.017 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.817 -0.132 -0.023 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 0.624 -0.155 -0.018 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.107 0.821 -0.173 -0.025 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.123 1.018 -0.198 -0.032 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.144 0.825 -0.23 -0.027 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 145 lasted for 23 time steps with total reward of 22.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.179 0.032 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.375 0.038 0.012 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.57 0.049 0.018 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.766 0.067 0.024 output tensor([[18,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.962 0.091 0.03 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.768 0.121 0.025 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.575 0.145 0.02 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.771 0.165 0.026 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -0.968 0.191 0.033 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -1.165 0.225 0.04 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 146 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.188 -0.024 0.007 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.383 -0.018 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.577 -0.006 0.018 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.772 0.012 0.024 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.968 0.036 0.03 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -1.163 0.066 0.036 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -1.359 0.102 0.042 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -1.555 0.144 0.049 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -1.361 0.192 0.044 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.192 -1.169 0.236 0.039 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[9, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 147 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.165 0.008 -0.005 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.03 0.003 0.001 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.165 0.004 -0.005 output tensor([[ 4, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.36 -0.001 -0.011 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.555 -0.012 -0.017 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.36 -0.028 -0.011 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.166 -0.039 -0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.029 -0.044 0.001 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 -0.223 -0.043 0.006 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.418 -0.037 0.012 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.612 -0.026 0.017 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.807 -0.008 0.023 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -1.002 0.015 0.029 output tensor([[12,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -1.198 0.043 0.035 output tensor([[12,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -1.393 0.078 0.041 output tensor([[21,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -1.589 0.119 0.047 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.131 -1.395 0.166 0.042 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.159 -1.591 0.208 0.049 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.191 -1.788 0.257 0.056 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[12,  3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 148 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.178 0.007 0.006 output tensor([[18,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.373 0.013 0.012 output tensor([[19,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.178 0.026 0.007 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.017 0.032 0.001 output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.179 0.033 0.007 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.375 0.04 0.013 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.57 0.053 0.019 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.766 0.072 0.025 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.091 -0.962 0.097 0.032 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.111 -1.158 0.129 0.038 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -1.354 0.167 0.045 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -1.161 0.211 0.04 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 149 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.232 0.049 0.007 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 -0.038 0.056 0.002 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.234 0.057 0.008 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.429 0.065 0.014 output tensor([[20,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.625 0.079 0.02 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.821 0.099 0.027 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.628 0.125 0.021 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.434 0.147 0.016 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.631 0.163 0.023 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.439 0.186 0.018 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.636 0.204 0.025 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.833 0.229 0.032 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 150 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.198 0.015 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.003 0.021 0.0 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.192 0.021 -0.006 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.004 0.016 0.0 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.191 0.016 -0.005 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.004 0.011 0.001 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 -0.2 0.012 0.007 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.395 0.018 0.013 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.2 0.031 0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.005 0.038 0.001 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.189 0.039 -0.004 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.006 0.034 0.002 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.202 0.036 0.008 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.398 0.044 0.014 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 -0.593 0.057 0.02 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.399 0.077 0.014 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.595 0.092 0.021 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.401 0.112 0.015 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.208 0.128 0.01 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.015 0.138 0.005 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.212 0.143 0.012 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.408 0.155 0.019 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.605 0.174 0.025 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.802 0.2 0.032 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.999 0.232 0.039 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 151 lasted for 26 time steps with total reward of 25.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.235 -0.044 0.006 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.43 -0.039 0.011 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.234 -0.027 0.005 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.038 -0.022 -0.001 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.157 -0.022 -0.007 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.038 -0.029 -0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.232 -0.03 0.005 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -0.037 -0.025 -0.001 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.232 -0.026 0.004 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.036 -0.022 -0.002 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.159 -0.023 -0.008 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.355 -0.031 -0.014 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.55 -0.045 -0.02 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.356 -0.064 -0.014 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.162 -0.078 -0.009 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.032 -0.087 -0.003 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.226 -0.09 0.002 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.42 -0.088 0.007 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.224 -0.081 0.001 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.027 -0.08 -0.005 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.221 -0.086 -0.0 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.415 -0.086 0.005 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.609 -0.081 0.01 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.803 -0.07 0.016 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -0.997 -0.054 0.021 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.801 -0.033 0.015 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -0.606 -0.018 0.009 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.41 -0.009 0.003 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -0.605 -0.006 0.009 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 -0.41 0.002 0.003 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.147 -0.605 0.005 0.009 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.159 -0.8 0.014 0.015 output tensor([[16,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.175 -0.996 0.028 0.021 output tensor([[22,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.195 -1.191 0.049 0.027 output tensor([[21,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.219 -1.387 0.076 0.033 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.246 -1.583 0.108 0.039 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.278 -1.389 0.147 0.034 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.306 -1.585 0.181 0.041 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.337 -1.782 0.222 0.047 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[13,  3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 152 lasted for 40 time steps with total reward of 39.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.212 0.031 0.007 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.018 0.038 0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.213 0.039 0.007 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.409 0.046 0.013 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.215 0.059 0.008 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.41 0.067 0.014 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -0.606 0.081 0.02 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.803 0.101 0.027 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.108 -0.609 0.128 0.021 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.12 -0.416 0.149 0.016 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.612 0.165 0.023 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -0.42 0.188 0.018 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.149 -0.617 0.207 0.025 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.161 -0.425 0.232 0.021 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 153 lasted for 15 time steps with total reward of 14.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.192 -0.032 0.006 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.386 -0.026 0.011 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.581 -0.015 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.776 0.002 0.023 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.581 0.025 0.017 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.386 0.042 0.011 output tensor([[ 3, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.192 0.053 0.006 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 0.003 0.059 0.0 output tensor([[4, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 0.197 0.059 -0.005 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 0.391 0.054 -0.011 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.585 0.044 -0.016 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.78 0.027 -0.022 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.975 0.006 -0.027 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.001 0.779 -0.022 -0.022 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.585 -0.043 -0.016 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.39 -0.059 -0.01 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.196 -0.069 -0.005 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.002 -0.074 0.001 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.198 -0.074 -0.006 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.394 -0.079 -0.012 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.2 -0.091 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.396 -0.098 -0.013 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.203 -0.111 -0.008 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.399 -0.119 -0.014 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.596 -0.133 -0.021 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.403 -0.154 -0.016 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.599 -0.17 -0.023 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.407 -0.193 -0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.215 -0.211 -0.013 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 154 lasted for 30 time steps with total reward of 29.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.172 -0.034 -0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.367 -0.04 -0.012 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.563 -0.053 -0.018 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.759 -0.071 -0.025 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.565 -0.096 -0.019 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.761 -0.115 -0.026 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 0.957 -0.141 -0.032 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 1.154 -0.173 -0.039 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.145 1.35 -0.212 -0.046 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 155 lasted for 10 time steps with total reward of 9.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.219 -0.01 0.006 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.414 -0.004 0.012 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.219 0.008 0.006 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.414 0.014 0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.61 0.026 0.018 output tensor([[19,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.415 0.044 0.012 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.22 0.056 0.007 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.026 0.063 0.001 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.222 0.064 0.007 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.418 0.072 0.014 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.614 0.085 0.02 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.42 0.105 0.015 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.227 0.12 0.009 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.423 0.129 0.016 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.23 0.145 0.011 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.427 0.157 0.018 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.624 0.174 0.025 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -0.431 0.199 0.02 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.628 0.219 0.027 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 156 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.233 0.033 0.007 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.429 0.039 0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.624 0.052 0.019 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.43 0.071 0.013 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.236 0.084 0.008 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.042 0.092 0.003 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.238 0.095 0.009 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.045 0.104 0.004 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.149 0.108 -0.001 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.048 0.106 0.005 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.244 0.112 0.012 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.441 0.123 0.018 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.637 0.141 0.025 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.834 0.166 0.031 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.641 0.197 0.027 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.449 0.224 0.022 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 157 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 0.19 0.024 -0.005 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.385 0.019 -0.011 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.19 0.009 -0.005 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.006 0.004 0.001 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.201 0.005 0.007 output tensor([[12,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.396 0.012 0.013 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.591 0.025 0.019 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.787 0.044 0.025 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.982 0.069 0.031 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -1.178 0.1 0.037 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -1.374 0.137 0.044 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -1.57 0.181 0.05 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.117 -1.766 0.231 0.057 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[17,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 158 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.184 -0.047 -0.005 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.38 -0.053 -0.012 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.575 -0.064 -0.018 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.381 -0.082 -0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.187 -0.095 -0.007 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.384 -0.102 -0.013 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.19 -0.115 -0.008 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 0.387 -0.123 -0.015 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.193 -0.138 -0.01 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.39 -0.148 -0.016 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.197 -0.164 -0.012 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.005 -0.176 -0.007 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 -0.187 -0.183 -0.002 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.01 -0.185 -0.009 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.207 -0.194 -0.016 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.015 -0.21 -0.011 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 159 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.158 -0.006 0.006 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.353 -0.0 0.012 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.158 0.012 0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.353 0.018 0.012 output tensor([[23,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.158 0.03 0.006 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.037 0.036 0.001 output tensor([[ 1, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.231 0.037 -0.005 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 0.426 0.031 -0.011 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.621 0.021 -0.016 output tensor([[ 2, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.815 0.004 -0.022 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.62 -0.018 -0.016 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.425 -0.034 -0.01 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 0.231 -0.044 -0.005 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.036 -0.049 0.001 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.232 -0.048 -0.005 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.038 -0.054 0.0 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.234 -0.054 -0.006 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 0.039 -0.06 -0.001 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 -0.155 -0.06 0.005 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 -0.349 -0.055 0.01 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 -0.544 -0.045 0.016 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 -0.738 -0.029 0.021 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.933 -0.008 0.027 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.738 0.019 0.021 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.543 0.041 0.015 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.738 0.056 0.022 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.544 0.078 0.016 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.74 0.094 0.022 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.936 0.116 0.029 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -1.133 0.145 0.035 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.076 -0.939 0.18 0.03 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.747 0.211 0.026 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 160 lasted for 33 time steps with total reward of 32.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 0.211 0.017 -0.005 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.016 0.012 0.001 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.211 0.012 -0.005 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.406 0.007 -0.011 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.601 -0.004 -0.017 output tensor([[ 1, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 0.406 -0.021 -0.011 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.211 -0.032 -0.005 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.016 -0.037 0.0 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.212 -0.036 -0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.407 -0.042 -0.012 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.603 -0.054 -0.018 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.409 -0.072 -0.012 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.215 -0.084 -0.007 output tensor([[2, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.021 -0.091 -0.002 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.217 -0.092 -0.008 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.413 -0.1 -0.014 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.22 -0.115 -0.009 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.026 -0.124 -0.004 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.223 -0.128 -0.011 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.42 -0.139 -0.017 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 0.616 -0.156 -0.024 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.098 0.424 -0.18 -0.019 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.107 0.621 -0.199 -0.026 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.119 0.429 -0.226 -0.022 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 161 lasted for 25 time steps with total reward of 24.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.24 0.038 -0.005 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.045 0.033 0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.151 0.033 0.007 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.044 0.04 0.001 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.238 0.042 -0.004 output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.433 0.037 -0.01 output tensor([[ 3, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.627 0.027 -0.016 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.432 0.012 -0.009 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.237 0.002 -0.004 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.042 -0.001 0.002 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 -0.154 0.001 0.008 output tensor([[11,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 -0.349 0.009 0.014 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 -0.154 0.023 0.008 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.041 0.032 0.003 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.236 0.034 -0.003 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.04 0.031 0.003 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.235 0.034 -0.003 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.039 0.031 0.003 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 -0.156 0.035 0.009 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.038 0.044 0.004 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.233 0.048 -0.002 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.427 0.046 -0.007 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.622 0.038 -0.013 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.092 0.816 0.026 -0.019 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.621 0.007 -0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 0.425 -0.005 -0.007 output tensor([[18,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.129 0.23 -0.012 -0.001 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.035 -0.013 0.005 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.231 -0.008 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.139 0.036 -0.009 0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.14 -0.159 -0.004 0.011 output tensor([[23,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 -0.354 0.007 0.016 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 -0.549 0.023 0.022 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.119 -0.355 0.045 0.017 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.111 -0.16 0.062 0.011 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.034 0.073 0.006 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 0.228 0.079 0.0 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.422 0.079 -0.005 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 0.616 0.074 -0.01 output tensor([[ 2, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.81 0.064 -0.016 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.15 0.614 0.048 -0.01 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.163 0.808 0.038 -0.015 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.179 0.613 0.023 -0.009 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.191 0.807 0.014 -0.015 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.207 1.002 -0.0 -0.02 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.227 1.198 -0.021 -0.026 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.251 1.393 -0.047 -0.032 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.279 1.589 -0.079 -0.038 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.311 1.394 -0.118 -0.033 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.339 1.201 -0.151 -0.028 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.363 1.397 -0.179 -0.035 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.391 1.205 -0.214 -0.03 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 162 lasted for 53 time steps with total reward of 52.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.226 0.034 0.006 output tensor([[15,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.032 0.041 0.001 output tensor([[ 2, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.227 0.041 0.007 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.423 0.048 0.013 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.228 0.06 0.007 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.034 0.068 0.002 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.16 0.069 -0.004 output tensor([[ 1, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.354 0.066 -0.009 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.548 0.057 -0.015 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 0.352 0.042 -0.008 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.156 0.034 -0.002 output tensor([[13,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 0.351 0.032 -0.008 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.546 0.024 -0.013 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.741 0.01 -0.019 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.936 -0.009 -0.025 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 1.131 -0.034 -0.031 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.936 -0.065 -0.025 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 1.132 -0.09 -0.031 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.12 0.938 -0.122 -0.026 output tensor([[3, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.138 1.134 -0.148 -0.033 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.161 0.941 -0.181 -0.028 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.18 1.138 -0.209 -0.035 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.203 0.946 -0.243 -0.03 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 163 lasted for 24 time steps with total reward of 23.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 -0.188 -0.008 0.006 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 -0.383 -0.002 0.012 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.578 0.01 0.018 output tensor([[24,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.383 0.028 0.012 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.578 0.04 0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.774 0.058 0.024 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.97 0.082 0.03 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -1.166 0.112 0.037 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.972 0.149 0.032 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -1.169 0.181 0.038 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -1.365 0.219 0.045 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[15,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 164 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.214 -0.033 -0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.02 -0.038 0.0 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.175 -0.038 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 0.021 -0.033 -0.0 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 0.216 -0.033 -0.006 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.022 -0.039 -0.001 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.217 -0.04 -0.007 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.413 -0.047 -0.013 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 0.219 -0.06 -0.007 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.415 -0.068 -0.014 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.611 -0.081 -0.02 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.807 -0.101 -0.026 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.613 -0.128 -0.021 output tensor([[5, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.809 -0.149 -0.028 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 0.617 -0.176 -0.023 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.058 0.424 -0.199 -0.018 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.232 -0.218 -0.014 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[13,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 165 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.155 0.04 -0.006 output tensor([[ 6, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.349 0.034 -0.011 output tensor([[ 4, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.544 0.023 -0.017 output tensor([[ 4, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.739 0.006 -0.023 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 0.934 -0.016 -0.028 output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.086 1.129 -0.045 -0.034 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 0.934 -0.079 -0.029 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 0.74 -0.108 -0.023 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.142 0.937 -0.131 -0.03 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.16 1.133 -0.161 -0.036 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.183 1.33 -0.198 -0.043 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.21 1.526 -0.241 -0.05 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 166 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.214 0.034 0.006 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 -0.41 0.04 0.012 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.605 0.052 0.018 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 -0.801 0.07 0.024 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.997 0.095 0.031 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 -1.193 0.125 0.037 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -1.0 0.163 0.032 output tensor([[0, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.08 -1.196 0.195 0.039 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -1.393 0.233 0.046 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[6, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 167 lasted for 10 time steps with total reward of 9.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.153 -0.031 0.007 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.348 -0.024 0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.152 -0.012 0.006 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 -0.347 -0.006 0.012 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.152 0.006 0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.043 0.012 0.0 output tensor([[ 2, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.238 0.012 -0.006 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 0.042 0.007 0.0 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.153 0.007 0.006 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 0.042 0.014 0.0 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.237 0.014 -0.005 output tensor([[ 3, 21]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 0.042 0.009 0.001 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.012 0.237 0.01 -0.005 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.432 0.004 -0.011 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.627 -0.006 -0.017 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.822 -0.023 -0.023 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.627 -0.046 -0.017 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.823 -0.063 -0.023 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 1.019 -0.086 -0.029 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.079 1.215 -0.115 -0.036 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 1.021 -0.151 -0.031 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.124 1.218 -0.181 -0.037 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.148 1.025 -0.219 -0.033 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[18,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 168 lasted for 24 time steps with total reward of 23.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.165 0.001 -0.006 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 0.36 -0.006 -0.012 output tensor([[ 2, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.555 -0.018 -0.018 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.751 -0.036 -0.024 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.556 -0.06 -0.018 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.362 -0.078 -0.013 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.168 -0.091 -0.008 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.364 -0.098 -0.014 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.171 -0.112 -0.009 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.023 -0.121 -0.004 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.216 -0.125 0.001 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.019 -0.123 -0.005 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.177 -0.128 -0.012 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.374 -0.14 -0.018 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.181 -0.158 -0.013 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.012 -0.172 -0.009 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.185 -0.18 -0.015 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.007 -0.196 -0.011 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.199 -0.206 -0.006 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.39 -0.213 -0.002 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 169 lasted for 21 time steps with total reward of 20.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 -0.216 0.009 0.006 output tensor([[18,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.021 0.015 0.0 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.216 0.016 0.006 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 -0.412 0.022 0.012 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.217 0.034 0.007 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.022 0.041 0.001 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.172 0.041 -0.005 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.367 0.037 -0.01 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.562 0.026 -0.016 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.756 0.011 -0.022 output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.951 -0.011 -0.027 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 1.147 -0.038 -0.033 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 1.342 -0.072 -0.039 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 1.538 -0.111 -0.046 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 1.344 -0.157 -0.041 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.117 1.151 -0.197 -0.036 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.14 1.347 -0.233 -0.043 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 170 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.185 -0.017 0.005 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.38 -0.012 0.011 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.575 -0.002 0.016 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.77 0.015 0.022 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.966 0.037 0.028 output tensor([[16,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -1.161 0.065 0.034 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.967 0.1 0.029 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -1.163 0.129 0.035 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -0.97 0.164 0.03 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.777 0.194 0.026 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.585 0.22 0.021 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 171 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.196 0.044 -0.006 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 0.391 0.038 -0.011 output tensor([[ 2, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.585 0.027 -0.017 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.78 0.01 -0.023 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.975 -0.013 -0.028 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 1.17 -0.041 -0.034 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.976 -0.076 -0.029 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 1.172 -0.105 -0.035 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.154 0.978 -0.14 -0.03 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.174 0.785 -0.17 -0.025 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.19 0.982 -0.195 -0.032 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.209 1.179 -0.227 -0.039 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 172 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.175 -0.046 0.006 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.369 -0.04 0.011 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.564 -0.029 0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.759 -0.012 0.023 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.075 -0.954 0.01 0.028 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.759 0.039 0.023 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.954 0.061 0.029 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -1.15 0.09 0.035 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -1.346 0.125 0.041 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.178 -1.542 0.166 0.048 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.209 -1.738 0.214 0.055 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[14,  3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 173 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.16 0.002 0.007 output tensor([[13,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.035 0.009 0.001 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.161 0.01 0.007 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.356 0.016 0.013 output tensor([[10,  6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.551 0.029 0.019 output tensor([[18,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.747 0.047 0.025 output tensor([[23,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.552 0.072 0.019 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.748 0.091 0.025 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.944 0.116 0.032 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -1.141 0.148 0.038 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -1.337 0.186 0.045 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.087 -1.144 0.231 0.04 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 5]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 174 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.178 -0.046 0.006 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.372 -0.04 0.012 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.177 -0.028 0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.019 -0.023 -0.001 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.214 -0.023 -0.007 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.41 -0.03 -0.013 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 0.215 -0.042 -0.007 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.02 -0.049 -0.001 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 -0.174 -0.051 0.004 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 -0.368 -0.046 0.01 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.173 -0.037 0.004 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.023 -0.033 -0.002 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.219 -0.036 -0.009 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.024 -0.044 -0.003 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.17 -0.047 0.003 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.025 -0.044 -0.003 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.169 -0.048 0.002 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.027 -0.046 -0.004 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.222 -0.05 -0.01 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.418 -0.06 -0.016 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.224 -0.076 -0.011 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.42 -0.087 -0.017 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.616 -0.105 -0.024 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.423 -0.128 -0.018 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.084 0.229 -0.147 -0.013 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.426 -0.16 -0.02 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.097 0.234 -0.18 -0.015 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 0.041 -0.196 -0.011 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.102 -0.151 -0.206 -0.006 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 -0.342 -0.212 -0.002 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 175 lasted for 31 time steps with total reward of 30.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 -0.236 -0.013 0.005 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.015 -0.431 -0.007 0.011 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.626 0.004 0.017 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.431 0.021 0.011 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 -0.236 0.032 0.005 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.041 0.038 -0.0 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 0.153 0.038 -0.006 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.348 0.032 -0.011 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.542 0.02 -0.017 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.347 0.003 -0.011 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 0.152 -0.008 -0.005 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.043 -0.013 0.001 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.238 -0.012 0.006 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.433 -0.006 0.012 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.238 0.006 0.006 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.043 0.013 0.0 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 0.152 0.013 -0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.347 0.008 -0.011 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 0.152 -0.003 -0.005 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.043 -0.008 0.001 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.238 -0.008 0.006 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.433 -0.001 0.012 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.628 0.011 0.018 output tensor([[13,  6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.824 0.029 0.024 output tensor([[14,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -1.019 0.053 0.03 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -1.215 0.083 0.036 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -1.021 0.119 0.031 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.827 0.15 0.026 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -1.024 0.176 0.033 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -1.221 0.209 0.039 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -1.028 0.248 0.035 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 176 lasted for 32 time steps with total reward of 31.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.241 -0.0 0.007 output tensor([[11,  6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.437 0.007 0.013 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.632 0.02 0.019 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.437 0.038 0.013 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.633 0.051 0.019 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.089 -0.828 0.07 0.025 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -1.024 0.095 0.031 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.126 -1.22 0.126 0.038 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.151 -1.027 0.164 0.033 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.171 -0.834 0.197 0.028 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.188 -1.031 0.225 0.035 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 177 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.216 0.02 0.005 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.411 0.025 0.011 output tensor([[17,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.606 0.036 0.017 output tensor([[15,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 -0.412 0.053 0.012 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.608 0.065 0.018 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.804 0.082 0.024 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -1.0 0.106 0.03 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.806 0.137 0.025 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -1.002 0.162 0.032 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.11 -1.199 0.193 0.039 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.134 -1.396 0.232 0.045 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 178 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.229 0.049 0.005 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.035 0.054 -0.0 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.231 0.054 0.006 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.036 0.06 0.001 output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.232 0.061 0.007 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.428 0.068 0.013 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -0.234 0.081 0.008 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.43 0.088 0.014 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.626 0.102 0.02 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.823 0.122 0.027 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.106 -1.019 0.149 0.033 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.826 0.182 0.028 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -1.023 0.211 0.035 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 179 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.234 -0.027 0.005 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.039 -0.023 -0.001 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 0.156 -0.024 -0.007 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.038 -0.031 -0.002 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.233 -0.033 0.004 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.037 -0.029 -0.002 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.158 -0.031 -0.008 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.037 -0.039 -0.002 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.231 -0.041 0.003 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.426 -0.038 0.009 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.62 -0.029 0.014 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.077 -0.815 -0.015 0.02 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.093 -1.01 0.005 0.026 output tensor([[18,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -1.205 0.031 0.032 output tensor([[23,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 -1.4 0.063 0.038 output tensor([[8, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.166 -1.596 0.101 0.044 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.198 -1.402 0.145 0.039 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.226 -1.599 0.184 0.045 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.258 -1.795 0.229 0.052 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[20,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 180 lasted for 20 time steps with total reward of 19.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.166 0.029 0.007 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.361 0.035 0.013 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.557 0.048 0.019 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.362 0.067 0.013 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.168 0.081 0.008 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.026 0.089 0.003 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 0.22 0.091 -0.003 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.413 0.089 -0.008 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.607 0.081 -0.013 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.411 0.068 -0.007 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.605 0.061 -0.012 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.799 0.049 -0.018 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.603 0.031 -0.012 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.798 0.02 -0.017 output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.603 0.002 -0.011 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.408 -0.009 -0.005 output tensor([[18,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.213 -0.014 0.0 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.018 -0.014 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 -0.177 -0.007 0.012 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 -0.372 0.005 0.018 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 -0.177 0.022 0.012 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 -0.373 0.034 0.018 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 -0.178 0.052 0.012 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.016 0.065 0.007 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.21 0.072 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.405 0.073 -0.004 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 0.599 0.069 -0.009 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 0.402 0.06 -0.003 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 0.207 0.057 0.003 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 0.401 0.06 -0.002 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.113 0.205 0.057 0.004 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.117 0.399 0.061 -0.002 output tensor([[ 3, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.125 0.203 0.06 0.005 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 0.007 0.064 0.011 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 0.201 0.075 0.005 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 0.005 0.08 0.012 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 -0.191 0.092 0.018 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 0.003 0.11 0.013 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 0.196 0.123 0.008 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 -0.0 0.131 0.014 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.134 -0.197 0.145 0.021 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 -0.004 0.166 0.016 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 -0.201 0.182 0.023 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 -0.009 0.205 0.018 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 -0.206 0.223 0.025 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[16,  3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 181 lasted for 46 time steps with total reward of 45.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.146 0.021 0.006 output tensor([[21,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.049 0.026 0.0 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.244 0.026 -0.006 output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.048 0.021 0.0 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.243 0.021 -0.005 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.048 0.016 0.001 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.148 0.016 0.007 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.343 0.023 0.013 output tensor([[13,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 -0.538 0.035 0.019 output tensor([[21,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.734 0.054 0.025 output tensor([[16,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 -0.93 0.078 0.031 output tensor([[9, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -1.126 0.109 0.037 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.932 0.146 0.032 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -1.129 0.178 0.039 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -1.325 0.217 0.045 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 182 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.02 0.178 0.011 -0.006 output tensor([[ 3, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.023 0.373 0.005 -0.012 output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.568 -0.007 -0.018 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.763 -0.025 -0.024 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 0.568 -0.049 -0.018 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.374 -0.067 -0.012 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.57 -0.079 -0.019 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.766 -0.098 -0.025 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.103 0.572 -0.123 -0.02 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.114 0.379 -0.143 -0.015 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 0.186 -0.158 -0.01 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 0.383 -0.168 -0.017 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.133 0.19 -0.184 -0.012 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 -0.002 -0.196 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 0.195 -0.204 -0.014 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.141 0.393 -0.218 -0.021 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 183 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.047 -0.221 -0.042 0.005 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.416 -0.037 0.01 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 -0.22 -0.027 0.004 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.025 -0.023 -0.002 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 -0.22 -0.024 0.004 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.024 -0.02 -0.002 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.171 -0.022 -0.008 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.367 -0.03 -0.014 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.562 -0.044 -0.02 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.758 -0.064 -0.026 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.564 -0.09 -0.021 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.76 -0.111 -0.027 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.088 0.956 -0.138 -0.034 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.107 1.153 -0.171 -0.04 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.13 0.96 -0.211 -0.035 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[11,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 184 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.163 0.039 0.006 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.032 0.045 0.001 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.164 0.046 0.007 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.36 0.053 0.013 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.556 0.066 0.019 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.752 0.085 0.026 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 -0.948 0.111 0.032 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -1.144 0.143 0.038 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.951 0.181 0.033 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -0.758 0.214 0.029 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 185 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 -0.159 -0.007 0.005 output tensor([[11,  6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.036 -0.002 -0.001 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 -0.159 -0.003 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.036 0.002 -0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.231 0.001 -0.007 output tensor([[ 2, 19]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.427 -0.006 -0.013 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.232 -0.019 -0.007 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.037 -0.026 -0.001 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.158 -0.027 0.005 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.037 -0.023 -0.002 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.157 -0.024 0.004 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.352 -0.02 0.01 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.157 -0.01 0.004 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.039 -0.006 -0.002 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 -0.157 -0.008 0.004 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 0.039 -0.004 -0.002 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.234 -0.006 -0.008 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.429 -0.014 -0.014 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.624 -0.028 -0.02 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.052 0.43 -0.048 -0.014 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.235 -0.062 -0.009 output tensor([[7, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.041 -0.071 -0.003 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 -0.153 -0.074 0.002 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.043 -0.072 -0.004 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 -0.151 -0.076 0.001 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.045 -0.074 -0.005 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 -0.149 -0.079 0.0 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.047 -0.079 -0.006 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.06 0.244 -0.085 -0.012 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.05 -0.097 -0.007 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 -0.144 -0.104 -0.002 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 -0.337 -0.106 0.003 output tensor([[6, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 -0.141 -0.102 -0.003 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.334 -0.105 0.002 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 -0.138 -0.103 -0.004 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 -0.331 -0.108 0.001 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.525 -0.107 0.006 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 -0.718 -0.101 0.011 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.522 -0.09 0.005 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.326 -0.085 -0.002 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.129 -0.087 -0.008 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.067 -0.095 -0.015 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.006 -0.127 -0.11 -0.009 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 0.07 -0.119 -0.016 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 -0.124 -0.135 -0.011 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.01 -0.317 -0.146 -0.006 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.509 -0.151 -0.001 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.702 -0.152 0.004 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 -0.505 -0.148 -0.003 output tensor([[0, 2]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.308 -0.151 -0.01 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.501 -0.161 -0.005 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.694 -0.166 0.0 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.496 -0.166 -0.007 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.689 -0.172 -0.002 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.492 -0.174 -0.009 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.114 -0.684 -0.183 -0.004 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -0.876 -0.187 0.0 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -1.068 -0.187 0.005 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -1.26 -0.182 0.01 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.192 -1.452 -0.173 0.014 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.221 -1.255 -0.158 0.007 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.246 -1.448 -0.151 0.012 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.275 -1.251 -0.139 0.005 output tensor([[5, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.3 -1.054 -0.134 -0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.321 -0.857 -0.135 -0.008 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.338 -0.661 -0.143 -0.015 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.351 -0.854 -0.158 -0.01 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.368 -0.657 -0.167 -0.016 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.382 -0.849 -0.184 -0.012 output tensor([[7, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.399 -1.041 -0.196 -0.007 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.419 -1.233 -0.203 -0.003 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.444 -1.425 -0.205 0.002 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.473 -1.616 -0.204 0.006 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.505 -1.808 -0.197 0.011 output tensor([[14,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.541 -1.611 -0.187 0.004 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.573 -1.803 -0.183 0.008 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.609 -1.995 -0.175 0.013 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.649 -1.798 -0.162 0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.685 -1.99 -0.156 0.011 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.725 -1.794 -0.145 0.004 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.761 -1.986 -0.141 0.009 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.801 -2.179 -0.132 0.014 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.844 -2.372 -0.118 0.019 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.892 -2.176 -0.1 0.012 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.935 -2.369 -0.087 0.017 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.983 -2.173 -0.07 0.011 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.026 -1.977 -0.059 0.005 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.066 -2.171 -0.054 0.01 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.109 -1.976 -0.044 0.004 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.149 -1.78 -0.04 -0.002 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.184 -1.584 -0.042 -0.008 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.216 -1.779 -0.05 -0.003 output tensor([[3, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.251 -1.583 -0.053 -0.009 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.283 -1.387 -0.061 -0.015 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.311 -1.581 -0.076 -0.009 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.342 -1.775 -0.086 -0.004 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.378 -1.969 -0.09 0.001 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.417 -1.773 -0.089 -0.005 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.453 -1.576 -0.094 -0.012 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -1.484 -1.38 -0.105 -0.018 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "\\Successful episode!\n",
            "\n",
            "Episode 186 lasted for 101 time steps with total reward of 101.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.045 -0.163 -0.003 0.006 output tensor([[8, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.359 0.003 0.012 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.055 -0.554 0.015 0.018 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.359 0.033 0.012 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.554 0.045 0.018 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.75 0.063 0.024 output tensor([[16,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.946 0.087 0.03 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.118 -0.752 0.117 0.025 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.948 0.142 0.032 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.152 -0.755 0.174 0.027 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.167 -0.563 0.2 0.022 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.179 -0.371 0.222 0.017 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 3, 12]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 187 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.167 -0.015 0.005 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.361 -0.01 0.011 output tensor([[8, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.556 0.001 0.017 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.752 0.017 0.022 output tensor([[17,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 -0.947 0.04 0.028 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -1.142 0.068 0.034 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -1.338 0.102 0.041 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -1.534 0.143 0.047 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.128 -1.73 0.19 0.054 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.163 -1.537 0.244 0.049 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 188 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.173 -0.015 0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.022 -0.01 -0.001 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.173 -0.011 0.005 output tensor([[14,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.023 -0.006 -0.001 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.052 -0.172 -0.008 0.005 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.056 -0.367 -0.003 0.01 output tensor([[17,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.172 0.007 0.004 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.067 -0.367 0.012 0.01 output tensor([[18,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.563 0.022 0.016 output tensor([[14,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.085 -0.758 0.038 0.022 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.1 -0.563 0.061 0.017 output tensor([[ 1, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.112 -0.759 0.077 0.023 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -0.565 0.1 0.018 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.138 -0.762 0.118 0.024 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.153 -0.958 0.142 0.031 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -1.155 0.172 0.037 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.196 -0.962 0.21 0.033 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 189 lasted for 18 time steps with total reward of 17.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 0.206 -0.05 -0.007 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 0.402 -0.057 -0.013 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.598 -0.07 -0.019 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.015 0.794 -0.089 -0.025 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.6 -0.114 -0.02 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 0.407 -0.135 -0.015 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.603 -0.15 -0.022 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.411 -0.171 -0.017 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.218 -0.188 -0.012 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.046 0.415 -0.201 -0.019 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.613 -0.22 -0.026 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 190 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.013 -0.241 0.033 0.006 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.047 0.039 0.0 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.148 0.039 -0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.343 0.034 -0.011 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 0.537 0.023 -0.017 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.342 0.006 -0.011 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 0.147 -0.004 -0.005 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 -0.049 -0.009 0.001 output tensor([[8, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.147 -0.008 -0.005 output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.048 -0.013 0.001 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.147 -0.012 -0.005 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.042 0.342 -0.017 -0.011 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.538 -0.027 -0.017 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.059 0.343 -0.044 -0.011 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.148 -0.055 -0.006 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 -0.046 -0.061 -0.0 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 -0.24 -0.061 0.005 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 -0.044 -0.055 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.152 -0.056 -0.007 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.348 -0.063 -0.013 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.073 0.544 -0.076 -0.019 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.084 0.35 -0.096 -0.014 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.091 0.156 -0.11 -0.009 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.352 -0.119 -0.015 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 0.549 -0.134 -0.022 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.356 -0.156 -0.017 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.119 0.163 -0.173 -0.012 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.122 -0.029 -0.185 -0.007 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 -0.221 -0.193 -0.003 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.117 -0.024 -0.196 -0.01 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.117 0.173 -0.205 -0.017 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.12 -0.019 -0.222 -0.012 output tensor([[16,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 191 lasted for 33 time steps with total reward of 32.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.238 -0.003 0.006 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 -0.433 0.002 0.012 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.628 0.014 0.018 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.824 0.032 0.023 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -1.019 0.055 0.03 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.825 0.085 0.024 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -1.021 0.109 0.03 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.09 -0.827 0.139 0.025 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -1.024 0.164 0.032 output tensor([[4, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.127 -1.22 0.196 0.039 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.152 -1.417 0.235 0.046 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[17,  0]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 192 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.164 -0.003 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 -0.359 0.003 0.011 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.033 -0.554 0.014 0.017 output tensor([[16,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 -0.75 0.031 0.023 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.007 -0.555 0.055 0.018 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.004 -0.751 0.072 0.024 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.02 -0.557 0.096 0.018 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.753 0.114 0.025 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.559 0.139 0.02 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.756 0.159 0.026 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.563 0.185 0.022 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.76 0.207 0.028 output tensor([[11,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.098 -0.568 0.235 0.024 output tensor([[5, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[10,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 193 lasted for 14 time steps with total reward of 13.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.218 0.034 -0.005 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 0.413 0.028 -0.011 output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.218 0.017 -0.005 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.028 0.412 0.012 -0.011 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.036 0.217 0.001 -0.005 output tensor([[6, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.022 -0.004 0.001 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 -0.173 -0.002 0.007 output tensor([[14,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.022 0.004 0.001 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.038 0.217 0.005 -0.005 output tensor([[ 5, 22]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.412 0.001 -0.011 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 0.217 -0.01 -0.005 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.055 0.412 -0.015 -0.011 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.063 0.608 -0.026 -0.017 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.413 -0.042 -0.011 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.084 0.609 -0.053 -0.017 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.096 0.804 -0.07 -0.023 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.112 0.61 -0.094 -0.018 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.124 0.417 -0.112 -0.013 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.133 0.223 -0.124 -0.008 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.137 0.42 -0.132 -0.014 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.145 0.227 -0.146 -0.009 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.15 0.034 -0.155 -0.004 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.151 -0.159 -0.159 0.001 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.147 0.038 -0.159 -0.006 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.148 -0.154 -0.165 -0.001 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.145 -0.347 -0.166 0.003 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.138 -0.539 -0.163 0.008 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.127 -0.342 -0.155 0.001 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.121 -0.535 -0.154 0.006 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.11 -0.727 -0.148 0.011 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 -0.53 -0.137 0.004 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.085 -0.334 -0.133 -0.003 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 -0.527 -0.135 0.002 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 -0.72 -0.133 0.007 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.913 -0.126 0.012 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.035 -0.716 -0.113 0.006 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.519 -0.108 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.01 -0.323 -0.108 -0.007 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.004 -0.126 -0.116 -0.014 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 -0.32 -0.13 -0.009 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.123 -0.138 -0.015 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 -0.316 -0.154 -0.01 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 -0.119 -0.164 -0.017 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.016 -0.312 -0.181 -0.012 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.023 -0.504 -0.193 -0.008 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.696 -0.201 -0.003 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.498 -0.204 -0.01 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.69 -0.215 -0.006 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 194 lasted for 49 time steps with total reward of 48.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.024 -0.157 0.046 0.007 output tensor([[9, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 0.038 0.053 0.001 output tensor([[ 0, 23]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.158 0.054 0.007 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.354 0.061 0.014 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 -0.55 0.075 0.02 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.746 0.095 0.026 output tensor([[12,  4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.063 -0.942 0.121 0.032 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.082 -1.139 0.153 0.039 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.105 -0.945 0.192 0.034 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -1.142 0.226 0.041 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 195 lasted for 11 time steps with total reward of 10.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.041 -0.147 -0.016 0.006 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.342 -0.01 0.012 output tensor([[12,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.147 0.002 0.006 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.048 0.008 0.0 output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 0.243 0.008 -0.006 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 0.048 0.002 0.0 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.147 0.003 0.006 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.342 0.009 0.012 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.057 -0.147 0.021 0.006 output tensor([[0, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 0.048 0.027 0.0 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.148 0.028 0.007 output tensor([[13,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.062 -0.343 0.034 0.013 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.069 -0.149 0.047 0.007 output tensor([[3, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 0.046 0.054 0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.071 -0.15 0.055 0.008 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.346 0.062 0.014 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.081 -0.542 0.076 0.02 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.348 0.096 0.015 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.099 -0.544 0.111 0.021 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.109 -0.741 0.132 0.028 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.124 -0.937 0.159 0.034 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.143 -0.744 0.193 0.029 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -0.941 0.223 0.036 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[14,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 196 lasted for 24 time steps with total reward of 23.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 0.148 0.025 -0.005 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.027 0.343 0.02 -0.011 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.147 0.009 -0.005 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 0.342 0.004 -0.01 output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.537 -0.006 -0.016 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.342 -0.022 -0.01 output tensor([[10,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.147 -0.033 -0.005 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 0.343 -0.037 -0.011 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.071 0.539 -0.048 -0.017 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.082 0.344 -0.065 -0.011 output tensor([[4, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 0.54 -0.077 -0.018 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.099 0.346 -0.094 -0.012 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.152 -0.106 -0.007 output tensor([[9, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.109 -0.041 -0.113 -0.002 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.108 -0.234 -0.115 0.003 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.104 -0.428 -0.112 0.008 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.095 -0.621 -0.104 0.013 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 -0.425 -0.09 0.007 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.074 -0.228 -0.083 0.001 output tensor([[2, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.07 -0.032 -0.083 -0.006 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 0.164 -0.089 -0.012 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.072 0.36 -0.101 -0.019 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.08 0.167 -0.119 -0.013 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.363 -0.133 -0.02 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.17 -0.153 -0.015 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 -0.023 -0.167 -0.01 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.093 -0.215 -0.178 -0.005 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.089 -0.407 -0.183 -0.001 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.081 -0.599 -0.184 0.004 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 -0.402 -0.18 -0.003 output tensor([[1, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 -0.205 -0.183 -0.01 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.008 -0.193 -0.017 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.189 -0.21 -0.024 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 5]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 197 lasted for 34 time steps with total reward of 33.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.213 0.034 -0.006 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.007 0.408 0.028 -0.012 output tensor([[ 3, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.001 0.212 0.016 -0.006 output tensor([[13,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.005 0.017 0.01 -0.0 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.006 -0.178 0.01 0.006 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.002 -0.374 0.016 0.012 output tensor([[9, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.005 -0.179 0.027 0.006 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.009 -0.374 0.033 0.012 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.017 -0.57 0.045 0.018 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.028 -0.375 0.063 0.013 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.181 0.076 0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.377 0.083 0.013 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.183 0.096 0.008 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.01 0.104 0.003 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 -0.186 0.107 0.009 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.007 0.117 0.004 output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.201 0.121 -0.001 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.004 0.12 0.006 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.197 0.126 0.001 output tensor([[1, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.0 0.126 0.007 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 0.194 0.133 0.002 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.003 0.136 0.009 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.042 -0.2 0.145 0.015 output tensor([[7, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.046 -0.397 0.16 0.022 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.594 0.182 0.029 output tensor([[15,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.066 -0.79 0.211 0.036 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 198 lasted for 27 time steps with total reward of 26.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.222 0.029 0.006 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.053 -0.028 0.034 0.0 output tensor([[ 2, 18]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 0.167 0.034 -0.006 output tensor([[ 1, 16]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.05 0.362 0.029 -0.011 output tensor([[ 4, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.556 0.018 -0.017 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.032 0.361 0.001 -0.011 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.025 0.166 -0.01 -0.005 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.021 -0.029 -0.015 0.001 output tensor([[17,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 -0.224 -0.014 0.007 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.419 -0.008 0.012 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.224 0.005 0.006 output tensor([[6, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.039 -0.029 0.011 0.001 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.04 0.166 0.012 -0.005 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.037 0.361 0.006 -0.011 output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.029 0.556 -0.005 -0.017 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.018 0.361 -0.021 -0.011 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 0.166 -0.032 -0.005 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.008 0.362 -0.038 -0.011 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.0 0.558 -0.049 -0.017 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.011 0.363 -0.066 -0.012 output tensor([[7, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.018 0.169 -0.078 -0.006 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.365 -0.085 -0.013 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.171 -0.097 -0.007 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.368 -0.105 -0.014 output tensor([[4, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.04 0.174 -0.119 -0.009 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.371 -0.127 -0.015 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.178 -0.143 -0.01 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.374 -0.153 -0.017 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.061 0.182 -0.17 -0.012 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 -0.011 -0.182 -0.007 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.186 -0.19 -0.014 output tensor([[1, 2]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.069 -0.006 -0.204 -0.01 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.068 0.192 -0.214 -0.017 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 199 lasted for 34 time steps with total reward of 33.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.025 -0.205 -0.043 0.005 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.01 -0.038 -0.001 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 -0.204 -0.038 0.005 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.017 -0.399 -0.034 0.01 output tensor([[12,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.009 -0.593 -0.023 0.016 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.003 -0.788 -0.007 0.022 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.019 -0.593 0.014 0.016 output tensor([[1, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.788 0.03 0.022 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.594 0.052 0.016 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.058 -0.789 0.068 0.022 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.074 -0.595 0.09 0.017 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.791 0.107 0.023 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.102 -0.988 0.131 0.03 output tensor([[9, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.122 -1.184 0.16 0.036 output tensor([[15,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -1.381 0.197 0.043 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.173 -1.188 0.24 0.039 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 200 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "\n",
            " Model saved!\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 -0.233 0.007 0.006 output tensor([[19,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -0.428 0.012 0.012 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.233 0.024 0.006 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.051 -0.428 0.03 0.012 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.06 -0.234 0.041 0.006 output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.065 -0.429 0.048 0.012 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.625 0.06 0.018 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.086 -0.431 0.078 0.013 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.094 -0.627 0.091 0.019 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.107 -0.823 0.11 0.026 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.123 -0.63 0.136 0.02 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.136 -0.436 0.156 0.016 output tensor([[1, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.145 -0.244 0.172 0.011 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.149 -0.441 0.183 0.018 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.158 -0.638 0.2 0.024 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.171 -0.835 0.225 0.031 output tensor([[13,  5]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[1, 6]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 201 lasted for 17 time steps with total reward of 16.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.165 -0.015 -0.006 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.36 -0.021 -0.011 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.556 -0.032 -0.017 output tensor([[ 2, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.041 0.751 -0.05 -0.024 output tensor([[3, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.056 0.947 -0.073 -0.03 output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.075 0.753 -0.103 -0.024 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.559 -0.127 -0.019 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.101 0.756 -0.146 -0.026 output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.116 0.952 -0.172 -0.032 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.135 1.149 -0.204 -0.039 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.158 1.346 -0.244 -0.046 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[6, 6]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 202 lasted for 12 time steps with total reward of 11.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.019 0.162 0.033 -0.007 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.357 0.026 -0.012 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.029 0.161 0.014 -0.006 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 0.356 0.008 -0.012 output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.551 -0.004 -0.018 output tensor([[3, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.746 -0.022 -0.024 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.065 0.551 -0.046 -0.018 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.076 0.357 -0.064 -0.012 output tensor([[9, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.083 0.163 -0.076 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.087 0.359 -0.083 -0.013 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.094 0.555 -0.096 -0.02 output tensor([[1, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.105 0.751 -0.116 -0.026 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.12 0.948 -0.142 -0.033 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.139 1.144 -0.174 -0.039 output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.162 0.951 -0.214 -0.035 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[3, 3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 203 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.22 0.008 -0.006 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.03 0.415 0.002 -0.012 output tensor([[ 1, 17]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.22 -0.01 -0.006 output tensor([[14,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 0.025 -0.016 -0.0 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.22 -0.016 -0.006 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.025 -0.022 -0.0 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.221 -0.022 -0.006 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 0.026 -0.029 -0.001 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.053 -0.169 -0.029 0.005 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.363 -0.024 0.011 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.043 -0.558 -0.014 0.016 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.032 -0.363 0.003 0.01 output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.168 0.013 0.005 output tensor([[2, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.021 0.027 0.018 -0.001 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.022 0.222 0.016 -0.007 output tensor([[ 2, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.027 0.009 -0.001 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.026 0.222 0.008 -0.007 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 0.417 0.002 -0.013 output tensor([[4, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.039 0.222 -0.011 -0.007 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.027 -0.018 -0.001 output tensor([[4, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.044 0.222 -0.019 -0.007 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.027 -0.025 -0.001 output tensor([[7, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.049 0.223 -0.027 -0.007 output tensor([[0, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.054 0.418 -0.034 -0.013 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.223 -0.047 -0.008 output tensor([[8, 0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.066 0.029 -0.055 -0.002 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.067 -0.165 -0.057 0.003 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.064 -0.36 -0.053 0.009 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.057 -0.554 -0.044 0.014 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.748 -0.03 0.02 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.031 -0.943 -0.01 0.026 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -1.138 0.016 0.032 output tensor([[6, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -1.333 0.048 0.037 output tensor([[6, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.038 -1.529 0.085 0.044 output tensor([[20,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -1.725 0.129 0.05 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.103 -1.921 0.179 0.057 output tensor([[15,  0]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.141 -2.117 0.235 0.063 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[12,  3]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 204 lasted for 38 time steps with total reward of 37.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 0.192 -0.039 -0.006 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.002 -0.045 0.0 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.012 -0.197 -0.045 0.006 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.008 -0.391 -0.039 0.011 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.0 -0.586 -0.028 0.017 output tensor([[2, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.011 -0.78 -0.011 0.022 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.027 -0.975 0.011 0.028 output tensor([[6, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -1.171 0.04 0.034 output tensor([[9, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.07 -1.366 0.074 0.04 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.097 -1.562 0.114 0.047 output tensor([[8, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.129 -1.758 0.161 0.053 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.164 -1.564 0.214 0.048 output tensor([[2, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 205 lasted for 13 time steps with total reward of 12.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.033 0.163 0.022 -0.006 output tensor([[ 4, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.03 -0.032 0.016 -0.0 output tensor([[5, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 -0.228 0.016 0.006 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.423 0.022 0.012 output tensor([[10,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.228 0.033 0.006 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.048 -0.034 0.039 0.0 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 -0.229 0.039 0.006 output tensor([[13,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.035 0.046 0.001 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.054 -0.23 0.046 0.007 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.059 -0.426 0.053 0.013 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.068 -0.232 0.066 0.008 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.072 -0.038 0.074 0.002 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.073 -0.234 0.076 0.008 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.04 0.084 0.003 output tensor([[2, 3]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.078 -0.236 0.087 0.009 output tensor([[3, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.083 -0.432 0.097 0.016 output tensor([[11,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.092 -0.629 0.112 0.022 output tensor([[7, 6]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.104 -0.825 0.135 0.029 output tensor([[5, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.121 -0.632 0.163 0.024 output tensor([[3, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.133 -0.439 0.187 0.019 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.142 -0.636 0.206 0.026 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.155 -0.833 0.232 0.033 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 206 lasted for 23 time steps with total reward of 22.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.049 0.218 0.05 -0.005 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 0.022 0.045 0.001 output tensor([[5, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.044 -0.173 0.047 0.008 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.021 0.054 0.002 output tensor([[5, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 0.215 0.056 -0.003 output tensor([[ 3, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.043 0.41 0.053 -0.009 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.034 0.604 0.044 -0.014 output tensor([[ 2, 15]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.408 0.029 -0.008 output tensor([[6, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.014 0.603 0.021 -0.014 output tensor([[ 2, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.002 0.798 0.007 -0.02 output tensor([[1, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.014 0.993 -0.013 -0.026 output tensor([[2, 9]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.034 0.798 -0.038 -0.02 output tensor([[12,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 0.603 -0.058 -0.014 output tensor([[4, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.062 0.799 -0.072 -0.02 output tensor([[3, 4]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.078 0.605 -0.093 -0.015 output tensor([[6, 4]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.09 0.801 -0.108 -0.021 output tensor([[0, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.106 0.608 -0.129 -0.016 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.118 0.415 -0.146 -0.011 output tensor([[3, 3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.126 0.222 -0.157 -0.006 output tensor([[4, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.131 0.419 -0.163 -0.013 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.139 0.616 -0.176 -0.02 output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.151 0.813 -0.196 -0.027 output tensor([[1, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.168 1.01 -0.223 -0.034 output tensor([[4, 6]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 207 lasted for 24 time steps with total reward of 23.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.048 0.154 -0.004 -0.005 output tensor([[ 2, 10]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.051 -0.041 -0.009 0.001 output tensor([[11,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.05 -0.236 -0.008 0.007 output tensor([[8, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.045 -0.431 -0.001 0.013 output tensor([[7, 2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.037 -0.626 0.012 0.018 output tensor([[11,  3]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.024 -0.431 0.03 0.013 output tensor([[2, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.016 -0.627 0.043 0.019 output tensor([[10,  2]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): 0.003 -0.823 0.061 0.025 output tensor([[5, 5]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.013 -0.628 0.086 0.019 output tensor([[ 2, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.026 -0.434 0.105 0.014 output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.035 -0.631 0.119 0.021 output tensor([[5, 1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.047 -0.827 0.14 0.027 output tensor([[10,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.064 -1.024 0.167 0.034 output tensor([[12,  1]]) selected_action: tensor([0]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.084 -0.831 0.201 0.029 output tensor([[0, 7]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.101 -0.639 0.23 0.025 output tensor([[4, 5]]) selected_action: tensor([1]) rew: 1.0\n",
            "output tensor([[12,  2]]) selected_action: tensor([0]) rew: 0.0\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 208 lasted for 16 time steps with total reward of 15.0\n",
            "\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.036 0.242 0.009 -0.006 output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.031 0.437 0.004 -0.012 output tensor([[2, 8]]) selected_action: tensor([1]) rew: 1.0\n",
            "state (x, x_dot, theta (rad), theta_dot): -0.022 0.632 -0.008 -0.017 output tensor([[ 3, 13]]) selected_action: tensor([1]) rew: 1.0\n"
          ]
        }
      ],
      "source": [
        "manual_seed(SEED)\n",
        "\n",
        "if WANDB:\n",
        "    wandb.init(project='BioLCNet', entity='singularbrain', config=network_hparams)\n",
        "net = LCNet(time,**network_hparams, reward_fn=RLTasks('CartPole-v0'), wandb_active = WANDB)\n",
        "net.learn(**network_hparams)\n",
        "plot_locally_connected_weights(net.connections[('input','main1')].w, net.n_channels1, net.filter_size1,\n",
        "                                                compute_size(net.crop_size, net.filter_size1, net.stride1), net.connections[('input','main1')].locations,\n",
        "                                                net.crop_size ** 2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_locally_connected_weights(net.connections[('input','main1')].w, net.n_channels1, net.filter_size1,\n",
        "                                                compute_size(net.crop_size, net.filter_size1, net.stride1), net.connections[('input','main1')].locations,\n",
        "                                                net.crop_size ** 2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KmwqostVRPJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gym Playground"
      ],
      "metadata": {
        "id": "9M7NvCJlr0E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "intensity = 400\n",
        "crop_size = 90\n",
        "time = 100+300+50\n",
        "def frame_process(x):\n",
        "        x[x<0.99] = 2.0\n",
        "        x[x<=1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "screen = env.render(mode='rgb_array')\n",
        "screen = screen.transpose((2, 0, 1))\n",
        "_, screen_height, screen_width = screen.shape\n",
        "screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "view_width = int(screen_width * 0.6)\n",
        "world_width = env.x_threshold * 2\n",
        "scale = screen_width / world_width\n",
        "cart_location = int(env.state[0] * scale + screen_width / 2.0)\n",
        "if cart_location < view_width // 2:\n",
        "    slice_range = slice(view_width)\n",
        "elif cart_location > (screen_width - view_width // 2):\n",
        "    slice_range = slice(-view_width, None)\n",
        "else:\n",
        "    slice_range = slice(cart_location - view_width // 2,\n",
        "                        cart_location + view_width // 2)\n",
        "# Strip off the edges, so that we have a square image centered on a cart\n",
        "screen = screen[:, :, slice_range]\n",
        "# Convert to float, rescale, convert to torch tensor\n",
        "screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "screen = torch.from_numpy(screen)\n",
        "print(screen.shape)\n",
        "plt.imshow(screen.cpu().permute(\n",
        "        1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "h = screen.shape[1]\n",
        "w = screen.shape[2]\n",
        "print(h*0.75)\n",
        "resize = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize([80, 180]),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "            #transforms.Resize([80, 80]),\n",
        "            #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "            # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "            #transforms.CenterCrop((crop_size, crop_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "            transforms.Lambda(frame_process),\n",
        "            #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "            transforms.Lambda(lambda x: x * intensity),\n",
        "            transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "device = 'cuda'\n",
        "\n",
        "# Resize, and add a batch dimension (BCHW)\n",
        "screen = resize(screen)\n",
        "# print(screen.shape)\n",
        "print(int(h*0.8), int(w*0.8))\n",
        "print(screen.shape)\n",
        "# screen = screen.to(device)\n",
        "# plt.imshow(screen.cpu().permute(\n",
        "#         1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "\n",
        "# print(a.shape)\n",
        "a = screen.sum(axis=0)\n",
        "\n",
        "a = a.to(device)\n",
        "plt.imshow(a.cpu().numpy().squeeze(), cmap='gray')\n",
        "obs, reward, done, info = env.step(0)\n",
        "obs, reward, done, info "
      ],
      "metadata": {
        "id": "JDFJB9GeldHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done, _ = env.step(0)   \n",
        "obs"
      ],
      "metadata": {
        "id": "9VmVGl3wPRBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zt5WXE2TgMzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(1)"
      ],
      "metadata": {
        "id": "fmN6aPHyidbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, x_dot, theta, theta_dot = env.state\n",
        "x, x_dot, theta, theta_dot"
      ],
      "metadata": {
        "id": "1MAMB52EiOMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.theta_threshold_radians"
      ],
      "metadata": {
        "id": "K83DiktzRLGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BindsNet Breakout"
      ],
      "metadata": {
        "id": "9jFzOzYIFdGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/BindsNET/bindsnet.git\n"
      ],
      "metadata": {
        "id": "9Qi4v29mDF3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ],
      "metadata": {
        "id": "6ROgQ2ZWA-VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install matplotlib==2.1.1\n",
        "\n",
        "### Restart your runtime after this"
      ],
      "metadata": {
        "id": "_K7CuZmqFTny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "wONJ1L1uEGMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bindsnet.encoding import bernoulli\n",
        "from bindsnet.environment import GymEnvironment\n",
        "from bindsnet.learning import MSTDP\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes\n",
        "from bindsnet.network.topology import Connection\n",
        "from bindsnet.pipeline import EnvironmentPipeline\n",
        "from bindsnet.pipeline.action import select_softmax\n",
        "import time\n",
        "\n",
        "# Build network.\n",
        "network = Network(dt=1.0)\n",
        "\n",
        "# Layers of neurons.\n",
        "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
        "middle = LIFNodes(n=100, traces=True)\n",
        "out = LIFNodes(n=4, refrac=0, traces=True)\n",
        "\n",
        "# Connections between layers.\n",
        "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
        "middle_out = Connection(\n",
        "    source=middle,\n",
        "    target=out,\n",
        "    wmin=0,\n",
        "    wmax=1,\n",
        "    update_rule=MSTDP,\n",
        "    nu=1e-1,\n",
        "    norm=0.5 * middle.n,\n",
        ")\n",
        "\n",
        "# Add all layers and connections to the network.\n",
        "network.add_layer(inpt, name=\"Input Layer\")\n",
        "network.add_layer(middle, name=\"Hidden Layer\")\n",
        "network.add_layer(out, name=\"Output Layer\")\n",
        "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
        "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
        "\n",
        "# Load the Breakout environment.\n",
        "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
        "environment.reset()\n",
        "\n",
        "# Build pipeline from specified components.\n",
        "environment_pipeline = EnvironmentPipeline(\n",
        "    network,\n",
        "    environment,\n",
        "    encoding=bernoulli,\n",
        "    action_function=select_softmax,\n",
        "    output=\"Output Layer\",\n",
        "    time=100,\n",
        "    history_length=1,\n",
        "    delta=1,\n",
        "    plot_interval=None,\n",
        "    render_interval=None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(pipeline, episode_count):\n",
        "    for i in range(episode_count):\n",
        "        total_reward = 0\n",
        "        pipeline.reset_state_variables()\n",
        "        is_done = False\n",
        "        render_counter = 0\n",
        "        while not is_done:\n",
        "            # if render_counter % 10 == 0:\n",
        "            #     # plt.title(\"Game image\")\n",
        "            #     # plt.imshow(pipeline.env.render())\n",
        "            #     # plt.show()\n",
        "            #     a = pipeline.env.render()\n",
        "            #     print(a)\n",
        "            #     time.sleep(0.1)\n",
        "                \n",
        "            render_counter += 1\n",
        "            result = pipeline.env_step()\n",
        "            pipeline.step(result)\n",
        "\n",
        "            reward = result[1]\n",
        "            total_reward += reward\n",
        "            \n",
        "            is_done = result[2]\n",
        "        print(f\"Episode {i} total reward:{total_reward}\")\n",
        "    pipeline.env.close()\n",
        "\n",
        "print(\"Training: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)\n",
        "\n",
        "# stop MSTDP\n",
        "environment_pipeline.network.learning = False\n",
        "\n",
        "print(\"Testing: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)"
      ],
      "metadata": {
        "id": "y_jx13I5-VTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PLVXCK0Djg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7fTSvrK3T_GA",
        "ULGGHW43UksI",
        "MBKedMpIleMr",
        "8clxN_npa1WY",
        "sCXSLZZoGS4z"
      ],
      "machine_shape": "hm",
      "name": "BioLCNet_CartPole.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "87ae7d1e75b14a98f2d7b99b6b39b40721989d38e6517f9dbec64ca4d8e3011b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69c8e26557ec4ff1bad490a064e5b7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7b670731fdad4787bc2030b316daaf72",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0109f360f404fb39b4e4cbdb66c416a",
              "IPY_MODEL_9b0581c2937a42489254106d72918e8b",
              "IPY_MODEL_5ca6a3795181445cbd5690b3afe65f72"
            ]
          }
        },
        "7b670731fdad4787bc2030b316daaf72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0109f360f404fb39b4e4cbdb66c416a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c91e809d389341229fb005b933649c4d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Episode: 208, Number of steps: 16, Episode Total Reward: 15.00:   2%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ecd142a68eb4dd983fadd2d53bb4ffe"
          }
        },
        "9b0581c2937a42489254106d72918e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7397cf9abc7b45bc95107fccee11d9b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3235166c5d3e45d78da4d67dca961897"
          }
        },
        "5ca6a3795181445cbd5690b3afe65f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6612ea4b33504514bbc1e60141101757",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 208/10000 [43:59&lt;34:37:31, 12.73s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_734471b7292747999ca38a7f929332df"
          }
        },
        "c91e809d389341229fb005b933649c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ecd142a68eb4dd983fadd2d53bb4ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7397cf9abc7b45bc95107fccee11d9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3235166c5d3e45d78da4d67dca961897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6612ea4b33504514bbc1e60141101757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "734471b7292747999ca38a7f929332df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}