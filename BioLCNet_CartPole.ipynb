{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/DeepBioLCNet/blob/main/BioLCNet_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTSvrK3T_GA"
      },
      "source": [
        "#Notebook setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lXtgP_iEPE0G"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KXcXvvsXcOlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40ad48b-6129-47c7-f2a9-5725f07d56c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepBioLCNet' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "0QFC6xL2uAaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aaa967a-be92-4def-9e3b-c972137d684e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fb06f612ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K4l3AVRbGS4Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.image import AxesImage\n",
        "from torch.nn.modules.utils import _pair\n",
        "from matplotlib.collections import PathCollection\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from typing import Tuple, List, Optional, Sized, Dict, Union\n",
        "import math\n",
        "import random\n",
        "from torchvision.transforms.functional import crop\n",
        "# from ..utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights\n",
        "\n",
        "import gym\n",
        "import tkinter\n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BFGNAecpT-Lj"
      },
      "outputs": [],
      "source": [
        "from bindsnet.network.nodes import Nodes\n",
        "import os\n",
        "### import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, Tuple, Optional, Sequence\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "from bindsnet.datasets import MNIST\n",
        "from bindsnet.encoding import PoissonEncoder\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes, IFNodes\n",
        "from bindsnet.network.topology import Connection, MaxPool2dLocalConnection\n",
        "from bindsnet.network.topology import LocalConnection, LocalConnectionOrig\n",
        "from bindsnet.network.monitors import Monitor, AbstractMonitor, TensorBoardMonitor\n",
        "from bindsnet.learning import PostPre, MSTDP, MSTDPET, WeightDependentPostPre, Hebbian\n",
        "from bindsnet.learning.reward import DynamicDopamineInjection, DopaminergicRPE, AbstractReward\n",
        "from bindsnet.analysis.plotting import plot_locally_connected_weights,plot_locally_connected_weights_meh,plot_spikes,\\\n",
        "plot_LC_timepoint_spikes,plot_locally_connected_weights_meh2,plot_convergence_and_histogram,plot_locally_connected_weights_meh3\n",
        "from bindsnet.analysis.visualization import plot_weights_movie, plot_spike_trains_for_example,summary, plot_voltage\n",
        "from bindsnet.utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RLTasks(AbstractReward):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Computes the reward for a given RL task in the current state\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_id,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if env_id == \"CartPole-v0\":\n",
        "            self.compute = self._cartPole_compute\n",
        "        elif env_id == \"MountainCar-v0-v0\":\n",
        "            self.compute = self._mountainCar_compute\n",
        "        elif env_id == \"BreakoutDeterministic-v4\":\n",
        "            self.compute = self._breakout_compute\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"This rl environment is not currently supported.\"\n",
        "            )\n",
        "    \n",
        "    def _cartPole_compute(self, **kwargs):\n",
        "        success = kwargs['success']\n",
        "        failure = kwargs['failure']\n",
        "        if kwargs['classic_reward']:\n",
        "            if failure:\n",
        "                return torch.tensor([0.])\n",
        "            else:\n",
        "                return torch.tensor([1.])\n",
        "\n",
        "        env = kwargs['env']\n",
        "        state = env.state\n",
        "        success = kwargs['success']\n",
        "        failure = kwargs['failure']\n",
        "        x, x_dot, theta, theta_dot = state\n",
        "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "        r2 = ((env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5)#*2\n",
        "        reward = r1 + r2\n",
        "        if success:\n",
        "            reward += 20\n",
        "        elif failure:\n",
        "            reward -= 20\n",
        "        reward = torch.tensor([reward])\n",
        "        return reward\n",
        "\n",
        "    def _mountainCar_compute(self):\n",
        "        pass\n",
        "\n",
        "    def _breakout_compute(self):\n",
        "        pass\n",
        "        \n",
        "    def update(self):\n",
        "        pass\n",
        "\n",
        "    def online_compute(self,):\n",
        "        pass"
      ],
      "metadata": {
        "id": "WqFD8yro6WCb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGGHW43UksI"
      },
      "source": [
        "## Sets up Gpu use and manual seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LiUmFrpcUfmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f58758-5fdf-4f54-e73a-ee30aaefd44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Device =  cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device =  torch.device(\"cuda\")\n",
        "    gpu = True\n",
        "else:\n",
        "    device =  torch.device(\"cpu\")\n",
        "    gpu = False\n",
        "\n",
        "def manual_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SEED = 2045 # The Singularity is Near!\n",
        "manual_seed(SEED)\n",
        "WANDB = False\n",
        "\n",
        "torch.set_num_threads(os.cpu_count() - 1)\n",
        "print(\"Running on Device = \", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKedMpIleMr"
      },
      "source": [
        "# Custom Monitors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqpsr2a1WV"
      },
      "source": [
        "## Reward Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M44GJ65GleMs"
      },
      "outputs": [],
      "source": [
        "class RewardMonitor(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: None,\n",
        "        batch_size: int = 1,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.time = time\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # if time is not specified the monitor variable accumulate the logs\n",
        "        if self.time is None:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        self.recording = []\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if \"reward\" in kwargs:\n",
        "            self.recording.append(kwargs[\"reward\"])\n",
        "        # remove the oldest element (first in the list)\n",
        "        # if self.time is not None:\n",
        "        #     self.recording.pop(0)\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clxN_npa1WY"
      },
      "source": [
        "## Plot Eligibility trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SshGlRwpa1WZ"
      },
      "outputs": [],
      "source": [
        "class PlotET(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        i,\n",
        "        j,\n",
        "        source,\n",
        "        target,\n",
        "        connection,\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.connection = connection\n",
        "\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if hasattr(self.connection.update_rule, 'p_plus'):\n",
        "            self.recording['spikes_i'].append(self.source.s.ravel()[self.i].item())\n",
        "            self.recording['spikes_j'].append(self.target.s.ravel()[self.j].item())\n",
        "            self.recording['p_plus'].append(self.connection.update_rule.p_plus[self.i].item())\n",
        "            self.recording['p_minus'].append(self.connection.update_rule.p_minus[self.j].item())\n",
        "            self.recording['eligibility'].append(self.connection.update_rule.eligibility[self.i,self.j].item())\n",
        "            self.recording['eligibility_trace'].append(self.connection.update_rule.eligibility_trace[self.i,self.j].item())\n",
        "            self.recording['w'].append(self.connection.w[self.i,self.j].item())\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        fig, axs  = plt.subplots(7)\n",
        "        fig.set_size_inches(10, 20)\n",
        "        for i, (name, p) in enumerate(self.recording.items()):\n",
        "            axs[i].plot(p[-250:])\n",
        "            axs[i].set_title(name)\n",
        "    \n",
        "        fig.show()\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = {\n",
        "        'spikes_i': [],\n",
        "        'spikes_j': [],\n",
        "        'p_plus':[],\n",
        "        'p_minus':[],\n",
        "        'eligibility':[],\n",
        "        'eligibility_trace':[],\n",
        "        'w': [],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YGE1XjvIkZ"
      },
      "source": [
        "## Kernel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-4hp2V46vOUv"
      },
      "outputs": [],
      "source": [
        "class AbstractKernel(ABC):\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Base class for generating image filter kernels such as Gabor, DoG, etc. Each subclass should override :attr:`__call__` function.\n",
        "        Instantiates a ``Filter Kernel`` object.\n",
        "        :param window_size : The size of the kernel (int)\n",
        "        \"\"\"\n",
        "        self.window_size = kernel_size\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PL2L6_ABwBH4"
      },
      "outputs": [],
      "source": [
        "class DoGKernel(AbstractKernel):\n",
        "    def __init__(self, kernel_size: Union[int, Tuple[int, int]], sigma1 : float, sigma2 : float):\n",
        "        \"\"\"\n",
        "        Generates DoG filter kernels.\n",
        "        :param kernel_size: Horizontal and vertical size of DOG kernels.(If pass int, we consider it as a square filter) \n",
        "        :param sigma1 : The sigma parameter for the first Gaussian function.\n",
        "        :param sigma2 : The sigma parameter for the second Gaussian function.\n",
        "        \"\"\"\n",
        "        super(DoGKernel, self).__init__(kernel_size)\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        \n",
        "    def __call__(self):\n",
        "        k = self.window_size//2\n",
        "        x, y = np.mgrid[-k:k+1:1, -k:k+1:1]\n",
        "        a = 1.0 / (2 * math.pi)\n",
        "        prod = x*x + y*y\n",
        "        f1 = (1/(self.sigma1*self.sigma1)) * np.exp(-0.5 * (1/(self.sigma1*self.sigma1)) * (prod))\n",
        "        f2 = (1/(self.sigma2*self.sigma2)) * np.exp(-0.5 * (1/(self.sigma2*self.sigma2)) * (prod))\n",
        "        dog = a * (f1-f2)\n",
        "        dog_mean = np.mean(dog)\n",
        "        dog = dog - dog_mean\n",
        "        dog_max = np.max(dog)\n",
        "        dog = dog / dog_max\n",
        "        dog_tensor = torch.from_numpy(dog)\n",
        "        # returns a 2d tensor corresponding to the requested DoG filter\n",
        "        return dog_tensor.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zBUT0IUZDXxW"
      },
      "outputs": [],
      "source": [
        "class Filter():\n",
        "    \"\"\"\n",
        "    Applies a filter transform. Each filter contains a sequence of :attr:`FilterKernel` objects.\n",
        "    The result of each filter kernel will be passed through a given threshold (if not :attr:`None`).\n",
        "    Args:\n",
        "        filter_kernels (sequence of FilterKernels): The sequence of filter kernels.\n",
        "        padding (int, optional): The size of the padding for the convolution of filter kernels. Default: 0\n",
        "        thresholds (sequence of floats, optional): The threshold for each filter kernel. Default: None\n",
        "        use_abs (boolean, optional): To compute the absolute value of the outputs or not. Default: False\n",
        "    .. note::\n",
        "        The size of the compund filter kernel tensor (stack of individual filter kernels) will be equal to the \n",
        "        greatest window size among kernels. All other smaller kernels will be zero-padded with an appropriate \n",
        "        amount.\n",
        "    \"\"\"\n",
        "    # filter_kernels must be a list of filter kernels\n",
        "    # thresholds must be a list of thresholds for each kernel\n",
        "    def __init__(self, filter_kernels, padding=0, thresholds=None, use_abs=False):\n",
        "        tensor_list = []\n",
        "        self.max_window_size = 0\n",
        "        for kernel in filter_kernels:\n",
        "            if isinstance(kernel, torch.Tensor):\n",
        "                tensor_list.append(kernel)\n",
        "                self.max_window_size = max(self.max_window_size, kernel.size(-1))\n",
        "            else:\n",
        "                tensor_list.append(kernel().unsqueeze(0))\n",
        "                self.max_window_size = max(self.max_window_size, kernel.window_size)\n",
        "        for i in range(len(tensor_list)):\n",
        "            p = (self.max_window_size - filter_kernels[i].window_size)//2\n",
        "            tensor_list[i] = fn.pad(tensor_list[i], (p,p,p,p))\n",
        "\n",
        "        self.kernels = torch.stack(tensor_list)\n",
        "        self.number_of_kernels = len(filter_kernels)\n",
        "        self.padding = padding\n",
        "        if isinstance(thresholds, list):\n",
        "            self.thresholds = thresholds.clone().detach()\n",
        "            self.thresholds.unsqueeze_(0).unsqueeze_(2).unsqueeze_(3)\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "        self.use_abs = use_abs\n",
        "\n",
        "    # returns a 4d tensor containing the flitered versions of the input image\n",
        "    # input is a 4d tensor. dim: (minibatch=1, filter_kernels, height, width)\n",
        "    def __call__(self, input):\n",
        "\n",
        "        # if input.dim() == 3:\n",
        "        #     input2 = torch.unsqueeze(input, 0)\n",
        "        input.unsqueeze_(0)\n",
        "        output = fn.conv2d(input, self.kernels, padding = self.padding).float()\n",
        "        if not(self.thresholds is None):\n",
        "            output = torch.where(output < self.thresholds, torch.tensor(0.0, device=output.device), output)\n",
        "        if self.use_abs:\n",
        "            torch.abs_(output)\n",
        "        return output.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXSLZZoGS4z"
      },
      "source": [
        "# Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3DIAdG1mGS4z"
      },
      "outputs": [],
      "source": [
        "def plot_LC_timepoint_spikes(spikes: torch.Tensor,\n",
        "    timepoint: int,\n",
        "    n_filters: int,\n",
        "    in_chans: int,\n",
        "    slice_to_plot: int,\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    im: Optional[AxesImage] = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (10, 10),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(n_filters)))\n",
        "    sel_slice = spikes[timepoint].view(in_chans, n_filters, conv_size, conv_size).cpu()\n",
        "    sel_slice = sel_slice[slice_to_plot, ...].view(n_filters, conv_size, conv_size)\n",
        "    spikes_ = np.zeros((n_sqrt*conv_size, n_sqrt*conv_size))\n",
        "    filt_counter = 0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for n1 in range(n_sqrt):\n",
        "        for n2 in range(n_sqrt):\n",
        "            filter_ = sel_slice[filt_counter, :, :].view(conv_size, conv_size)\n",
        "            spikes_[n1 * conv_size : (n1 + 1) * conv_size, n2 * conv_size : (n2 + 1) * conv_size] = filter_\n",
        "            filt_counter += 1\n",
        "            ax.axhline((n1 + 1) * conv_size, color=\"g\", linestyle=\"-\")\n",
        "            ax.axvline((n2 + 1) * conv_size, color=\"g\", linestyle=\"--\")\n",
        "    ax.imshow(spikes_, cmap='Greys')\n",
        "    return spikes_\n",
        "    \n",
        "def plot_FC_response_map(lc: object,\n",
        "    fc: object,\n",
        "    ind_neuron_in_group: int,\n",
        "    label: int,\n",
        "    n_per_action: int,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param fc: FC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input\n",
        "    :param scale_factor: determines intensity of activation map  \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\t\n",
        "    ind_neuron = label * n_per_action + ind_neuron_in_group\n",
        "    w = fc.w[:,ind_neuron].view(reshaped.shape[0]//lc.kernel_size[0],reshaped.shape[1]//lc.kernel_size[1])\n",
        "    w = w.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*w, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_activation_map(lc : object,\n",
        "    spikes: torch.tensor,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot an activation map of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param scale_factor: determines intensity of activation map \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "    spikes = spikes.sum(0).squeeze().view(lc.conv_size[0]*int(np.sqrt(lc.out_channels)),lc.conv_size[1]*int(np.sqrt(lc.out_channels)))\n",
        "    x = scale_factor * spikes / torch.max(spikes)\n",
        "    x = x.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*x, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def reshape_LC_weights(\n",
        "    w: torch.Tensor,\n",
        "    n_filters: int,\n",
        "    kernel_size: Union[int, Tuple[int, int]],\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    input_sqrt: Union[int, Tuple[int, int]],\n",
        ") -> torch.Tensor:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Get the weights from a locally connected layer and reshape them to be two-dimensional and square.\n",
        "    :param w: Weights from a locally connected layer.\n",
        "    :param n_filters: No. of neuron filters.\n",
        "    :param kernel_size: Side length(s) of convolutional kernel.\n",
        "    :param conv_size: Side length(s) of convolution population.\n",
        "    :param input_sqrt: Sides length(s) of input neurons.\n",
        "    :return: Locally connected weights reshaped as a collection of spatially ordered square grids.\n",
        "    \"\"\"\n",
        "    k1, k2 = kernel_size\n",
        "    c1, c2 = conv_size\n",
        "    i1, i2 = input_sqrt\n",
        "    c1sqrt, c2sqrt = int(math.ceil(math.sqrt(c1))), int(math.ceil(math.sqrt(c2)))\n",
        "    fs = int(math.ceil(math.sqrt(n_filters)))\n",
        "\n",
        "    w_ = torch.zeros((n_filters * k1, k2 * c1 * c2))\n",
        "\n",
        "    for n1 in range(c1):\n",
        "        for n2 in range(c2):\n",
        "            for feature in range(n_filters):\n",
        "                n = n1 * c2 + n2\n",
        "                filter_ = w[feature, n1, n2, :, :\n",
        "                ].view(k1, k2)\n",
        "                w_[feature * k1 : (feature + 1) * k1, n * k2 : (n + 1) * k2] = filter_\n",
        "\n",
        "    if c1 == 1 and c2 == 1:\n",
        "        square = torch.zeros((i1 * fs, i2 * fs))\n",
        "\n",
        "        for n in range(n_filters):\n",
        "            square[\n",
        "                (n // fs) * i1 : ((n // fs) + 1) * i2,\n",
        "                (n % fs) * i2 : ((n % fs) + 1) * i2,\n",
        "            ] = w_[n * i1 : (n + 1) * i2]\n",
        "\n",
        "        return square\n",
        "    else:\n",
        "        square = torch.zeros((k1 * fs * c1, k2 * fs * c2))\n",
        "\n",
        "        for n1 in range(c1):\n",
        "            for n2 in range(c2):\n",
        "                for f1 in range(fs):\n",
        "                    for f2 in range(fs):\n",
        "                        if f1 * fs + f2 < n_filters:\n",
        "                            square[\n",
        "                                k1 * (n1 * fs + f1) : k1 * (n1 * fs + f1 + 1),\n",
        "                                k2 * (n2 * fs + f2) : k2 * (n2 * fs + f2 + 1),\n",
        "                            ] = w_[\n",
        "                                (f1 * fs + f2) * k1 : (f1 * fs + f2 + 1) * k1,\n",
        "                                (n1 * c2 + n2) * k2 : (n1 * c2 + n2 + 1) * k2,\n",
        "                            ]\n",
        "\n",
        "        return square\n",
        "\n",
        "def plot_semantic_pooling(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_weights(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        #print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            lc.kernel_size[0],#n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt*lc.conv_size[0] * lc.kernel_size[0],#n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            lc.kernel_size[0],#,n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt*lc.conv_size[1] * lc.kernel_size[1],#n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color=color, linestyle=\"--\")\n",
        "            \n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywXyWP0I83Au"
      },
      "source": [
        "# Design network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PcU9FSsVi4Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a378a5c-98a6-4d7d-babf-b3c4d7ae4107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ],
      "source": [
        "WANDB = True\n",
        "if WANDB:\n",
        "    !pip install -q wandb\n",
        "    !wandb login\n",
        "    import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8bZpJmlrJDa9"
      },
      "outputs": [],
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "def convergence(c):\n",
        "    if c.norm is None:\n",
        "        return 1-torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2\n",
        "    else:\n",
        "        mean_norm_factor = c.norm / c.w.shape[-1]\n",
        "        return  1-(torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2)\n",
        "\n",
        "        \n",
        "class LCNet(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: int,\n",
        "        n_actions: int,\n",
        "        neuron_per_action: int,\n",
        "        in_channels : int,\n",
        "        n_channels1: int,\n",
        "        n_channels2: int,\n",
        "        filter_size1: int,\n",
        "        filter_size2: int,\n",
        "        stride1: int,\n",
        "        stride2: int,\n",
        "        maxPool1: bool,\n",
        "        maxPool2: bool,\n",
        "        online: bool,\n",
        "        deep: bool,\n",
        "        reward_fn,\n",
        "        n_neurons: int,\n",
        "        pre_observation: bool,\n",
        "        has_decision_period: bool,\n",
        "        local_rewarding: bool,\n",
        "        nu_LC: Union[float, Tuple[float, float]],\n",
        "        nu_LC2: Union[float, Tuple[float, float]],\n",
        "        nu_Output: float,\n",
        "        dt: float,\n",
        "        crop_size:int ,\n",
        "        nu_inh_LC: float,\n",
        "        nu_inh: float,\n",
        "        inh_type,\n",
        "        inh_LC: bool,\n",
        "        inh_LC2: bool,\n",
        "        inh_factor_LC: float,\n",
        "        inh_factor_LC2: float,\n",
        "        inh_factor:float,\n",
        "        single_output_layer:bool,\n",
        "        NodesType_LC,\n",
        "        NodesType_Output, \n",
        "        update_rule_LC,\n",
        "        update_rule_LC2,\n",
        "        update_rule_Output,\n",
        "        update_rule_inh,\n",
        "        update_rule_inh_LC,\n",
        "        wmin: float,\n",
        "        wmax: float ,\n",
        "        soft_bound,\n",
        "        theta_plus: float,\n",
        "        tc_theta_decay: float,\n",
        "        tc_trace:int,\n",
        "        normal_init:bool,\n",
        "        mu: float,\n",
        "        std:float,\n",
        "        norm_factor_fc,\n",
        "        norm_factor_inh_LC: bool,\n",
        "        norm_factor_LC,\n",
        "        norm_factor_LC2,\n",
        "        norm_factor_out,\n",
        "        norm_factor_inh,\n",
        "        trace_additive,\n",
        "        load_path,\n",
        "        save_path,\n",
        "        LC_weights_path,\n",
        "        LC2_weights_path,\n",
        "        confusion_matrix,\n",
        "        lc_weights_vis,\n",
        "        out_weights_vis,\n",
        "        lc_convergence_vis,\n",
        "        out_convergence_vis,\n",
        "        thresh_LC,\n",
        "        thresh_FC,\n",
        "        num_episodes,\n",
        "        max_steps,\n",
        "        NodeType_FC,\n",
        "        add_layer_fc,\n",
        "        num_neurons_in_fc,\n",
        "        nu_fc,\n",
        "        update_rule_fc,\n",
        "        inh_fc,\n",
        "        inh_factor_fc,\n",
        "        classic_reward,\n",
        "        wandb_active = False,\n",
        "        batch_size=1,\n",
        "        save_interval = 100,\n",
        "\n",
        "\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructor for class ``BioLCNet``.\n",
        "\n",
        "        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n",
        "        :param n_neurons: Number of excitatory, inhibitory neurons.\n",
        "        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n",
        "        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n",
        "        :param dt: Simulation time step.\n",
        "        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n",
        "            respectively.\n",
        "        :param reduction: Method for reducing parameter updates along the minibatch\n",
        "            dimension.\n",
        "        :param wmin: Minimum allowed weight on input to excitatory synapses.\n",
        "        :param wmax: Maximum allowed weight on input to excitatory synapses.\n",
        "        :param norm: Input to excitatory layer connection weights normalization\n",
        "            constant.\n",
        "        :param theta_plus: On-spike increment of ``(adaptive)LIFNodes`` membrane\n",
        "            threshold potential.\n",
        "        :param tc_theta_decay: Time constant of ``(adaptive)LIFNodes`` threshold\n",
        "            potential decay.\n",
        "        :param inpt_shape: The dimensionality of the input layer.\n",
        "        \"\"\"\n",
        "        manual_seed(SEED)\n",
        "        super().__init__(dt=dt, reward_fn = None, online=online)\n",
        "        kwargs['single_output_layer'] = single_output_layer\n",
        "        kwargs['dt'] = dt\n",
        "        kwargs['n_labels'] = n_actions\n",
        "        kwargs['neuron_per_actionn'] = neuron_per_action\n",
        "        self.true_label = 0\n",
        "        self.dt = dt\n",
        "        self.intensity = kwargs['intensity']\n",
        "        self.reward_fn = reward_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_fn.network = self\n",
        "        self.reward_fn.dt = self.dt\n",
        "        self.n_actions = n_actions\n",
        "        self.neuron_per_action = neuron_per_action\n",
        "        self.n_classes = n_actions\n",
        "        self.classic_reward = classic_reward\n",
        "        self.add_layer_fc = add_layer_fc\n",
        "        self.num_neurons_in_fc = num_neurons_in_fc\n",
        "        self.neuron_per_class = neuron_per_action\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "        self.save_interval = save_interval\n",
        "        self.deep = deep\n",
        "        self.maxPool1 = maxPool1\n",
        "        self.maxPool2 = maxPool2\n",
        "        self.time = time\n",
        "        self.crop_size = crop_size\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.clamp_intensity = kwargs.get('clamp_intensity',None)\n",
        "        self.single_output_layer = single_output_layer\n",
        "        self.pre_observation = pre_observation\n",
        "        self.has_decision_period = has_decision_period\n",
        "        self.local_rewarding = local_rewarding\n",
        "        self.soft_bound = soft_bound\n",
        "        self.confusion_matrix = confusion_matrix\n",
        "        self.lc_weights_vis = lc_weights_vis\n",
        "        self.out_weights_vis = out_weights_vis\n",
        "        self.lc_convergence_vis = lc_convergence_vis\n",
        "        self.out_convergence_vis = out_convergence_vis\n",
        "        self.frame_analysis = frame_analysis\n",
        "        self.in_channels = in_channels\n",
        "        self.n_channels1 = n_channels1\n",
        "        self.n_channels2 = n_channels2\n",
        "        self.stride1 = stride1\n",
        "        self.stride2 = stride2\n",
        "        self.convergences = {}\n",
        "        self.norm_factor_LC = norm_factor_LC\n",
        "        self.norm_factor_LC2 = norm_factor_LC2\n",
        "        self.norm_factor_out = norm_factor_out\n",
        "        self.wmin = wmin \n",
        "        self.wmax = wmax\n",
        "        self.wandb_active = wandb_active\n",
        "        self.epochs_trained = 0\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_steps= max_steps\n",
        "        self.rew = 0.0\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.env.reset()\n",
        "\n",
        "        self.time_analysis = kwargs.get('time_analysis', False)\n",
        "        if kwargs['variant'] == 'scalar':\n",
        "            assert self.has_decision_period == True, ''\n",
        "\n",
        "        if self.online == False:\n",
        "            assert self.has_decision_period == True, ''\n",
        "        \n",
        "        if self.has_decision_period == True:\n",
        "            assert self.online == False, \"Decision period is not compatible with online learning.\"\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.decision_period = kwargs['decision_period']\n",
        "            assert self.decision_period > 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period - self.decision_period\n",
        "\n",
        "        elif self.pre_observation == True:\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period\n",
        "            self.decision_period = self.time - self.observation_period\n",
        "\n",
        "        else:\n",
        "            self.observation_period = 0\n",
        "            self.decision_period = self.time\n",
        "            self.learning_period = self.time\n",
        "\n",
        "        ### nodes\n",
        "        inp = Input(shape= [in_channels,crop_size,crop_size], traces=True, tc_trace=tc_trace,traces_additive = trace_additive)\n",
        "        self.add_layer(inp, name=\"input\")\n",
        "\n",
        "        ## First hidden layer\n",
        "        conv_size1 = compute_size(crop_size, filter_size1, stride1)\n",
        "        main1 = NodesType_LC(shape= [n_channels1, conv_size1, conv_size1], thresh = thresh_LC, traces=True, tc_trace=tc_trace,\n",
        "                             traces_additive = trace_additive,tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "        \n",
        "        self.add_layer(main1, name=\"main1\")\n",
        "\n",
        "        ### connections \n",
        "        LC1 = LocalConnectionOrig(inp, main1, filter_size1, stride1, n_channels1,\\\n",
        "                              nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, norm = norm_factor_LC)\n",
        "\n",
        "        # LC1 = LocalConnection(inp, main1, filter_size1, stride1, in_channels, n_channels1,input_shape=(crop_size,crop_size),\\\n",
        "        #                      nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC)\n",
        "\n",
        "\n",
        "        if LC_weights_path:\n",
        "            a = torch.load(LC_weights_path)\n",
        "            LC1.w.data = a['state_dict']['input_to_main1.w']\n",
        "            print(\"Weights loaded ...\")\n",
        "        \n",
        "        elif normal_init:\n",
        "            w_lc_init = torch.normal(mu,std, size = (in_channels, n_channels1 * compute_size(crop_size, filter_size1, stride1)**2, filter_size1**2))\n",
        "            LC1.w.data = w_lc_init\n",
        "       \n",
        "        self.add_connection(LC1, \"input\", \"main1\")\n",
        "        self.convergences['lc1'] = []\n",
        "\n",
        "        if inh_LC:\n",
        "            main_width = compute_size(crop_size, filter_size1, stride1)\n",
        "            w_inh_LC = torch.zeros(n_channels1,main_width,main_width,n_channels1,main_width,main_width)\n",
        "            for c in range(n_channels1):\n",
        "                for w1 in range(main_width):\n",
        "                    for w2 in range(main_width):\n",
        "                        w_inh_LC[c,w1,w2,:,w1,w2] = - inh_factor_LC\n",
        "                        w_inh_LC[c,w1,w2,c,w1,w2] = 0\n",
        "        \n",
        "            w_inh_LC = w_inh_LC.reshape(main1.n,main1.n)\n",
        "                                                             \n",
        "            LC_recurrent_inhibition = Connection(\n",
        "                source=main1,\n",
        "                target=main1,\n",
        "                w=w_inh_LC,\n",
        "            )\n",
        "            self.add_connection(LC_recurrent_inhibition, \"main1\", \"main1\")\n",
        "        \n",
        "        self.final_connection_source_name = 'main1'\n",
        "        self.final_connection_source = main1\n",
        "        \n",
        "        if self.add_layer_fc:\n",
        "            FC1 = NodeType_FC(n= self.num_neurons_in_fc, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "            self.add_layer(FC1, \"fc1\")\n",
        "\n",
        "            main_fc = Connection(main1, FC1, nu = nu_fc, update_rule = update_rule_fc, wmin = wmin, wmax= wmax, norm = norm_factor_fc)\n",
        "            \n",
        "            self.add_connection(main_fc, \"main1\", \"fc1\")\n",
        "            self.final_connection_source_name = 'fc1'\n",
        "            self.final_connection_source = FC1\n",
        "\n",
        "            if inh_fc:\n",
        "                w = -inh_factor_fc * (torch.ones(FC1.n, FC1.n) - torch.eye(FC1.n, FC1.n))\n",
        "                fc_recurrent_inhibition = Connection(\n",
        "                    source=FC1,\n",
        "                    target=FC1,\n",
        "                    w=w,\n",
        "                    update_rule = None,\n",
        "                    wmin=-inh_factor_fc,\n",
        "                    wmax=0,\n",
        "                )\n",
        "                self.add_connection(fc_recurrent_inhibition, \"fc1\", \"fc1\")\n",
        "\n",
        "        self.hidden2 = main1\n",
        "        self.hidden2_name = 'main1'\n",
        "        if maxPool1:\n",
        "            maxPool_kernel = 2\n",
        "            maxPool_stride = 2\n",
        "            \n",
        "            conv_size1 =compute_size(conv_size1, maxPool_kernel, maxPool_stride)\n",
        "            self.final_connection_source_name = 'maxpool1'\n",
        "            \n",
        "            maxpool1 = LIFNodes(shape= [self.n_channels1, conv_size1, conv_size1], refrac = 0)\n",
        "            self.add_layer(maxpool1, name=\"maxpool1\")\n",
        "            self.final_connection_source = maxpool1\n",
        "            \n",
        "            maxPoolConnection = MaxPool2dLocalConnection(main1, maxpool1, maxPool_kernel, maxPool_stride)\n",
        "            self.add_connection(maxPoolConnection, \"main1\", 'maxpool1')\n",
        "            \n",
        "            self.hidden2 = maxpool1\n",
        "            self.hidden2_name = 'maxpool1'\n",
        "\n",
        "        if deep:\n",
        "            # # Second hidden layer\n",
        "            conv_size2 = compute_size(conv_size1, filter_size2, stride2)\n",
        "\n",
        "            main2 = NodesType_LC(shape= [n_channels2, conv_size2, conv_size2],traces=True, tc_trace=tc_trace,traces_additive = trace_additive,\n",
        "                                            tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "            \n",
        "            self.add_layer(main2, name=\"main2\")\n",
        "\n",
        "            ### connections \n",
        "            lc2_input_shape = (conv_size1,conv_size1)\n",
        "            LC2 = LocalConnection(self.hidden2, main2, filter_size2, stride2, n_channels1, n_channels2, input_shape= lc2_input_shape,\n",
        "            nu = _pair(nu_LC2), update_rule = update_rule_LC2, wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC2)\n",
        "\n",
        "            self.add_connection(LC2,  self.hidden2_name, \"main2\")\n",
        "            self.convergences['lc2'] = []\n",
        "            if LC2_weights_path:\n",
        "                a = torch.load(LC2_weights_path)\n",
        "                LC2.w.data = a['state_dict']['main1_to_main2.w']\n",
        "                print(\"Weights loaded ...\")\n",
        "                \n",
        "            \n",
        "            elif normal_init:\n",
        "                w_lc_init = torch.normal(mu,std, size = (n_channels1, n_channels2 * compute_size(conv_size1, filter_size2, stride2)**2, filter_size2**2))\n",
        "                LC2.w.data = w_lc_init\n",
        "\n",
        "            self.final_connection_source_name = 'main2'\n",
        "            self.final_connection_source = main2\n",
        "\n",
        "            if inh_LC2:\n",
        "                main_width = conv_size2\n",
        "                w_inh_LC2 = torch.zeros(n_channels2,main_width,main_width,n_channels2,main_width,main_width)\n",
        "                for c in range(n_channels2):\n",
        "                    for w1 in range(main_width):\n",
        "                        for w2 in range(main_width):\n",
        "                            w_inh_LC2[c,w1,w2,:,w1,w2] = - inh_factor_LC2\n",
        "                            w_inh_LC2[c,w1,w2,c,w1,w2] = 0\n",
        "            \n",
        "                w_inh_LC2 = w_inh_LC2.reshape(main2.n,main2.n)\n",
        "                                                                \n",
        "                LC_recurrent_inhibition2 = Connection(\n",
        "                    source=main2,\n",
        "                    target=main2,\n",
        "                    w=w_inh_LC2,\n",
        "                )\n",
        "                self.add_connection(LC_recurrent_inhibition2, \"main2\", \"main2\")\n",
        "\n",
        "\n",
        "            if maxPool2:\n",
        "                maxPool_kernel = 2\n",
        "                maxPool_stride = 2\n",
        "                conv_size2 =compute_size(conv_size2, maxPool_kernel, maxPool_stride)\n",
        "                self.final_connection_source_name = 'maxpool2'\n",
        "                maxpool2 = LIFNodes(shape= [self.n_channels2, conv_size2, conv_size2], refrac = 0)\n",
        "                self.final_connection_source = maxpool2\n",
        "                maxPoolConnection2 = MaxPool2dLocalConnection(main2, maxpool2, maxPool_kernel, maxPool_stride)\n",
        "\n",
        "                self.add_layer(maxpool2, name=\"maxpool2\")\n",
        "                self.add_connection(maxPoolConnection2, \"main2\", 'maxpool2')\n",
        "\n",
        "\n",
        "        ### main2 to output\n",
        "        out = NodesType_Output(n= n_neurons, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "        self.add_layer(out, \"output\")\n",
        "\n",
        "        last_main_out = Connection(self.final_connection_source, out, nu = nu_Output, update_rule = update_rule_Output, wmin = wmin, wmax= wmax, norm = norm_factor_out)\n",
        "\n",
        "        if normal_init:\n",
        "            w_last_main_init = torch.normal(mu,std,size = (self.final_connection_source.n,out.n)) \n",
        "            last_main_out.w.data = w_last_main_init\n",
        "\n",
        "        self.add_connection(last_main_out, self.final_connection_source_name, \"output\")\n",
        "        self.convergences['last_main_out'] = []\n",
        "        ### Inhibitory:\n",
        "        if inh_type == 'between_layers':\n",
        "            w = -inh_factor * torch.ones(out.n, out.n)\n",
        "            for c in range(n_actions):\n",
        "                ind = slice(c*neuron_per_action,(c+1)*neuron_per_action)\n",
        "                w[ind, ind] = 0\n",
        "\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        elif inh_type == 'one_2_all':\n",
        "            w = -inh_factor * (torch.ones(out.n, out.n) - torch.eye(out.n, out.n))\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        # Diehl and Cook\n",
        "        elif inh_type == 'DC':\n",
        "            raise NotImplementedError('Diehl and cook not implemented yet fo r 10 classes')\n",
        "        elif inh_type == None:\n",
        "            pass\n",
        "        # Directs network to GPU\n",
        "        if gpu:\n",
        "            self.to(\"cuda\")\n",
        "\n",
        "    def frame_process(self, x):\n",
        "        x[x<1.0] = 2.0\n",
        "        x[x==1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "\n",
        "\n",
        "    def get_state_spiking(self):\n",
        "        intensity = self.intensity\n",
        "        screen = self.env.render(mode='rgb_array')\n",
        "        screen = screen.transpose((2, 0, 1))\n",
        "        _, screen_height, screen_width = screen.shape\n",
        "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "        view_width = int(screen_width * 0.6)\n",
        "        world_width = self.env.x_threshold * 2\n",
        "        scale = screen_width / world_width\n",
        "        cart_location = int(self.env.state[0] * scale + screen_width / 2.0)\n",
        "        if cart_location < view_width // 2:\n",
        "            slice_range = slice(view_width)\n",
        "        elif cart_location > (screen_width - view_width // 2):\n",
        "            slice_range = slice(-view_width, None)\n",
        "        else:\n",
        "            slice_range = slice(cart_location - view_width // 2,\n",
        "                                cart_location + view_width // 2)\n",
        "            \n",
        "        # Strip off the edges, so that we have a square image centered on a cart\n",
        "        screen = screen[:, :, slice_range]\n",
        "\n",
        "        # Convert to float, rescale, convert to torch tensor\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        h = screen.shape[1]\n",
        "        w = screen.shape[2]\n",
        "        resize = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize([80, 180]),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "                    # transforms.Resize([80, 80]),\n",
        "                    #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "                    # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "                    #transforms.CenterCrop((crop_size, crop_size)),\n",
        "                    transforms.Grayscale(),\n",
        "                    transforms.ToTensor(),\n",
        "                    #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "                    transforms.Lambda(self.frame_process),\n",
        "                    #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "                    transforms.Lambda(lambda x: x * intensity),\n",
        "                    transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "        screen = resize(screen)\n",
        "        if self.frame_analysis:\n",
        "            f = screen.sum(axis=0)\n",
        "            f = f.to(device)\n",
        "            plt.imshow(f.cpu().numpy().squeeze(), cmap='gray')\n",
        "            plt.show()\n",
        "        return screen\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        hparams = None,\n",
        "        online_validate = True,\n",
        "        running_window_length = 250,\n",
        "        verbose = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        manual_seed(SEED)\n",
        "        if self.wandb_active:\n",
        "            wandb.watch(self)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "\n",
        "        self.add_monitor(reward_monitor, name=\"reward\")\n",
        "\n",
        "        reward_hist = []\n",
        "\n",
        "        self.spikes = {}\n",
        "        for layer in set(self.layers):\n",
        "            self.spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=None)\n",
        "            self.add_monitor(self.spikes[layer], name=\"%s_spikes\" % layer)\n",
        "            self.dopaminergic_layers = self.layers[\"output\"]\n",
        "       \n",
        "        self.episode = 0\n",
        "        rew = 0.0\n",
        "        tot_rew = 0.0\n",
        "\n",
        "\n",
        "        if self.load_path:\n",
        "            \n",
        "            self.model_params = torch.load(self.load_path)\n",
        "            self.load_state_dict(torch.load(self.load_path)['state_dict'])\n",
        "            self.episode =  self.model_params['episode']\n",
        "            hparams = self.model_params['hparams']\n",
        "            reward_hist = self.model_params['reward_hist']\n",
        "            print(f'Previous model loaded! Resuming training from episode {self.episode}...\\n')\n",
        "        else:\n",
        "            self.env = gym.make('CartPole-v0')\n",
        "            self.env.reset()\n",
        "            print(f'Previous model not found! Training from the beginning...\\n')\n",
        "\n",
        "        pbar = tqdm(total=self.num_episodes)\n",
        "        \n",
        "        if self.time_analysis:\n",
        "            self.sample_spikes = {'input': [], 'main1': [], 'output': []}\n",
        "\n",
        "        for ep in range(self.num_episodes):\n",
        "            # print(f\"episode: {ep+1}\")\n",
        "            self.reset_state_variables()\n",
        "            self.env.reset()\n",
        "            done = False\n",
        "            tot_rew = 0.0\n",
        "            success = False\n",
        "            failure = False\n",
        "            num_steps = 0\n",
        "            for t in count():\n",
        "                \n",
        "                if t != 0:\n",
        "                    print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                    if self.time_analysis:\n",
        "                        self.sample_spikes['input'].append(self.spikes['input'].get('s'))\n",
        "                        self.sample_spikes['main1'].append(self.spikes['main1'].get('s'))\n",
        "                        self.sample_spikes['output'].append(self.spikes['output'].get('s').view(self.time, self.batch_size, n_actions, neuron_per_action))\n",
        "\n",
        "                        # w_lc1 = self.connections[('input', 'main1')].w\n",
        "                        # w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "                \n",
        "                image = self.get_state_spiking()\n",
        "                if gpu:\n",
        "                    inputs = {\"input\": image.cuda().view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "                else:\n",
        "                    inputs = {\"input\": image.view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "\n",
        "                clamp = {}\n",
        "                if self.clamp_intensity is not None:\n",
        "                    encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "                    clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "                # if done:\n",
        "                #     failure = True\n",
        "\n",
        "                if t >= self.max_steps:\n",
        "                    success = True\n",
        "                    done = True\n",
        "                elif done:\n",
        "                    failure = True\n",
        "\n",
        "\n",
        "                self.run(inputs=inputs, \n",
        "                        time = self.time,\n",
        "                        one_step=False,\n",
        "                        clamp = clamp,\n",
        "                        env = self.env,\n",
        "                        success = success,\n",
        "                        failure = failure,\n",
        "                        **kwargs,\n",
        "                        )\n",
        "                \n",
        "                rew = float(reward_monitor.get()[0])\n",
        "                tot_rew += rew\n",
        "                \n",
        "                lc_spikes1 = self.spikes['main1'].get('s')\n",
        "                #lc_spikes2 = self.spikes['main2'].get('s')\n",
        "                out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, self.n_actions, self.neuron_per_action)\n",
        "                sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "                selected_action = torch.argmax(sum_spikes, dim=1)\n",
        "\n",
        "                self.spikes['main1'].reset_state_variables()\n",
        "                self.spikes[\"output\"].reset_state_variables()\n",
        "                reward_monitor.reset_state_variables()\n",
        "                #self.reset_state_variables()\n",
        "\n",
        "                if done:\n",
        "                    if success == True:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\Successful episode!')\n",
        "                        num_steps = t+1\n",
        "                    else:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\nEpisode not successful!')\n",
        "                        num_steps = t+1\n",
        "                    break\n",
        "\n",
        "                obs, reward, done, _ = self.env.step(int(selected_action[0]))         \n",
        "                # Get voltage recording.\n",
        "                #main_voltage = main_monitor.get(\"v\")\n",
        "\n",
        "                #tensorboard.update(step= i)\n",
        "\n",
        "\n",
        "            if self.lc_weights_vis:\n",
        "                plot_locally_connected_weights(self.connections[('input','main1')].w, self.n_channels1, self.filter_size1,\n",
        "                                                compute_size(self.crop_size, self.filter_size1, self.stride1), self.connections[('input','main1')].locations,\n",
        "                                                self.crop_size ** 2)\n",
        "                plt.show()\n",
        "\n",
        "            if self.wandb_active:\n",
        "                wandb.log({\n",
        "                        **{'reward': tot_rew},\n",
        "                        **{' to '.join(name) + ' std': c.w.std().item() for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                        #**{name + ' spikes': monitor.get('s').sum().item() for name, monitor in self.spikes.items()},\n",
        "                        **{' to '.join(name) + \" gradients\": wandb.Histogram(c.w.cpu()) for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                    },\n",
        "                    step = self.episode)\n",
        "\n",
        "\n",
        "            #self.reward_fn.update() \n",
        "            #Plot_et.plot()    \n",
        "            # self.reset_state_variables()  # Reset state variables.\n",
        "            \n",
        "            self.episode += 1\n",
        "            reward_hist.append(tot_rew)\n",
        "            print(f'\\nEpisode {self.episode} lasted for {num_steps} time steps with total reward of {tot_rew}\\n')\n",
        "            if self.episode % self.save_interval == 0 and self.save_path:\n",
        "                model_params = {'state_dict': self.state_dict(), 'hparams': network_hparams, 'episode': self.episode, 'reward_hist': reward_hist}\n",
        "                torch.save(model_params, self.save_path)\n",
        "                print(\"\\n Model saved!\\n\")\n",
        "\n",
        "            pbar.set_description_str(\"Episode: \"+str(self.episode)+ \", Number of steps: \" + str(num_steps) +\", Episode Total Reward: \" + \"{:.2f}\".format(tot_rew))\n",
        "            pbar.update()\n",
        "\n",
        "    \n",
        "    def single_trial(self):\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        image = self.get_state_spiking()\n",
        "\n",
        "        if gpu:\n",
        "            inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "        else:\n",
        "            inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step = False,\n",
        "                mode = 'RL',\n",
        "                env = 'CartPole'\n",
        "                )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqAFucAUDb8"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3TerGeJoFdzg"
      },
      "outputs": [],
      "source": [
        "n_neurons = 2 #100\n",
        "n_actions = 2\n",
        "neuron_per_action = int(n_neurons/n_actions)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "crop_size = 40\n",
        "intensity = 200\n",
        "frame_analysis = False\n",
        "lc_weights_vis = False\n",
        "\n",
        "obs = 100\n",
        "dec = 200\n",
        "learn = 100\n",
        "time = obs+dec+learn\n",
        "\n",
        "max_steps = 100\n",
        "num_episodes = 10000\n",
        "\n",
        "filter_size1 = 24\n",
        "stride1 = 8\n",
        "n_channels1 = 100\n",
        "\n",
        "save_interval = 100\n",
        "\n",
        "learning_rate = 0.1\n",
        "norm_factor = 0.25\n",
        "\n",
        "add_layer_fc = False\n",
        "num_neurons_in_fc = 10\n",
        "NodeTypeFC = LIFNodes\n",
        "\n",
        "nu_fc = (0.0001,0.01)\n",
        "update_rule_fc = PostPre\n",
        "\n",
        "classic_reward = False\n",
        "inh_type = 'between_layers'\n",
        "\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': crop_size,\n",
        "    'intensity': intensity,\n",
        "    'round_input': False,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': n_channels1,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': filter_size1,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': stride1,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_actions': n_actions,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'max_steps': max_steps,\n",
        "    'num_episodes': num_episodes,\n",
        "    'save_interval': save_interval,\n",
        "    'add_layer_fc': add_layer_fc,\n",
        "    'num_neurons_in_fc': num_neurons_in_fc,\n",
        "    'classic_reward': classic_reward,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': True,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': obs,\n",
        "    'decision_period': dec,\n",
        "    'time_analysis': False,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'NodeType_FC': NodeTypeFC,\n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': None,#PostPre,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDPET,\n",
        "    'update_rule_fc': update_rule_fc,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.0001,0.01),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output': learning_rate,\n",
        "    'nu_fc': nu_fc,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': True,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': inh_type,\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    'inh_fc': False,\n",
        "    'inh_factor_fc': 100,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': norm_factor*filter_size1*filter_size1,\n",
        "    'norm_factor_fc' : None,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': None,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': '/content/drive/My Drive/LCNet/BioLCNet_cartpole.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_cartpole1.pth',#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'LC_weights_path': '/content/drive/My Drive/LCNet/BioLCNet_cartpole1.pth',#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25_weights.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': lc_weights_vis,\n",
        "    'out_weights_vis': False,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': False,\n",
        "    'frame_analysis': False,\n",
        "\n",
        "    ## reward\n",
        "    'n_labels': n_actions,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': 1.,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0005,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokdidkrV2Z5"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Venb2KhSYrT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311cf783-4ad4-4c0f-bd54-aeb9f9491309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "18fa4a7a31484692bc940427393999b4",
            "4b046ca6d23b4314b0e085511e83f892",
            "969408a1d3354d48a387ede9e28b72be",
            "8e60db1d8527468d994e959139ddeede",
            "4df8f476b64f448b85f89c98d7ebab77",
            "e641dbefe7f64b9e897bf6573ac98ac6",
            "89f3a7998e5441dc93e518d516d944ba",
            "1f5c7238957f444daa403dc39eda101b",
            "bf89107616ef445a8fd14cca56577515",
            "76e9193de65b45d1aec85df45011ae85",
            "85ae4b78711f4054abe26d9cc580634b"
          ]
        },
        "id": "oThYyYvHJzeP",
        "outputId": "b8f34532-1818-4881-d0bd-972030b984e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/singularbrain/biolcnet/runs/nfrxe74m\" target=\"_blank\">light-tree-228</a></strong> to <a href=\"https://wandb.ai/singularbrain/biolcnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights loaded ...\n",
            "Previous model not found! Training from the beginning...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18fa4a7a31484692bc940427393999b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.56137301748996\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5579909163167213\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.52747944338142\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.469820763406227\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.3848234828791659\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.31942947355684603\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2810725650161392\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.26980090670497725\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.28535830362413095\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.32362236055398086\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.3823697491290633\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4649977786985715\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5719402670510967\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6311394829895625\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.45026748032348374\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.23959463080100218\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.057192792589318475\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.0982569310150293\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2279442736250471\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3329155955321354\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.414065090303836\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 317 lasted for 21 time steps with total reward of -14.294908475964382\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4948251011845588\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.49890032953665\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5280827521594621\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5823972020281708\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6620431288213127\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5724891540718553\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4253917497038021\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.24811500017457655\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.03982613457795242\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.1415094404321447\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.297264766468362\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.428682162217896\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 318 lasted for 12 time steps with total reward of -16.81538581686006\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6735418358957664\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6733104550602893\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6683699177412338\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6160700455050614\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5373398565376694\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4316552143509873\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.28261460503390023\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10236960940265782\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.05096598053501722\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.17858174871932364\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.28152381219865436\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.360686127643337\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 319 lasted for 12 time steps with total reward of -16.886486129568766\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.636915732862115\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6381127609310161\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6685454779851385\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6632504567370971\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.582680167955987\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4638011469693928\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.31462366958539\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.19355207324396617\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09963821886764335\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.03208963027372441\n",
            "output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.0097223229593143\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.02625767163587711\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.01780551518777429\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.015516606324671112\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07376208865804013\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1571535973092854\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.26608034138305636\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4010940510210482\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5437343020864372\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6476351877496357\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.42952711111940234\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.18165757248031167\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.03811482305873115\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.23134508291467415\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.399479579680953\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 320 lasted for 25 time steps with total reward of -13.713354801893963\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5446122984734506\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.5433518722198574\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5115656152310453\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.44924423633506305\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4153057761655684\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.40935511279207737\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.43117304427234027\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.480717800197216\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5518547564324189\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6438363943818133\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.602076287803858\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4385699586202799\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.24499522349995428\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.07950790520703749\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.059116014531279126\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.17196296327988148\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2599696621738215\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.32391574849518867\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.36441974342086003\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.3819372694747\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 321 lasted for 20 time steps with total reward of -15.215155119743752\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6409110881109372\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.643327562396847\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6717840213799807\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6367209354383897\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5466441077629782\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.42671090205042894\n",
            "output tensor([[1, 1]]) selected_action: tensor([0]) rew: 0.27639550992105977\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.09501991533577736\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.059290527439909146\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.18772936935497248\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2913472690355563\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.429231381156484\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 322 lasted for 12 time steps with total reward of -17.030084504590523\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5817661844459029\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.576912925960256\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5976796372869569\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6440369564184218\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6468577801927052\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5351841509244012\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3937180589049968\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.22181506979464272\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07773309089550018\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.03962738263717397\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.1312186918602073\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19783639654212182\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.24011358948884715\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2585179445338357\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3114346250256004\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.398946308015994\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 323 lasted for 16 time steps with total reward of -17.301991083279997\n",
            "\n",
            "output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.48558420350793663\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.48284596289144577\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5083015177610806\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5570045858953767\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6299594567123017\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6454433694844794\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5056206377792513\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.33596380888310595\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.1947770508748401\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.08099462942283575\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.006301703641809486\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.06787205514054206\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.10431188824820259\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.11604846634509425\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.10333933137080187\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.0662721814188863\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.00476576243866389\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.08142838425519167\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.19272326532698603\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.32969261021514695\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.49306540898555207\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6685239470298061\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4973363034077102\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.2489638602252382\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.030069560878281\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.282159993336061\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.50911433531038\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 324 lasted for 27 time steps with total reward of -14.352026275470434\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5004993192945446\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4958062303680846\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4636798699810235\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4040948200533584\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.36943285572797624\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3592789105628863\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.373391851008248\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4117060368241111\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.47433073456433983\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5615475638243198\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6451147892659156\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.48706997817094644\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.2993603198766819\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.14019618353668628\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.008410847055443482\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.09702242867199673\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.17697795930900456\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.23216991179184499\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2631479377688613\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.27029510588758265\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.3119258254719577\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.38806916626191196\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.49890899796059\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 325 lasted for 23 time steps with total reward of -16.244597023009185\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4957602821904675\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4948461219208604\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4664549083695494\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4105831733063663\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3796342939250419\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3732172070060903\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.39111518713197835\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.43328712441184514\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4998666659261446\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5911593625077926\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6187024937156957\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4565035020708812\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.2644525251955965\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.10072534208977552\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.035883088727850954\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.14643977308931688\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.23186122201052006\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2929057759734308\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3301693233531845\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3440833208145954\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3349143048598725\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3027643344203479\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2475720234203576\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.16911401594417286\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0670069346584487\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.05928999401736462\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2104712571448426\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.3873797448299805\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5909990379681653\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5591471356163104\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.3170633849247969\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.02619870920016082\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.23746715900810977\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.475799820557576\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 326 lasted for 34 time steps with total reward of -15.089123643368076\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.648036444198598\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6475747299498118\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6764733580671162\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6396119195693175\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5618964361196621\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4575614632577766\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.31178851160177845\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1917696841311971\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.09882954259382404\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.03218067692782217\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.008800295744903708\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.024568139640079423\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.015406780008923149\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.018571252514774594\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07742415517045026\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.16137916491430465\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.27082982495901387\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.39022447717586406\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5304030018057097\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6415115973112806\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.42299916521945624\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.17474652055112616\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.04538874912508584\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.23896272060567203\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.407423101394077\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 327 lasted for 25 time steps with total reward of -13.786737860479853\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5100930491761654\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5091086209534431\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4807048744415233\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.42487699078862784\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3940323794911307\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.38778004649148645\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4059033731617172\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.44836142657981737\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5152881055611557\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6069892646808853\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6148553788113322\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.45241739482967835\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.26006513253110963\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.09596876696016565\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.04108244351253626\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1521613475665664\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.23819053612549101\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.35823734396933987\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.512692255058223\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 328 lasted for 19 time steps with total reward of -15.195919121773917\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6764274711082423\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6764186659650115\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6467089522387401\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.58729895778898\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4980132042212311\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37850463526894673\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.22826257389751525\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10564676891352154\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.009686532060916908\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.060433639030544994\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.10536652870502294\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.12559793107953654\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.12144463563433328\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.09305395098577046\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.04040430371224468\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.03669331992022845\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.138593108947823\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2658111553542267\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4190207861895139\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5776330323371852\n",
            "output tensor([[1, 5]]) selected_action: tensor([1]) rew: 0.5931485061291893\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3564696736909514\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.1488563657513986\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.03112339271702913\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.18478844549383922\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.31332542044943373\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.476012366372142\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 329 lasted for 27 time steps with total reward of -15.208356904396275\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6160339278338186\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.6153665332793491\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5879456560893541\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.5337663403484565\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.45264928153427797\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.39685751344238396\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.36446438983214524\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.35742336601299485\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.376631766893587\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4182035165848539\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.48400103817159035\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5743248847160816\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6654263114730715\n",
            "output tensor([[ 1, 13]]) selected_action: tensor([1]) rew: 0.5054302390135174\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.31578331593997955\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.15468076684475895\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.02093954428343725\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.08648465484925649\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1684846833730591\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.225794041521863\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.31723835075277473\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.443077795435094\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 330 lasted for 22 time steps with total reward of -13.801151133638388\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4794014451440485\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4840598037745839\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5169357837857573\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5780608767754416\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6676425220172976\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.6139401319700817\n",
            "output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.4661431852668545\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.28826821083502274\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.13855800972924337\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.015883404412920743\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.08074061712510444\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.15214403558342915\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.19899521582401059\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.22179733364548626\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.22088683872039178\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.19643325269906686\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14843984574202257\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.07674495126030273\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.018976124708157793\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.13920845619244293\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.2845937816712495\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.436667703924845\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.606119651017881\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5196936716796396\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.2643872728945892\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.037780403625825965\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.16168656313669239\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.3354702897488774\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.484904010846485\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 331 lasted for 29 time steps with total reward of -15.721922514905986\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6630314917039601\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6656469540368067\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6418093637009322\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.591533130001467\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5146576840282501\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.40763205085509047\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.25948624075283344\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.08000389538797836\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0727141792996373\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.19985481502762475\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.3024609395670956\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.381423245908447\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 332 lasted for 12 time steps with total reward of -17.132652369335485\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.6337229383746054\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.629031873504409\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5976675586994854\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5396020416302997\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.45463335324665044\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3318168820784515\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.23485394213011113\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.16508765275132936\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12187998918915183\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10476226878798195\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11343840459848098\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.14778608274757093\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.20785636228039117\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.28993701706084785\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3859710012895744\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5056649281699417\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6497247289348183\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.49237443105982426\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.26760705736387314\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.07138610477770624\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.09767824671586167\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.24085458909370622\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.35927390004616966\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.453919223287027\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 333 lasted for 24 time steps with total reward of -14.206921340467261\n",
            "\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.44983718180416543\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4483569839211704\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4158767963457012\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4114647278594069\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4349166038717005\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4862057895505407\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5654821728554211\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6730688997478758\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5905433038125345\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.4247046697384277\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.28775206555991395\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.1786598434694905\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.09655110589887228\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.040708733357668\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.010582196530851462\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.00579133512148583\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.02612798847982667\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.07155608043511985\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.14221050086280235\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2383948867501683\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.36057817828653926\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5020028640106254\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6550012317141806\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5098307403192016\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.27588286062918854\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.01138093278026453\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.2259773839192994\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.437901599499032\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 334 lasted for 28 time steps with total reward of -12.350410309705188\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6813859538600209\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6821196569297502\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6791287766587963\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6251225884958145\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.538537630432106\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4191028569083284\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.26911440923280994\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.08790163261605444\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06643359020188411\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.19509131650063055\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.299128736997762\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.37945142644581\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 335 lasted for 12 time steps with total reward of -16.957691565012407\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.44546267609842927\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.44048176244341486\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4635390416878953\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5146035809170122\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5938211619363398\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6920001364550201\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5618399979244331\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.39558598074884477\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.19893445189910608\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.02903675733123723\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.23052658419151917\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.407051271782855\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 336 lasted for 12 time steps with total reward of -16.360345823195118\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5309951620629081\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5297179297130641\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5011053222660619\n",
            "output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.4451513160979239\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.35361873953521916\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.288704461040194\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.25112069769359713\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2404420849578225\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2564178285936626\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2989729483503415\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.36456610377331977\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4475003458564386\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5545938364740375\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6604424878418128\n",
            "output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.4801048626963784\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.2701110703476024\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.08854128992148086\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.06591341555858549\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19443281827468756\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2980530207348377\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.435875577464035\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 337 lasted for 21 time steps with total reward of -14.432168344810282\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.492957811052524\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.49458564876988953\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5245120541276228\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5827502907809106\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6694898983997668\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6149073247116207\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4699140553750587\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2948474162348381\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.08888296963233061\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.08998330513368552\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.24310242232201096\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.37169354668131\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 338 lasted for 12 time steps with total reward of -16.471931805052446\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6047573036901078\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6069395878684943\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6349404704704151\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.669005709623812\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5790191090645033\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.45939501987588294\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.30960564683028546\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12897024085956255\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.02435130930068302\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.15153608130517227\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.25361776371356204\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.33147718022424494\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.385836834955064\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 339 lasted for 13 time steps with total reward of -17.154186081215663\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5230311914138333\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5256840543278897\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4976699847889431\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4390055118738513\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.40862626691771053\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.40616264005469505\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.43142131000167283\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.484386376741884\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5612404067122598\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6573551050594459\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5880175673980036\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.420925541199792\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.2828056040565349\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.17262938527055083\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.08951675654965574\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.03274711240542466\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.0017663691276404991\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.003809124214542159\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.015809132816967553\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.06058192963096776\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.13064125085180006\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.22628805913838368\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3479888335100515\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4963705173097166\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6722133029541234\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5235595494605061\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.28989588715988357\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.025634056201787603\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.21153391032587765\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.423320726632863\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 340 lasted for 30 time steps with total reward of -11.326690058239306\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5789747176916367\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5788178993921148\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5516036915349307\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4916160289264452\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.45905089639776075\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.454730220218344\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.47844937920545727\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.530181421204224\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6017476872396749\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6815436355019593\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.5441871114602159\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3772050668673166\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.17980192097588066\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.010080150056350046\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.13322958390246964\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.25126082645259357\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.34499962010448004\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.41527753054976\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 341 lasted for 18 time steps with total reward of -14.626777734336992\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5087132069045965\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5109366629558793\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.541535747473168\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6005240167078665\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6781255807961745\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5954002234714038\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.44942312060321477\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2732945240946165\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.06619043665317903\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.11390715527373357\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.26835693898231994\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.398385998314815\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 342 lasted for 12 time steps with total reward of -16.55650657291077\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5437220783295924\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5452894934656894\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5754431427927307\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6279260054628076\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.678280794115573\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.561616363075692\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4152927043739524\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.23863217622296762\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.030816541123997787\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.15020154805309704\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.3057930419142792\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.437197482830847\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 343 lasted for 12 time steps with total reward of -16.67617277383522\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5310152956218764\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5308445071118646\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5572881065611145\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.60809968687691\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6843307280369637\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5840909943291023\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.44007986902145\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.26586662289392793\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1197083211128186\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.0004946388639476829\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0927389480956613\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.16080084285870144\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.2627119296054873\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.398790872394564\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 344 lasted for 14 time steps with total reward of -16.59322382252444\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6845389795462685\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6879452500543204\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6548379945747013\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5914821689217026\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4983532856204873\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3750804228176501\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.22113006467315188\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09484537200465365\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.004766320252418521\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.07854376211551634\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.12716390662180077\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.15113739778723934\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.20911940983920152\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3012232476832482\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.42771712134025\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 345 lasted for 15 time steps with total reward of -17.49145762742674\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5088519957087281\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5117619298321759\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5398462810236561\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5931234820597366\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6717867121427699\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5691572570996692\n",
            "output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.4225495106904501\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.24572516838725555\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.09692420375234934\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.024981752538273283\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.12097639150243034\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1918882199238049\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.23838424652055656\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.31915909585950564\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.43442609760482\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 346 lasted for 15 time steps with total reward of -17.1700892632526\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5593967175005112\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5558250224471498\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5809935967106505\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6348808106717412\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6787560317679052\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5703946688448793\n",
            "output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.4287339037967135\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2567240546302566\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11262618974223376\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.00466002091755352\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.09608803696181767\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.16245460754154162\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2627404335863215\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.397254078511722\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 347 lasted for 14 time steps with total reward of -16.544866181406913\n",
            "\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5138385575920699\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5132071383995995\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5409544031770259\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5913339522988642\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6669023443604494\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6046696416269551\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4616810151869134\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3477501841642233\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.26199625990493447\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.20369565956898295\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.17229123165192362\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.16739767540460104\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.18880422823544685\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.23647529478244889\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3105494054663476\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.41133662343235733\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5393142623975917\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6951205278999824\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5204545526959676\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.30648178680199545\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.061910153844086324\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1554523252951825\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3471888140831581\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.57301252433426\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 348 lasted for 24 time steps with total reward of -12.759488764819832\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6806404696293592\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6798102050719756\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6526079152057556\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5990292483964333\n",
            "output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.5188951063539116\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.403809190217019\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3124499019567487\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2487635416787186\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.2121596187168655\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2022187157983233\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.21869574282630255\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.26152098114166444\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3307993642567254\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4204337386366812\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5272843332266193\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6586801342968545\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.519523603896131\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.31056008748581054\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.13023859560092543\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.02273383189907191\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.14952087120988267\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.2511420204485209\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.3867683832569737\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.556856409259595\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 349 lasted for 24 time steps with total reward of -13.478901021681219\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.45965625341297045\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.45770101984855693\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.480650968243985\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5284961464515417\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6014016019393877\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6240502308189707\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.48287108902007814\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3118804052755735\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11027015859077366\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.0638991655212402\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2119308018551952\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.33499381999808503\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.49230409956962\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 350 lasted for 13 time steps with total reward of -17.046150013342302\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6588355109847345\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6583591637957424\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6314267742775882\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5780356501503849\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4980083960034648\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3909973302980233\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.24806603983714304\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.06593846880582105\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.0895410987052298\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2195842868441169\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.3252601546879773\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -20.40748683248065\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 351 lasted for 12 time steps with total reward of -17.31220503856507\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5484098866392336\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5438567911838597\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5646615410320557\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6107951175353593\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.682403968037464\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5862430215640044\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.44622939380632787\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.27603985353963145\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.13396082143265303\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.018912919860949395\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.07003554231239284\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.13365856334180715\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.17256644652436126\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.18720260703163305\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.17784228505719724\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14459253324088273\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.08739308837885112\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.006017954681075799\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.09992227317947289\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.23097712422896055\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3878515818226056\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5713994030054848\n",
            "output tensor([[2, 8]]) selected_action: tensor([1]) rew: 0.617386417969746\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.37738710436693657\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.16651988568936293\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.016664418895362243\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.17350423435468332\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.30520751980051736\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.412838982323528\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 352 lasted for 29 time steps with total reward of -15.024567071048185\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5217497511104512\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5203944596471904\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5442295509868189\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5932474440879179\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6676156891055827\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5956233578057565\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.45286405471893876\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.2799951486409247\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1352904719910311\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.01765521124431152\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.07385893961505163\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14004342236582606\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.181526568124667\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.19877023941284583\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.19206843263932027\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.16154718519439115\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.10716537482865629\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.02871621681795794\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.0741705351208996\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.20202548504136375\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3555351936531469\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5355358811094325\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6569955800151038\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4209544136593639\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.15512566411972073\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.08273318918646066\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.29429048450624773\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: -20.481116507526895\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 353 lasted for 28 time steps with total reward of -15.212828668160366\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5360739694355263\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5346066268341382\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5584259532646879\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6075211751575218\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.682056730410742\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5837766735023225\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.4414280279994235\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.26888827942938753\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12442599916271851\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.006942968985282849\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.08451248807896572\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.15073530824421066\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.19235756286837002\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.20984519079757735\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2034966613957967\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.17344291342228385\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.11964815650787186\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.04191133754494908\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06013172777460335\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.18700536763734554\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3393898278737576\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5150058814198482\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6758471402185313\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.44139706176744375\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.17732262424669332\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.058598463165147385\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2680107448366273\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.452459848707168\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 354 lasted for 28 time steps with total reward of -15.214772640448993\n",
            "\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.6111627428677743\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.612992164984894\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5847598127432841\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5264763519023723\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.437976609064077\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3189240954416117\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2278869682506156\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.16410446033088721\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1269807930853652\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11609081979257868\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.13118291920290598\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1721798613066312\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.23917808417439645\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.33244555649922847\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4524181503849808\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5996942059902841\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6249732716037987\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4206875824060754\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1864234950547463\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.019895054447996108\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19974892170700198\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.35450213726877533\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.5435476227593\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 355 lasted for 23 time steps with total reward of -14.231155791096565\n",
            "\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5196672949462356\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5217518392450836\n",
            "output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.4964205555627942\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4403239516090718\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.40952237246988377\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4066163118540904\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.43140885762718584\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4801971113957716\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5506492275304511\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6460258496163379\n",
            "output tensor([[1, 9]]) selected_action: tensor([1]) rew: 0.5902030881413034\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.42374852924683193\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.2863115742018213\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.1768702661806878\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.09455115754306342\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.038640596160172624\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.008591732341526104\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.004028433889401273\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.024746998610206017\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.07071626877822978\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.14207648910906817\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.23913700650407854\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3623726788538031\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5124186332191942\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6865341727120622\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5037647035650509\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.268006063772281\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.0014959033036227032\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2381069844091559\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.452540356781036\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 356 lasted for 30 time steps with total reward of -11.353849673200882\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5918704295977293\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5875600745679013\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5530462574067124\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4883044802503963\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.39313527879150456\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.26716953689246503\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10987757749090987\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.02050155633863915\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12500693555322162\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.20452535612377631\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2597843922113032\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.291348616718792\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.29961795193170493\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2848273892126667\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.24704756134680977\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.18618586490575778\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.10198802012412428\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.0059598677742098705\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1382285086427385\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2907767606826932\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4459473773414483\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6248081871000113\n",
            "output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.4728642847209312\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.2053428789103401\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.03371453812474701\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.24595089231866762\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.43291299924196\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 357 lasted for 27 time steps with total reward of -17.55852057398218\n",
            "\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4880667869522952\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.48766235323485496\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.45975296348566885\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4043350794965014\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3738091830205654\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.36778334767298004\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.38603995105293987\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.42853694562906086\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4954070072711716\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.5869547040432992\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6270815909188174\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4657529056367232\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.27461053909772903\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.1118342466875567\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.023776774559489844\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.13328515207947944\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.21760283118802726\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.27748335552846937\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3135175471826285\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3261314960951012\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3155860501108178\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2819772482855143\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.22523736015322404\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.14513638866528122\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.04128407420270225\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.08686738811616418\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.24001952604823973\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4190220795236542\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6248650125025896\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5413329568225754\n",
            "output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.2783405182420694\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.015164003467345866\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2816396303701678\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.5229753839943\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 358 lasted for 34 time steps with total reward of -14.97272221042709\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6929811949157809\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6950627331088126\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6702156847769538\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6094073562532402\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5188560739943727\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.3982024113287326\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.24692362986432226\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12338144088492697\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.02659945156681126\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.044244017927656476\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0898081160305998\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1105849830003951\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.10689759692817563\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.07889920805496764\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0265738859514425\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.050262062338656655\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.15195783024958243\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.27902482721005495\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4221489177153368\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5761657381095032\n",
            "output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.5801128240853495\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3434146863061127\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.13574755746161005\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.04432062148892618\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19810849158369814\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.32680231820401195\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.489669320760367\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 359 lasted for 27 time steps with total reward of -14.995444139760082\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5903095248421292\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5923592806335334\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6201349366929634\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6736486063651919\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5977315236452917\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.47941055018041845\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3310299016726347\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.15191090153181175\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.0002329245076653219\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12516895161646358\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.22531577698687943\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.3010751140921641\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.35315545536459786\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.440036369077134\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 360 lasted for 14 time steps with total reward of -17.4079835170656\n",
            "\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4614596465387968\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4587767459733334\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.48101310771856454\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5281487852841612\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6003389661481363\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6330022610275418\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4941445053288337\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.32550861031680667\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.18540684189111833\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.0727822000217504\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.013274454111566691\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.07351372281451041\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10852114635884269\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1187136277223621\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10433790343780552\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.06547043361923272\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.002018335815853922\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.08627878371698594\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.1998460333849732\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.33926919706711833\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.505288962877476\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.675244376627947\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4791970730280414\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.22753890175332203\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.055005774860696965\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -20.369743416977457\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 361 lasted for 26 time steps with total reward of -14.157353817013423\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6519125705144397\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6552662032550393\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6320294289881029\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5822242178948888\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4981972331726465\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3819301631915214\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.293964432872187\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.23356488789268437\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.20016223846193526\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.19335880478462986\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.21293143700179984\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2588323079408318\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3311879885735204\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4302969444830793\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5456104948104427\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6801618753653416\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5063968439442021\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2940474302996937\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.11021925544797273\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.04640802499795654\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.17702713100745449\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.28268814655059493\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.3642891064893402\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.48056462776722\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 362 lasted for 24 time steps with total reward of -13.658682277917606\n",
            "\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.46425989666448286\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.46157132636895515\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4869584290248071\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5404038039910178\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6124205818944694\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6677208090956267\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.5284591420345404\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3594909206557598\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.16001337537315485\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.01189093634748889\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.15750821518276914\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.27798971928691113\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.432590970453546\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 363 lasted for 13 time steps with total reward of -16.5986815561679\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5369550956161476\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5376611895161132\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5669574782292695\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6248502633596933\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6860953757546441\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5726712076910506\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.4272096394241238\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2514171180768937\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10353186967089711\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.01757197117730247\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.11287507269799546\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.18320271693133805\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2292187213091807\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2514222285907983\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2501464160431439\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.22555844431174982\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1776602049046344\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.1062896356524381\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.011122564197717233\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10832477925088546\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.25269225689250274\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3976604888922963\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5652728620633788\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.5560662212845922\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3027265457548033\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.019200267772885027\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.23687868834921766\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.46731082341408\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 364 lasted for 28 time steps with total reward of -15.759964828329421\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6379375091054642\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6402339300195546\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6685783409146013\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6313710382774552\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5413284110899385\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4214230344686335\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.27112964407742857\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.08977075439546989\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12346715716098666\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3108025611855522\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.473681491203642\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 365 lasted for 11 time steps with total reward of -17.006178547201635\n",
            "\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.4594936924626499\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.46147800709234965\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.43246830662227953\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4315514208800828\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4585417933867745\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5134314525309936\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5963887587805955\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6922451257772589\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5519622416007318\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3820967865929952\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.24096307755233726\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.1275082109679574\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.04082673464477227\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.019828263585456918\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.05503831741214604\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.06521578383088522\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.05060217166458114\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.011267963841105177\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.052886443474498945\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.14212882771751173\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.2568921757478244\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3977703769483666\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5655120644991265\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6389881277109294\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.414701850058366\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.16048403092830382\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.12490409601480162\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.442764560671566\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 366 lasted for 28 time steps with total reward of -12.751301651043836\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5538683105448993\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5524981486079028\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5798320333934253\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6358630166862004\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6753500076194527\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5651320585549744\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.42130718270590917\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.24710030120752258\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.04170568222021609\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1369169983520696\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.29012708291748107\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.41915158093101\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 367 lasted for 12 time steps with total reward of -16.57353892066006\n",
            "\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5541587016027599\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.555546102045481\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5297442317748058\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5293792828314445\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5542707316078451\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6044144262049549\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.679981325235982\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.5823686594186999\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4391836248560501\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.32494283470856744\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.23876040812078225\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1799077881434129\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.14782271410054315\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.1421145347115324\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.16256683601156474\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.20913805915282435\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.2819604996780968\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.38133781391041954\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5077409049162509\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6618018135450636\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5556950047476412\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.3438248766245847\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10153966155212973\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.17230969266546692\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.420241814137114\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 368 lasted for 25 time steps with total reward of -11.324350671301145\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6446661207757175\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6490309396382791\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6235152313797292\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5681446293587524\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.48276845902117227\n",
            "output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.3670638831371058\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.22054351781501969\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10158031792746358\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.009224361110876933\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.05731789335177351\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.09867645526278235\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.11531379677100473\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.10752302058210172\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.07542751601188996\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.018981670124263272\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.06202756908208429\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1679782678961329\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.29941000594443967\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.453396036928129\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6118796883792237\n",
            "output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.5457107925475319\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.30395476131627563\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.0909944785081554\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.09464614926581227\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2543323080143576\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.3892992614561317\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -20.500640190798272\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 369 lasted for 27 time steps with total reward of -15.510269200872301\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.48050982476065596\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.47578976365135817\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4960636357365129\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5413015629842187\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6116489687263847\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6335803719432149\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4954244931516545\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.32746261459614845\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12890273392824497\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.04217444930972647\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.18705154016431236\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.30687560039268824\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.402646776099818\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 370 lasted for 13 time steps with total reward of -16.74806439648815\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6812848970250975\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.681381065536445\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6634821379698925\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6046239559868917\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5160509070400562\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.39741531366272087\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.24820548133943043\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12680143756205137\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.03224312782016109\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.03627399414500676\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.07939121849875191\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.09758214490804379\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.09115056050486137\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06022991386239318\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0047839286432797\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07539186386467767\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.18066767615398416\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3115758929715442\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4688059596439944\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6404765445799588\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5342720198783456\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.29249767833601326\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.07939161126897065\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.10652981714303389\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2666390347301039\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.4021785642181\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 371 lasted for 26 time steps with total reward of -14.610191606013341\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5466166622382834\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5476989513918298\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.574147049144798\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6259686243981515\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6697782176736947\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5534173675571544\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.40737589616451786\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.23097680301441048\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.02340255271198277\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.15739759507567808\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.3127958996544372\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.444033231178352\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 372 lasted for 12 time steps with total reward of -16.734844601613645\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.563239011761726\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5623882618984561\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5344179818794119\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.47932527211998543\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.396342999627462\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.27390989168600866\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12006057987277285\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0069610959900666125\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.10817692067713491\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.18445541739035776\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.23650525179713544\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.26487168914903464\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.2699350972207094\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.2519107613578775\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.21084951967555543\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14663893803486627\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.059004933475942545\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.04139189953769484\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1544943330098214\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.28956721364785887\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4474912186337908\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6292840734813255\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4063647691387007\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.1342836082610227\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.10956910130314085\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.32687500490651866\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.519221592537242\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 373 lasted for 27 time steps with total reward of -17.662414208959543\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6327850351938182\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6310662789989034\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5994121695608389\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5378132497900486\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.44608440628668955\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3238697201930798\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1706509029964396\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.01424013905103444\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.17279807830345306\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.30626280297806363\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.415732875231438\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 374 lasted for 11 time steps with total reward of -17.567352132544173\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4991802654465507\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.49646776882919763\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.466308808119703\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4086899509104356\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.37600582470471455\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.36785477197047634\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.38400930658631494\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.42441752325884097\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4892023609816325\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5786588791270999\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6270233388605617\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.46649996345572353\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.2761949545171327\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.1142987286701691\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.020377888563694047\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.12888519609972088\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21212218245057413\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2708288410684413\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.36378601966379037\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.43318173791732\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 375 lasted for 20 time steps with total reward of -15.454369420324987\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5086518191420086\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5037809470318221\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4715305394499627\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.46447689768062383\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4823998160181847\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5252552757879403\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.5931747009029911\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6399225810165939\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5039361260672942\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3382287444384733\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.20113754841579246\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.09163258887176462\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.008833309012900026\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.04798100042067466\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.07936481830072456\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.08570291565083299\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.06720891229386478\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.023925274252828588\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.044275587809867456\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.13768922606317535\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2567759558642655\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.40215621974185145\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5746039661263181\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5714730256007801\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.37028936606285445\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.13583032382009264\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.09627307404643493\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.30247616052296067\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.484337418145568\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 376 lasted for 29 time steps with total reward of -13.361215008708335\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6194996613268848\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.619789253399838\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5900915615582525\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.53041075107739\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.4405751563492021\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3202418938706507\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.22797839038291978\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.16301990441694958\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12476647812923425\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.11278871060452167\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1268307428927155\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.16681118581598586\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2328224374648793\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3251285718036272\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.44416172862209324\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.5905166912560145\n",
            "output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.6312294906687915\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4316655180478619\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.1982868546853903\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0071532406773248125\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18613336303264544\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.34001502874798606\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.528188718190645\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 377 lasted for 23 time steps with total reward of -14.1648753682754\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5744427692006894\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.571367868804887\n",
            "output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.5412371065840367\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.48403208672229014\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.45217440004273735\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.44526806407672104\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4630915304047295\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5055992661294483\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5729209971925024\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6653587685232185\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.5662781989728194\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.40260023097315856\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.26781021541581373\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.16089905051410658\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.08100749044613337\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.02743691318857866\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.0003440521655134754\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0026958805949150455\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.02019337160832635\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.06830804556294146\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.14180363227184656\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.24100437556836452\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.36639957274854484\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5186381938492413\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6727646812742659\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4900509688306188\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.25488462800441214\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.014069108440616596\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2562183753288705\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.47332207804082\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 378 lasted for 30 time steps with total reward of -11.191077067660302\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.48364418877905335\n",
            "output tensor([[1, 3]]) selected_action: tensor([1]) rew: 0.4802638304532926\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4493486824506878\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.39087682512508726\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3572236860193466\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3479752077493671\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3628912169623675\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.40190690981977895\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4651322390854271\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5528493793936566\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6655081746868594\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5117328769165592\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.32533646319847265\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.16759069796710513\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.0373367308679875\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.06644417277461412\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.14461803609425844\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.19789031482420216\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.22680142667315095\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2317246076678529\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.21286532035849365\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.17026169434642685\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.10378570688774846\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.013145016890283912\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.10211444090981592\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.24260480337304557\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.40909107193690175\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6024836462644018\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5577805251430652\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.32571190551236884\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.044870980094599555\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2087262853788222\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.436882696616912\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 379 lasted for 33 time steps with total reward of -13.72887079580352\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6682225713434291\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6675052735052408\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.640376953115569\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5868313627500817\n",
            "output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.5066875725892555\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3995944275592329\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.30744441243834997\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.24275917619150778\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.2050971075669905\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.194034037847722\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2093199580184788\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.25088007534764456\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3188141292502932\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.40555607118762815\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5117332187460588\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6423616418804867\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5377816146271477\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3306337482460777\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.15224104758633406\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.0013247099968891063\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.12326422919861485\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2225299078254046\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.29732215104217796\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3483309595177303\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3760837372383316\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.38094431574072324\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.3631130952055816\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.32262785269475364\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2593649675060317\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.17304099165472508\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.06321466022627475\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.07071039782554778\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.22948181382235777\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4139928752209667\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.625274335309361\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5355174050388439\n",
            "output tensor([[1, 1]]) selected_action: tensor([0]) rew: 0.26711698849034005\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.03182693693875405\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.303810304580956\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.550761879347938\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 380 lasted for 40 time steps with total reward of -13.894943063216164\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6270949746201344\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6301486321977745\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.6031999530946411\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5462666210923546\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.4591902855949782\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3416409177415973\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.25220211802397063\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.19012493572632483\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.15482584667707733\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.14589239212155092\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.16308605224170286\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.2063430635301703\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.27577360447884225\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37165950727504715\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.49445040002243223\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.632533277463498\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5766524637074258\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3688732172526552\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.1900050560122566\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.038775512307686444\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.08595843879459425\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.1851950189284013\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2597778587043167\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.3103902483447813\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.33755218537455606\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.3416192701464539\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3227827660326608\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.28107037427181675\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.21634747707781515\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.12831878508630173\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.016530496864531674\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.11962675062433481\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2809136821318815\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4682362825075784\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6826369325628409\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.47471829286531153\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.2025574375072795\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.10047300329479891\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.43577581401353\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 381 lasted for 39 time steps with total reward of -13.524363527553213\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.572438123149327\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5731836667615218\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5995484165023097\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6515354939446182\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6197844255658214\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5034560996627769\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.3572100085501858\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.18037430931907117\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.027862480053144345\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.26843125000572476\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.54235562989192\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 382 lasted for 11 time steps with total reward of -16.781118816495155\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.47022780740769965\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.4734124452366557\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.44571375937320545\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3871495610468286\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3566353872708281\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.35379738654131276\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37843780000124794\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4305360268291363\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5102475079284357\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6179005396190832\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6460091543745734\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.48082653825751775\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.28574877547588384\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.118931002914\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.020854092644651268\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.13469721943247692\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.22353983760410934\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.28816638349093804\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.32919994741880304\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.3471002889296753\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.34216335534409886\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3145217202888655\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2641455764867085\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.1908441077903169\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.09426723865677789\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.026092074457068182\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.17089467593117064\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3409506536121125\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5206973356911935\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6392336663295294\n",
            "output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.38718374197379457\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10533314881585504\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.14865005177908386\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3765350965907326\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.58000320681794\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 383 lasted for 35 time steps with total reward of -15.508729134188044\n",
            "\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4928588700578117\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.49323325121876715\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5218794927322051\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5787996701606798\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6553542008467775\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6216515519472063\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4781546886853173\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.363807517951764\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.27773168973353657\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.21920604799831944\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.1876759040058381\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.18275855951951547\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.20424605631676662\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2521058268783861\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.32647963336230923\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4276809107532309\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5561903712531887\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6809078227619009\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5021488790989542\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.28727332112232506\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.04167174942440588\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.23580490972756313\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.487769337183646\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 384 lasted for 23 time steps with total reward of -12.371758231082001\n",
            "\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4857069466834164\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4881597094002048\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4630247351239334\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4103172218874832\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3824514233346702\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3790548027716052\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3999293879435363\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.44505278931999626\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.514577124975311\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6088259634447183\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.6197030829634765\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4549627545685305\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.2603098171038265\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.09389938589961011\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.045495730286048586\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.15896603119152525\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.24745273256316902\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.3700165641644009\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.527058103324524\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 385 lasted for 19 time steps with total reward of -15.34301401610935\n",
            "\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.47257991462020343\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4740761487023799\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5037647886021319\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5616560478650587\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6479364120561818\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6370352798207676\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4927344096546845\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3184803233577489\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.17254553520172433\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.05383093202772232\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.03861687923403845\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.1055947761260656\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.14773654956039145\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.16550931565084692\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2174544849971376\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.24545319885098438\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.3079639128076855\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.40511558475789\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 386 lasted for 18 time steps with total reward of -17.298804910076438\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5196670431617343\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5205788992978797\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.49410114719029175\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.4402393243619729\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3533134769241436\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.23083898744549647\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1356954854678838\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.06708380129475239\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.024368354231976974\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.007082225656603158\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.014929709091054677\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.04778707554546058\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10570202239769416\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1888920267345195\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2899284025159039\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4059443552005193\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5456393392150868\n",
            "output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.6155009519299199\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.39701325438933643\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.20780694670552713\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.04656675056366566\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.08789963922460908\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.19664413575023854\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.28056695335476534\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3404097899375135\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.376752159537837\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3900098328296252\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.380434618396299\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3481149602422286\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.29297703774545975\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2147862403764509\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.11314906095579175\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.012484381471891737\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.16281680527793196\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3386986469280485\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5411205877358147\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5578613842217911\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.33124710055732587\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07766624093687668\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.18121063065212673\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.4175322657215\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 387 lasted for 41 time steps with total reward of -15.939912598273345\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5764441402944609\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5725889409034759\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5976224867967486\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6515183700543502\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6540742499929612\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5533274863921713\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.41125505345342783\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.23870858479578427\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09393856588684257\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.024160545747954953\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.11654740395017393\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.18402417645859792\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.22723088248171885\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.24664246277841595\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.24256767086704162\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.21514913991435192\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.16436421482682428\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.09002634704508106\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.008212960784386936\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1308614914011212\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.27858257832130806\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4331607786720537\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6053862314526351\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5189821619505869\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.2614705445708403\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.03265037628282352\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.16905422986173407\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.34511788736665044\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.49689313986272\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 388 lasted for 29 time steps with total reward of -15.902993099155287\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6838085804828993\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6809015502985655\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6483761673662239\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5862151439455031\n",
            "output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.4942253222045957\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.372042490068624\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.27824102459399\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.21205037357653067\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.17286423229563463\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.16024687098203777\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.17393649697570235\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.21384640955409018\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2800644152639542\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.372850698854285\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4926340864092995\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6400063867976851\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.584285749713824\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.37935226002950506\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.2033089603822248\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.054898235840227017\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.06700666691846979\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.16338661994224435\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.23506706163475355\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.2827123991433824\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3068231468730092\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.30773487357353446\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2856182951976809\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.24048008312450375\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.17216416119570765\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0803534489258309\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.03542781741067741\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.17581087913679572\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.34157787416296936\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5336551914422364\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6468961398537825\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.3988941409764234\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12105495810325628\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.18798125872021698\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.470869540507525\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 389 lasted for 39 time steps with total reward of -12.862725099035316\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5444005112439319\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5432903172074107\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5116961423096615\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.44960930040665836\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4159519804065178\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4103309428112215\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.43252891334441024\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.48250609746859763\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5603993254988118\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6599415579215825\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5986564639622922\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.43448863352008493\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2993300002231475\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.1921750794159579\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.11216804834238553\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.05861374320877094\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.030984458324459452\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.028923709494338756\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.052247825471898324\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.10094594674468194\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.1751787478270853\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.27527595488911394\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.40173249757870855\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5552029042909169\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6513452604104661\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.45344971726095074\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.21355478853175514\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.05735330114897674\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.30168189934381046\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.521216792350742\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 390 lasted for 30 time steps with total reward of -11.23532312472771\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.48462303771753246\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4822630687868755\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5081338942516266\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.562222380781307\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6435113157245015\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6441207024074505\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.50371248053103\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.33342256752194843\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.13244592107484704\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.041164451320309936\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.18871510269601605\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.31137839418876767\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.46834665615294\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 391 lasted for 13 time steps with total reward of -16.715149235560915\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6158770143905087\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6135660915186452\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6403777635036002\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6865537230882516\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6109728018376024\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5037148307651281\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3588114414351138\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1831455368457302\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.034917200912933544\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.08701411539932585\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18364349898857918\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.25581129032306116\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.30419770954237335\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.3293201734917592\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.33153237449038986\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.3110244539025173\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.26782383219669226\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.20179645610045216\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.11264840133606036\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 7.206335081855197e-05\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1369716782325358\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2736932684950416\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4326099527083982\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6151720271829703\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4563590781093677\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.18444321210242653\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.05904104311960878\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.27576438803753966\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.467304201953283\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 392 lasted for 29 time steps with total reward of -16.83966425440257\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5036949017041488\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.49962929166594017\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5239956658758051\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5767693838188928\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6581026610513797\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6227814626874726\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.49208281594263936\n",
            "output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.32245854626995407\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.12201134069783015\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.051217624192742894\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1985367418073143\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.32111983021169915\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.419995092940233\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 393 lasted for 13 time steps with total reward of -16.669343219437927\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6048478654774606\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.609833566233323\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6406256529014099\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6673065107180116\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5754345818350454\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4538931578838181\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3021408932305715\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11948461497618357\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.03593896857030837\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.16532682159004564\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.26973362860167704\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.3500622498725009\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.407058252236627\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 394 lasted for 13 time steps with total reward of -17.254553077615334\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5911600216204\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5917041704309054\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5653307048878387\n",
            "output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.512040814344579\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4316614485094311\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3729639608032519\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.33828927193037894\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.3310575827748632\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.35104599571242046\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.39820841093075166\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.46415951731896365\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5541290493997129\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6689360449568444\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5362392022059085\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3478843262525425\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.18824153344365868\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.056141556998734854\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.049445834469436956\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.12939889815882677\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.18443581479016896\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.21511004801309097\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.22180808711241523\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.20474877912218759\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.16398371362343933\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.0993983520135055\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.010713799111298827\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.10251069185991613\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.24087419946670052\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4051291216912627\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5961739859580444\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5849568204019314\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.3371075159165312\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.05902088811809786\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1916483619973418\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.416675302617126\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 395 lasted for 35 time steps with total reward of -12.262400155095166\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5957499682804579\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6002472658509896\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.574576627774452\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5187661695324967\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.43266804370348066\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.31596266774697146\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.22723000075490785\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1657251918575679\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.13086906562449974\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12225351042720556\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1396441817189391\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.18298121555996594\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.25237836580032824\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3481207147870815\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4706608574256326\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6206132149329305\n",
            "output tensor([[1, 7]]) selected_action: tensor([1]) rew: 0.6012541073066611\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3940307427204388\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.21581872332717256\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.0653521070607509\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.05850566005390384\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.15674618867434137\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2302061939083912\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2795616115949232\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.30532454083113814\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.307842063004783\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2872962550421261\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.24370494984272828\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.17692300316040455\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.08664401186040155\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.027597396162245436\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.16642240481927456\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.33060350592208965\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5210579711763014\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6611615884972262\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4148806791405686\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1388216900333596\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.16837061336780057\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.449329486932974\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 396 lasted for 39 time steps with total reward of -13.51500660032992\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5454847968919181\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5421205632631497\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.508297081379741\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.44399304951022556\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.3490125934588125\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2229906766950014\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12439938565688285\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.052415688954125295\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.006379116530829898\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.014202759830687084\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.00965148783892239\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.019883240712788908\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.07442320781459033\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.15416018561526584\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2594533756624461\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3908255731680419\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5314173770247184\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6653193762408108\n",
            "output tensor([[5, 4]]) selected_action: tensor([0]) rew: 0.4510352939623441\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.20709620354905667\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.06769459287995477\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3157322686273186\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.538805890619972\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 397 lasted for 23 time steps with total reward of -15.397380213706105\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6312110235145583\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6329941491511101\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6048518177798855\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5467965064295423\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4586646426953366\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.34012102407150113\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2497500695435395\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.1868005599661351\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.15068656306192474\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1409931924724963\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.15747956374585925\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2000796634138533\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2689015613113377\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3642251280886557\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4864981653083601\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6363306080461608\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5855137851287209\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3781290811485274\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.19961581677131723\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.04869948158738113\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.07576492622647651\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.17477762620466647\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.24918433956315256\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2996705971347532\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.3267588311207391\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.38870987390354\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 398 lasted for 26 time steps with total reward of -14.246523790917125\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5967195378248789\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5969462969843697\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5702552722694002\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5166478760137402\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.43142573757060754\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.36930038673242904\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3350172707246315\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.32817819954199845\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3485605423302578\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3961184683682156\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.46656600558857086\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5562274430430915\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6707286950951199\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5367260090966253\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3479638471758092\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.18791055943598944\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.05539639810076202\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.05060937140604305\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.13098553243348898\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1864508161397312\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.21755926238770684\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.22469796372009287\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2080863981154253\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.16777681334540173\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.10365535417949295\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.01544382963646973\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.09729778949979329\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.23516786246809962\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.39891808309206267\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5894463134844137\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5922136535687313\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.3449065187884476\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.06737538728305154\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.1827226353461161\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.40716043412854\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 399 lasted for 35 time steps with total reward of -12.259134256757408\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.537655253266142\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5386830348238318\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.568311606476194\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.626550536119906\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6807836113741882\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.570224297831562\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.4243561866918639\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2481313168300584\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.04073073877150979\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.13989796479349703\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.29512882119325456\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.426204590370737\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 400 lasted for 12 time steps with total reward of -16.625804794172232\n",
            "\n",
            "\n",
            " Model saved!\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5090451994882648\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5061740514901523\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.531760178503554\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5857862214901156\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6684115927882712\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6150920426418693\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.47904276365064347\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3079667168862904\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.16508752386073983\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.04932138567245159\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.04026835439302412\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.10446093749450736\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.14387199225414815\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.1589502050438446\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.14997594204405773\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.11706117989153264\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0601503457930343\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.02097811287242396\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1267094068513137\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2575891125787395\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.39994784973488906\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5574488924233171\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.5911332740185423\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3512090481159553\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.08125650752668595\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1610046754515128\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.37728276511476727\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.569189698835086\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 401 lasted for 28 time steps with total reward of -15.078256215721296\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.6334969922701835\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6321571007003437\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.66007169524877\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6623322573621078\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5862285143309417\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.47988624382945455\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.333318945258095\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.15584236199844875\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.005630571202096146\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1184723628627441\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.21747750815221178\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2922419788113258\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.3434637368647729\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.37167905764084\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 402 lasted for 14 time steps with total reward of -17.194369962131454\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5086478287479175\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5115539133665178\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5396009564315699\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5928093968973603\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6713743748892251\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5764838662682981\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.4294188006760238\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.25216123631983534\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10295191230920431\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0193374983522418\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.11569071944175335\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.1869363054744287\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.23374128242381798\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2566077368619358\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2558713893367488\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.23170145993640323\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.18410137565894408\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.11291008295292104\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.017803922675198647\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10170079111052682\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.24624416254086756\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.41661755656721544\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6086099718735761\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5612716234758249\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.30727926511662595\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.0821126016026229\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.11577363270249247\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2878213810206113\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.43534861877957\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 403 lasted for 29 time steps with total reward of -15.944807147423855\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5483337671468721\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5449609893919024\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5702947973026933\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6243152819888247\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6768964272151814\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5772822012459117\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.43907462093708394\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.2670427407809721\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.06389110542094822\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.11239598732298794\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.263159026208792\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.389605098885276\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 404 lasted for 12 time steps with total reward of -16.453068180986666\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.638858534893931\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6409253609438783\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6163053703577029\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5604612553173491\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.47236829125200996\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.35384727381813474\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.20439938170961236\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.08236833218651557\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.0132172852140503\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.08317347825113952\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.1281537112161511\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.14864491712493821\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14496567614666378\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.1172658529328906\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0655272374960551\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.010435039830905435\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.11097034159875274\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.23658942850348919\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37893460983823424\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5315369814213086\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6286283673563311\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3945378840307947\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.13062701934629928\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.10533655473059772\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.31500965766577793\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.49994963787632\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 405 lasted for 26 time steps with total reward of -15.629450536249333\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6459762857908643\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6487853143036757\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6809444707908805\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6306665472036986\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5495537804440962\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.44176995521939855\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.29396107615485323\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11135980070778045\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.04426635758446501\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.17412026251597462\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.27926229110142264\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.360601322513503\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 406 lasted for 12 time steps with total reward of -16.855233003100118\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6841689128716307\n",
            "output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 0.6885994711631254\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6656274165427747\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6028926094979581\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5104134619666537\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.38781754356793363\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2345698342452277\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.04998498504983462\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.10787477824168556\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.24023801622060237\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3481919380661739\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.43267365531878\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 407 lasted for 12 time steps with total reward of -17.304904152942104\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6855329710364708\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6865679466886119\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6579785943629236\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5997719274333823\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.5117786733545266\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.39365764375165535\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2449034755218743\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1238952776627803\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.029675867905488096\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.03855618298293362\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.08143895525711309\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.09944285439735334\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.09286856310402403\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06184656243754422\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.00633777211213038\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07386490588329786\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.17913413946558476\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3100046049256229\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4671678926685171\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6348140068563772\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5361227320079759\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.29449045918639294\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.08154927166141201\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.10418386092030285\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2640808890018484\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.399383913655832\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 408 lasted for 26 time steps with total reward of -14.637229163496189\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6535509339298021\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.6568825249704298\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6890800667811671\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6485627552628542\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5571225945553293\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4358071113188621\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.284084915498352\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.16033559442197215\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06358925436184742\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.006969317536599462\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.051992448021422444\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.0719648159460336\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06720110576843014\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.03784533561813702\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.01612861697835416\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.09491552923091584\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1988764032725301\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.32853489618979703\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.47910247753315016\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6370313387540658\n",
            "output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.5207510891310484\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.2800434677246463\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.008854792089026065\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.23514460834495565\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.453699687743956\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 409 lasted for 25 time steps with total reward of -14.211562956975385\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6464485025636\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6460160452052224\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6190114928214486\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5654296707095781\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4850908546357603\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3684816725624521\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.21630516040575964\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09158216577843303\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.006674213730015421\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.07929635180996375\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1269546988570952\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.15015362109624308\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.14922946229324074\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.12435009774997102\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0755155036453189\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0025590913494223932\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09485023675150528\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.217205156378512\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.34577780459218954\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4954255112725656\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6588066176207811\n",
            "output tensor([[ 1, 12]]) selected_action: tensor([1]) rew: 0.42869946429564487\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.1689851104493283\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.06253556897490131\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2674808666474595\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.447367734198185\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 410 lasted for 26 time steps with total reward of -15.444001744309036\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.49135398850119516\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4943424526293283\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5223355775799874\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5753560028356596\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6536009674591677\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5994400618124986\n",
            "output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.4524954695417517\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2754848122561534\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12665948169189523\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.004899355761798951\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.09077096486414221\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.16117137524497427\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2069597326840404\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.22862831211475798\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2265022858567154\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.20073953720903226\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1513313618188878\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.0781038280789858\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.019280234071306024\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.14131845756399136\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.28866502770592595\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4621250903874531\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6603527250010511\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5086905093329455\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.25069158547341963\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.021175249064776247\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.18144972572677653\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3586729738185129\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -20.511862238188698\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 411 lasted for 29 time steps with total reward of -15.847925286935219\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.4653083884486444\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.46394819894359385\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4874443296432568\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5357879289193488\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6091450949742425\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6463289189157218\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5056474803037152\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.3351984868905713\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.13417274814541358\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.039363913119326355\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.186711782636178\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.30903694046769364\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.407359609065548\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 412 lasted for 13 time steps with total reward of -16.75949067010424\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.5590993873866962\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5603147139913446\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5870858938293458\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6394192908286299\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6243811584625626\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5077643488092815\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3612808754078658\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.18425582059463386\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.024125389514417506\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.20594960818690866\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.36260514384916365\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.495349395073813\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 413 lasted for 12 time steps with total reward of -17.064428047313942\n",
            "\n",
            "output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 0.6790174892863177\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6750028437486182\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6664032063614427\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6170071252978516\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5411809649906772\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.43860081785581084\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.29962569402043415\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.12321978918604631\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0851165329713986\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.32628451567636446\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -20.60127482293291\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 414 lasted for 11 time steps with total reward of -16.972617940833477\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5352747755460654\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5331085704836683\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5562722977713115\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6047534423076462\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6787147377386533\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.568430925344701\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.4257175560579535\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.25277794495924\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.048809345343668764\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12819337040311563\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2795692046729542\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.406523974043548\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 415 lasted for 12 time steps with total reward of -16.61042695356671\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6226749597101521\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6261907897609094\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.602402570112714\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5464393244014706\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.46029188983119995\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.343636346683757\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.19598480719549227\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07568186480791456\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.01823626242847992\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.08657748295919132\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1299869076263187\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.14894301356183437\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.14375592525717262\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.11456711986622525\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06135011402128654\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.01608808620903085\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.11810478444806594\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.24521845667268127\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3789380037860911\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5322838716019336\n",
            "output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.6153641549064338\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3796564517814126\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.17323044958912248\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.0053257626606307285\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.1573107804384538\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2838904212246064\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3860850357787521\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.464761846192413\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 416 lasted for 28 time steps with total reward of -16.068603860516983\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5953816058448341\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.599250977720801\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6321178337344743\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6879937720929254\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6069359250473051\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.49417783909885804\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.34334292299965197\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.16168963275555076\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.007374189783144469\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12078687279288625\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.22383411210074766\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.30265555810971617\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.357981163497307\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 417 lasted for 13 time steps with total reward of -16.876993007423113\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6719950314725207\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6714443702776516\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6827944532236708\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6306238694996867\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5520750966688626\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.43531311150152563\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2868539026521173\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10724497925765003\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.04538870584561261\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.17223112220219688\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2743225471411212\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.35255079082813\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 418 lasted for 12 time steps with total reward of -16.806148351463374\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6281716916468824\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.626986645356532\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6517949407215057\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6469633304108845\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5597996562077956\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4428857830682962\n",
            "output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.29571382867855023\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1176219613144655\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.033221897913769005\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.15797935371502714\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.25766746809523694\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.3331496401091728\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.385130391778034\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 419 lasted for 13 time steps with total reward of -17.197210914206327\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5185358293934742\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5135755977338031\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.47792896500471416\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4706715167145875\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4915803007375128\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5406100658776194\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6113185642128243\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6762667975227189\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5413941700790269\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.37685630231839484\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2409945649448697\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.1327840790129226\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.05134949639381048\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.004024482705155541\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.0338866771491087\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.03861591282217175\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.018419562791765764\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.02666644972881571\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.0967765199551247\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.1922132013606277\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3134438943333385\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4610960426647822\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6359502750674926\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5610692977623652\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.32890864410227716\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06640244350420715\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.16871779567714884\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.378135859551268\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 420 lasted for 28 time steps with total reward of -12.315407272271306\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.6662242777638985\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.669160433423496\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6582283950681477\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5966916274747375\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5054531543871859\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.38414833867689835\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2322494060635194\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.049077453745547506\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1072846872044253\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.23805162351541392\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.3442958869851749\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.426939162477478\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 421 lasted for 12 time steps with total reward of -17.35533827357906\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.6244465228025504\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6278805612465341\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6572187792973004\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6592807521565948\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5682342168267036\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.44744515294744636\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2963802935207651\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.11435424265922761\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.04049780995685942\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.1693689733520729\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.2733096824255308\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.3532183435669343\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.40983600221091\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 422 lasted for 13 time steps with total reward of -17.250990290055185\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5152409232510724\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5150272638687207\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5400361315320453\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5902639101193724\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6658820737738136\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5785439061260647\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4355134423672208\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.26232232374627606\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.11723584372877632\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.0008486841754958285\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0928877843155228\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.15968148994170372\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.20186712755087483\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2199159869004116\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.21413194264645374\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.18465136519494885\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.13144390063928485\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.05431391569323535\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.047097398395689716\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.17330907873266788\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.32499622418654484\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5029840319522252\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6907821527872886\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.45814198105736925\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.1949510540911491\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.04002607213934806\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2484238679815306\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.431777365169342\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 423 lasted for 28 time steps with total reward of -15.367641762631855\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6492576766886559\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6467022438645875\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6175946021318622\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5619164463060203\n",
            "output tensor([[6, 1]]) selected_action: tensor([0]) rew: 0.4794751356189437\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.36165460219378887\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2664387969283657\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.1986134539502289\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1575600699381534\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.14283022001022638\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.15414881881759884\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1914152539077496\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.25470286906937156\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.34425700772451706\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.44777177562733084\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5727962181642104\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.624533145510817\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4241809640443597\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.1939506430962772\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.008206582604555479\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.18374242704991584\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.33398993415332734\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.460147519114077\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 424 lasted for 23 time steps with total reward of -13.696286519328812\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.48306337969049407\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.47846879312336366\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.44624544442662495\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.38585402223055987\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.28870716169693744\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.16007739967198348\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.058356931919916644\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.017308000694106762\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06760990636840991\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.09307503625828067\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.09406086671118108\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.07075528208243476\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.02317693411585503\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.04882350868315638\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.13748350165020506\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.23996503417199744\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.36507499769388696\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5135953568632038\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6082127179063304\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.37755421289627134\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.11707989753334014\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.11541948573987076\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.3215748694788194\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.50291525634639\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 425 lasted for 24 time steps with total reward of -16.597333277637077\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4691876964082732\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4650200153726791\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4298965534832685\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.36379397243225964\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.26651552882365637\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.19669897559264904\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.15371116190391143\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1370886107157986\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.14654100843637924\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1819525073669933\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.243381343345336\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3310580015658494\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.44481220626776874\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5673688586593572\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6436186252615334\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.44535690093726155\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.21730015145446202\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.01741869164327403\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1557183547811788\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.3034219511158877\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.485152012349754\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 426 lasted for 21 time steps with total reward of -15.22357150857611\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.510673276044772\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.508523656732947\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.47564564945023446\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.41202850085912046\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37656982735947253\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3688669950476968\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.38869306935516446\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4359982890807683\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5109092979825915\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6063443871984796\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.655082030746179\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.4948861582444062\n",
            "output tensor([[3, 5]]) selected_action: tensor([1]) rew: 0.3049066934272431\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.08422950275975144\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.10920998251579306\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.27684655246084466\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.41999024379406\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 427 lasted for 17 time steps with total reward of -14.672689444481872\n",
            "\n",
            "output tensor([[ 0, 22]]) selected_action: tensor([1]) rew: 0.5571325768741864\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5614485428371303\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5912432805641947\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6465435481202894\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6339533517621232\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.514346254773575\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.36487109724524214\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.1848355210865661\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.03243071665442082\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.09350801818979981\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.19400200349017016\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2699191098325403\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3219678337502537\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.35069424597240617\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3564808358897082\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.33954654321664074\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.2999475018029358\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.23757822365117925\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.1521731315423952\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.04330851650345108\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.08959483787054073\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.24726794917945216\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.416487836671408\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5941349975037599\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5216386217800297\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.2547885580032083\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.01665885969862768\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.19437112594141293\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.37982605708121514\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.541112647305507\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 428 lasted for 30 time steps with total reward of -17.54705924354486\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5548037580239347\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5528504657445713\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.520534387534638\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.45784499983398697\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.36459680670145034\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2404344619442199\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.14385974386768585\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.07406882136973525\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.030421211351560784\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.012445284969489545\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.01984111313447956\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.05248141189397132\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11041107297920205\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.19384551130012662\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.30316782443003243\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.43892453094078765\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6018194271334072\n",
            "output tensor([[2, 9]]) selected_action: tensor([1]) rew: 0.6072951286647386\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.38743042492704993\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.1966187054414753\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.03352402982927777\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10306683761917546\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21422737541004278\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.3008803320926431\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.421925064472948\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 429 lasted for 25 time steps with total reward of -15.142880487578989\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.632661984177085\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6317230047939911\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6567487743185988\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.660511356574161\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5734255121753496\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4566158421798051\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.30957362996534343\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.13163616657947536\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.07799779883816027\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.2614828779205101\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.42022860937432\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 430 lasted for 11 time steps with total reward of -16.706813015369182\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5222559794985824\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.523427705541997\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.49722971513637515\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.44366890607341747\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.35821265099864863\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.29507971642900055\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.259295639492755\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2504496963790873\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2683059997772492\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.31280460369230234\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3803667022810904\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4660196957236008\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5759383015766875\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.6379111401908685\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.45509711886179394\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.24248182394082718\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.05812186468235969\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.09931841874008968\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2310468556882157\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.33812899093818394\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.479606369854746\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 431 lasted for 21 time steps with total reward of -14.601433374944595\n",
            "\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5007927736001411\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.4966856099738196\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.465058981564589\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4049408252536414\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3672049960746704\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3571196883037112\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.374440193203052\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4142171487069908\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.47677845774222183\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5638314505017635\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6758237482331538\n",
            "output tensor([[ 1, 14]]) selected_action: tensor([1]) rew: 0.5241365724675061\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.33776796061995096\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.18005881023810766\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.04985293624988152\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.05386556768498152\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.131959753600832\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.18513197492603206\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2139194216501114\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2186919796503456\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.25781400756356987\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.33130569305850077\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3812826933815802\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.40827501341202\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 432 lasted for 24 time steps with total reward of -15.993535952194772\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.46850725673530447\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4679690089058003\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4955174232978655\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5446295216407168\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.618341734639271\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6524845437421682\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5108917698996529\n",
            "output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.33952087121795615\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1375592976238722\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.03693278667089528\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.18526093658265497\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.3085968226717052\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.407966554016674\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 433 lasted for 13 time steps with total reward of -16.703335672239323\n",
            "\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5338733810229674\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5354248860521981\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5097477917772301\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5094692370176893\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5344117597428317\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5845742566355817\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6601306789799296\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5693219923281408\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4254414644273785\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.31052713227561635\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.22369173908377327\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.16420521475900562\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.13150371373171532\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.12519497565529125\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.14506098669911105\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.19105861993677276\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.26331864940542105\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3621432660032359\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4880019697990182\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6415254660490401\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5471019289064369\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3614689878944721\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12339862182152012\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.09093942215694517\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.27941071565894127\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.443458707520684\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 434 lasted for 26 time steps with total reward of -11.87321212533219\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6047826423609273\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6020185816735468\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5691714162530398\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5062267700319109\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4129949094175729\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.28911577804947\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.1931214555415144\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12421897903998702\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.08177906551607089\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06534194618811928\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.07462042476547165\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10950092221493962\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.1700429897986887\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2564775114417325\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.36920357163812323\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5087837285274271\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6759371966699509\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5284697919957265\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.30343741144610326\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.10696411107055226\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.06234473553457659\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.20576305953036134\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.3244275755790464\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.419327368522666\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 435 lasted for 24 time steps with total reward of -14.459653535525776\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.462098524845153\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.46500138223198273\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.49495589806899065\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5476140656936853\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6252830351296937\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6419519892742589\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.49692947876228244\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.32206226089097145\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11651846054111514\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.061675735245705965\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.21385700573355265\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.3412300384525106\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.444855716712933\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 436 lasted for 13 time steps with total reward of -16.88920340070657\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6142470871185299\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.61577617705344\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6464573213763737\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6627534070004362\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5827087546354365\n",
            "output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.47620490298530727\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.33644451150342813\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2154670638931474\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12170879267098383\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.05437566259905441\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.012837431639580066\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.003367491641234732\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.0054703540092488345\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.03923215351202669\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.09797021206987444\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.18190628948548548\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.27690199484165023\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.39277746513474066\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5322665089100422\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6201974462733144\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.40129423662037667\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.15258144974519228\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.06808655128678526\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2622695205145443\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.43141864035855\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 437 lasted for 25 time steps with total reward of -13.725562980723446\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5715237213533493\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5750814166194825\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.551632687906457\n",
            "output tensor([[5, 2]]) selected_action: tensor([0]) rew: 0.501200433256135\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4236330840533129\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37120744964011976\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3433798128975676\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3397775384259034\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.36020234404526985\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.40463120078288517\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.473215249854811\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5662768458960635\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6566023303499665\n",
            "output tensor([[4, 3]]) selected_action: tensor([0]) rew: 0.49330161406573014\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.30033921415981624\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.07678044093556952\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.119443999248107\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.28978153378723603\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.435556337492034\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 438 lasted for 19 time steps with total reward of -13.835996486284937\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5201759001802421\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.5174034686421987\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.48405630680491674\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4201185586870707\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.38449643252733867\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.37678591850651255\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3967587511246796\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.44436399405391636\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5197273065848789\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6231480560422662\n",
            "output tensor([[2, 6]]) selected_action: tensor([1]) rew: 0.6402840072362364\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.48380580714719257\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3519194189119883\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.24826546921728077\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.17201327043405468\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.12249374585791978\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.09920629979738121\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.10182261527378966\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.13018822052321222\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.18432237744912183\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2644165767278762\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.37083167291434793\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5040934517404597\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6582420044505729\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5459565190948953\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.32746446363355086\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.07854838081926996\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1429943580502192\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3387697816984857\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.510274929449864\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 439 lasted for 30 time steps with total reward of -11.021130074815396\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6747786294395929\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6710812571906576\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6935648214697341\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6407107225534676\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.55532501713348\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.4400455674162298\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2943790342958601\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.1767469355403981\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.08622338940030522\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.022039525251822567\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.016408766066391134\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.029556385520527084\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.017666913685034646\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.019167849089238076\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.08102710805999813\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.16815924081944056\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2809788543393067\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.42006246096872557\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5861422804266878\n",
            "output tensor([[1, 8]]) selected_action: tensor([1]) rew: 0.6199025515445279\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.39705831407925996\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.20333662932783914\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.03738491972730945\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.10202880036975198\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.21599829548486071\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.36383008072837014\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.546038628952775\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 440 lasted for 27 time steps with total reward of -14.223412762733828\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.45669954179024286\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4516764763828648\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.47150600085997263\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5161575465337489\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5857757518889076\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6535403097395986\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5159783998533474\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.3487562516123943\n",
            "output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.15108155265096412\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.07796731836620463\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2806015637750956\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.458343311858716\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 441 lasted for 12 time steps with total reward of -16.665740362687977\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5833829820665126\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5797612141673735\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5491865875000094\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4916376341065536\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4069191175360668\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2933650256046372\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.19593607718495343\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12541001817154662\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.08114075333418819\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.06265100956358799\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.06963535582817137\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1019612610870908\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.15966868830168018\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.23121903401658994\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3243669204735482\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.44080001278332115\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5812121687045249\n",
            "output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.5579159917397316\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.33780689553856524\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.14668307609636488\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.01679463980189705\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.15384253577995954\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.265537119692107\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.35280428882264847\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -20.416412916898462\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 442 lasted for 25 time steps with total reward of -14.884731677190056\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5144363426458765\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5187522714326575\n",
            "output tensor([[7, 2]]) selected_action: tensor([0]) rew: 0.49246710900653945\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4356058399240751\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.40711972467660995\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4066504874632185\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4340163430026427\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4892130292728032\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5724125058401334\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6839594068470252\n",
            "output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.575348732730792\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4057014979056778\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.26455439715677764\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.15114164493912696\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.0645585483831822\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.004058935164260813\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.03093791800413631\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.04084342310732775\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.025897968381862257\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.013829279305950182\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.07843943492899846\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.16820196080626554\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2835516580562858\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.42508428871506854\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.579196848400522\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6101539860532874\n",
            "output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.38499898623438955\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.12983893690777232\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.09752610497164638\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.2987146145948165\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.47524038803313\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 443 lasted for 31 time steps with total reward of -11.87586822129298\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6625456890196126\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6647749857871972\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.693823278362736\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6397326989567268\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5520220560321569\n",
            "output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.43149022007110616\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.28049010834797583\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.15740150187622187\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06125788264211507\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.008752288125648933\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.053277531435529435\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.07279870832902235\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.06762675547114305\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.037902066486856234\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.01640495057281577\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09549237127191218\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1997243199108253\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.32962740192628337\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.47207779341556677\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6303263190727882\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5190566909616862\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.27818413289628185\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.06596425405407341\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.11908389653233542\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2783296611027841\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.413012190156035\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 444 lasted for 26 time steps with total reward of -14.300386442461274\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5784303111181368\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5807606037760173\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5528729662239186\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4947835571552208\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4063328376598576\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.2871900636797837\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.13686146231229557\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.04529740436539653\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.20129348093258415\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.3323523880485131\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.439557322714773\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 445 lasted for 11 time steps with total reward of -17.981268794136035\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6419716568196543\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6471593799042328\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6816614856074515\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6326330017910194\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5498164593164014\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.43776522662600037\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.28396838023046\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09901117007302684\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.059014499718557156\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.19132720219523047\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.29900394559944954\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.382971066411095\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 446 lasted for 12 time steps with total reward of -16.958329953556085\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6116299969726737\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6132292283628443\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5847811056638124\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.526294400359806\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.43760205639919103\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37748152524370515\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.34537647280373784\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3409027158429858\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.36385198401211993\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4141930910294078\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.49207093768749854\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5978034748126708\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6659134030980202\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5050643380338207\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3122244232404402\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.14777718920732175\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.010514919177668669\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.100632576296672\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18658526670700354\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.24810505166588742\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.3439573095933855\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.41639037792417\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 447 lasted for 22 time steps with total reward of -13.948959320239393\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.654398875116461\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6552405564433608\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6821329711970732\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6227532352408393\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.533798042238722\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.414914628922218\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.2655861371776439\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.08514314118431854\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12722022907531005\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.31372517225954794\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.47581271133148\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 448 lasted for 11 time steps with total reward of -17.0027905251457\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4876365319723849\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.48858575936404247\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4586709004664713\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.45698704991944794\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.48334521975108\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5377341057930798\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6109905155258208\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6685623416976489\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5284046047830925\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.35854517525443397\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.21729396609345497\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.10359181621421903\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.016527027138063588\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.04465375015007311\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.08053883327291744\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.09154782260580413\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.13633642088685155\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21495690944929985\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.3276209026644924\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.474690475209034\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 449 lasted for 20 time steps with total reward of -15.953470100265232\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6358817628128653\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.639389958051099\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6129910877276583\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5567061136105499\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.47037979153910303\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.35368491838003413\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.26521851973895705\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.2042407004665301\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.17017728847171054\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.16262553355834042\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.18135700538497213\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.22631839229089235\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.29763061571256944\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.3955864051508169\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5206462220198425\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6734321709562946\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5452807097488446\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.33457662852726633\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.1525365397356669\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0021426248234016065\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1306356566682937\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.23397392364573627\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.3130358128372596\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.36854121009681\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 450 lasted for 24 time steps with total reward of -13.64966886418749\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.656517882077341\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6523849664850823\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6207277967409186\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5569070701921991\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4630173319494799\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.3386870770050331\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.1833831323712054\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.05540428323880231\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.04626463582444057\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.12248564966661601\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.1739601626411087\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.2012247604701195\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.20464925429803266\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.18443620795677418\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.19871535976784238\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.24740160229981284\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.330567517643388\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.448439311316324\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 451 lasted for 18 time steps with total reward of -18.6311149218244\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6296125758095895\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6270440507208488\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.650436624744615\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6652673640726402\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.580440948938002\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4659306430678237\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.32123884549925796\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.14571302815477882\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0024545689715905206\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12440700452370701\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.22114185519479557\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.2935020812904393\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.3421707698499658\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.3676685398263396\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.370352683538936\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 452 lasted for 15 time steps with total reward of -17.63601342218822\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4555240220668496\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.4579609820510596\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4326975283643312\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3797481034511334\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.35151362309813616\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.34761769053823877\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3678583155818417\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4122089139607038\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4808172700579564\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5740025792462523\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6352857460085775\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4721425306867255\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.27924655152008115\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.11476986987615811\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.022496619676076024\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.13362472262179986\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2195361550692091\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.28099462206180537\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.3186013599753166\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.33279303641437646\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.38178052956944003\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.46562663145513\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 453 lasted for 22 time steps with total reward of -16.39405995033511\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.4836987206854003\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4863318601571941\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5139443750359063\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5665538944917758\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6443527098195253\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6056525592663077\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.46005703923387276\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2844445311341236\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.077986547066493\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.10131586122903741\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.25481299218162873\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.383723449350388\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 454 lasted for 12 time steps with total reward of -16.616830065870456\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5993319232616016\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6030147186523543\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6356991623697494\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6902412620713843\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6090376903043445\n",
            "output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.4911205107734511\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3404530893483728\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.21805562255862898\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12297353143367556\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.05440720728924903\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.01172061234115751\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.0055536653601186425\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.002288126183888739\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.035121662154507016\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09299398952922822\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.17612188636378645\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.28488912291201907\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4121798401574054\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5515775502855268\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6287291173311771\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.41041174105294986\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.16225185691233412\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.057896193203669344\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.25159208275085604\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.42028665111493\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 455 lasted for 25 time steps with total reward of -13.602708369142784\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5935203565354668\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5899218084335982\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5561631713905961\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.49222433298308754\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.3979100546462272\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2728551692624328\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.17557456589502107\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1052637782523485\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06128157830448333\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.04315577613293081\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.050586264746623644\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.08344608290178784\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1417809899104432\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.225807788470066\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3359113901507319\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4726403854909025\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6356685083949583\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5710537212802139\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.34963307041567304\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.157060935300474\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.008019302288828078\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1468420338167052\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.26050265856813065\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.40822520221856\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 456 lasted for 24 time steps with total reward of -14.512129467994157\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.689895141850495\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6941251685445066\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6720045325827556\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6235596140574106\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5403304619794149\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4255444746638066\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.28022164094917323\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.16277180118243106\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07226455979250185\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.007926533938198643\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.030851144765732585\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.04450856280438026\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.03331502798147856\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.0026313357245419944\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06340281988393415\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.14924037012104874\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.26055078928664255\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.39790264743790116\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5443281565656103\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6462238196631451\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4258221504512343\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.17564701183042675\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.0464566044512949\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.24205892958346553\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.412622877873886\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 457 lasted for 25 time steps with total reward of -13.975420116955059\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5893339562718277\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5917458540296735\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6230880189118181\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6833787782980392\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6237054261723963\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5082437624580379\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.35925993166497827\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.17955183237261285\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.02729538383925928\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.09867918437873197\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.19939811717882638\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.27573442799921777\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.32840225668330375\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.357954031146765\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 458 lasted for 14 time steps with total reward of -17.074565073368202\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.522524602080773\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5244653467322961\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.49910105077304456\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.44644611249858923\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.36634109220598676\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.25845723119962916\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.17376488246299637\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.10775199735342889\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.06780532569930281\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.05347539130199497\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.06448462005004196\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.09864523922089657\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1499128690569393\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.22349977303796353\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3198064364625114\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.43939473077374014\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5829817829031453\n",
            "output tensor([[ 1, 10]]) selected_action: tensor([1]) rew: 0.5364352892627909\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3124505035250147\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.11736773384502275\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.05017922241219652\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1914356309031371\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.3075086824835559\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.3993565249331842\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.4677818067675\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 459 lasted for 25 time steps with total reward of -15.551149857053467\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.544651726333019\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5463994970697433\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.520930129592762\n",
            "output tensor([[13,  1]]) selected_action: tensor([0]) rew: 0.4682567809481599\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.38821863423191616\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.33214364716650346\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.29873363118052354\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.29248039029732675\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.31209464891964334\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.35254457264424366\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4167844595688802\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5051148838342933\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6180031763540179\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5748898644694261\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3875323943641037\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.22917990305823532\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.09868651500204728\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.0049542060496513\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.08259590467021966\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.13493098152518757\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.16248567397973585\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.22396526259672078\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3194942174867508\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.391190554845579\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.43973206581104\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 460 lasted for 25 time steps with total reward of -14.872704011930042\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5438394576422944\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5394434932668961\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5637295990184066\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6166710293665023\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6843273487550912\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5859099449714219\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.45021049206879793\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.2794477898722122\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.07762676466341656\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.15616846411401342\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.36430527022244\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 461 lasted for 11 time steps with total reward of -16.179267814711416\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.570281862236925\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.571672410876201\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5459737943393831\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.49319568211272147\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.40705672481876565\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3458056999698014\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3122388591429879\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.3059608012414391\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.32675158458363596\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3745678504988209\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.444580693041671\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5338548883291496\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6478259252178896\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5587030244861709\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3704029006246097\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.21095739648788547\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.07920691402310076\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.025869038161303093\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10513867455456882\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.15930963896471917\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18892421346275406\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.25263310649339465\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.3505700691704903\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.483021968737727\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 462 lasted for 24 time steps with total reward of -14.466429697513796\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4951875086022929\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4937414267298408\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5173684374111505\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5660610158901802\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.6399867105397261\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6055909551797349\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.463315467949199\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.29104794780148036\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.08797971790476627\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.08785908119239938\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.23779083797991268\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.36300443262621984\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.464543965630373\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 463 lasted for 13 time steps with total reward of -16.99291912942053\n",
            "\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.490944538928454\n",
            "output tensor([[7, 1]]) selected_action: tensor([0]) rew: 0.4890983564291317\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.45648585963432986\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4521931622040606\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.47601406076040875\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5279200640849359\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6080594206272314\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6794500317065459\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5455076846758408\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3780771167640342\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.23927979184915005\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.12807321276037215\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.0435633843765324\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.014984497792074836\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.04813950003792333\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.056301357213865144\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.039698943003659615\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.0016098085575026455\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.06773688866450855\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.15896223245186875\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.27573067813497576\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.41864755956757005\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5813884103666026\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6138899868357077\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.3874021234883763\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.13091113231982088\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.1568277606766723\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.41835529728449\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 464 lasted for 28 time steps with total reward of -12.583361850820722\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5972262034069801\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5993739172504426\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6305278440947001\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6899433108893404\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6098267927228304\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5009396163732652\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.35186677824039914\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.17200700196377638\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.019529235348639484\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.10673883056853906\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.20782597032328154\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.2846078498716168\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3378014887127177\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.367962501475958\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 465 lasted for 14 time steps with total reward of -17.133695940661738\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5523748726356054\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5557735553045862\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5871619636294875\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6419581595868696\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6619798669611466\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5432158284979817\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.39468728870162084\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.21570333445542278\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.005434289249815538\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.23706797233594318\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.454293027113682\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 466 lasted for 11 time steps with total reward of -16.533071840427088\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4703273927968711\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.46662224623079196\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4910624798733245\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5436228951762359\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6224219572314783\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6661163150237902\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5275988780554333\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.35934204915991086\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.16054949301004884\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06970343722830658\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.27365237439053725\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.45283367384135\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 467 lasted for 12 time steps with total reward of -16.48852577890231\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5579866580200131\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5616099916650469\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5380813043347158\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.48742308461611084\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.40637320747364314\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2878427359906436\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.1379611717537244\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.015011067686913016\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.08199666521686355\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1538962265884275\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.20136048894208342\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.22489753494895137\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.22484919744416915\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.20139090260270098\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.15453235890392847\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.08411884850010354\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.010166928625441607\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12780145346006938\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.24914481378551812\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.393032932733574\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.560387194003893\n",
            "output tensor([[ 1, 11]]) selected_action: tensor([1]) rew: 0.5382574415594109\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.2854659401921923\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.06158379677915332\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.13492106960734768\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.30547678514659804\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -20.4513868382261\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 468 lasted for 27 time steps with total reward of -17.00069719344721\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6525004054605168\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6545005304319039\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6826213564153305\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6218282289887606\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5312523925115313\n",
            "output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.41074073405299416\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.25976830657911754\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.13671513400933033\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.04061455575818029\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.02934513080073492\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.0738126145571088\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.09326891598990794\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.08802511426712767\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.058221723375570744\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.0038292471366226377\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.07535031887192156\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.17968105022099456\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.309689521549649\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.46605968795649066\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.64962559050907\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.538638967226532\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.29763484411610175\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.0852765738672116\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.09991745316427608\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.25931724845036147\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.39416265385008\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 469 lasted for 26 time steps with total reward of -14.507401903066157\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.67622113191388\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6742378132443627\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6738521579866331\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6167444508249411\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5299837071047713\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.41323344309745247\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.26599242017045077\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.14666198966173538\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.05430000116393274\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.011879370959562985\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.05249790471555854\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06800890712667751\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.05869510697772412\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.024668165835516564\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.03413062863393018\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.11792868444823673\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.22711941740438923\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.36225831377087114\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5240572878358878\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6866233432991635\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.46878617082925866\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.22131241005591995\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.002069230700043889\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19048788608054945\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.3577948328690741\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.559342298042285\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 470 lasted for 26 time steps with total reward of -14.62786187046109\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4796845662190341\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.47904220961267774\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.45081403370132034\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3949977961404152\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.36398702742643774\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.35738903324759064\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3749853501683529\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.416732978010934\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.48276352929409216\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5733804385203607\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6429614303938415\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4818625430757689\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.2910446408364956\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.12869570474895026\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.006375836958818215\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.11522312769336163\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1987482436972976\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.316083065968495\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.467612239311812\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 471 lasted for 19 time steps with total reward of -15.185701232233514\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.505690950858956\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5082030792564615\n",
            "output tensor([[9, 1]]) selected_action: tensor([0]) rew: 0.4832877066996666\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.43095985416528404\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.40364944698231353\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.40098796512651147\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.4227816279366824\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.469012452417808\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5398371378382403\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6355838833001438\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5797143555092261\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4132428083125418\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.27578755782873265\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.1663253447538694\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.08398133439409422\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.0280404211854432\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.0020457514868438276\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.00665486680039884\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.014007785855809751\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.059909433466212414\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.13118868769217285\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.22815325921303753\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3512763876555961\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5011916299503208\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6786854212339345\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5022330244466576\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.2797482040671442\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.013454370137575333\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.22590392940212417\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.440060398504578\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 472 lasted for 30 time steps with total reward of -11.56773081590951\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5825260506782766\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5835806601123482\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5576456172652267\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5047256021076572\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.42272439286892094\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3014577082181059\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.20797534542036789\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.14149912898212436\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10141525388256606\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.0872797929631206\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09882152111581144\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.13594279096950485\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.19871891611558679\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.28739625944771285\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.39606853417025323\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.519393114714748\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6671050998814841\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.48820010402455416\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.2606693139679678\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.061604723620440616\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.11040855895944268\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2566659813017762\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.37832695152839046\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.476404419792406\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 473 lasted for 24 time steps with total reward of -14.617055981055238\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6805964120290507\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6835408463511747\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6568766140852348\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6006220166566146\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5146188793397295\n",
            "output tensor([[2, 2]]) selected_action: tensor([0]) rew: 0.3985367068606994\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.25188017516676464\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1330466189326881\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.04109454575723942\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.02476104648707239\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.06514112284080292\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.08049831455353429\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.07111489666084492\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.03710234825097358\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.02159793623097722\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10521297984135974\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.21413557532427485\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3489204189204991\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5102785739226359\n",
            "output tensor([[4, 2]]) selected_action: tensor([0]) rew: 0.6923561826995455\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.48370960694668363\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.23694020315253705\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.018497208151638855\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.17315371998220697\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3394380451092593\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.539876139847905\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 474 lasted for 26 time steps with total reward of -14.738624133363253\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.49803489362541487\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.5018311057696248\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5307033503381547\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5846774559833873\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6639538375791072\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5826685541168981\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4351471425152049\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2574729601006399\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.04881221125522034\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.13286587897053426\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.2889328378015845\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.42062946106284\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 475 lasted for 12 time steps with total reward of -16.739126666551307\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.47973859972357014\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.47914000368568577\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5067169982854262\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5602218213334552\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6341220617871355\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6406700406432921\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4986972349297216\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3268596610346398\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12434571990803284\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.10978188157978197\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.3178126516404545\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.501317820588877\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 476 lasted for 12 time steps with total reward of -16.678400212478152\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4895498553902097\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4903357973655038\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4636269532178402\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.40943053731496537\n",
            "output tensor([[5, 1]]) selected_action: tensor([0]) rew: 0.3275804221333337\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.21774190508007663\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1318851029636144\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.06482631068940847\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.02202155955763707\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.004519820571440292\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.046487826701758495\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.13109023187272495\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.24953598825741918\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.40222352904224\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 477 lasted for 14 time steps with total reward of -18.20781931159011\n",
            "\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5151655150754211\n",
            "output tensor([[5, 3]]) selected_action: tensor([0]) rew: 0.5112471755670293\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.476683605586141\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4705558383100107\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.49264364067411026\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.542904512833414\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6214729154986793\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6713429146569359\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5350667032054108\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.36906285976036834\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.2316664541173724\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.1218452935579043\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.038716458100694406\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.01844323042007351\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.05019081674765469\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.056913728547992704\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.038828326807201\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.004020105790041861\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.013278977811660764\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.04770963371383535\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.1073718233418528\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.19249541692120575\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3034774884929009\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.44087797444765386\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6054134299048894\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6020518238258199\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3805180963491155\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.12886741710373179\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.15411337750134302\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.41095129161247\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 478 lasted for 30 time steps with total reward of -12.304984696990534\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4715184290942763\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.46827674217786863\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.43737000123080405\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.37877937027174524\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3448693496610947\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3352256051722673\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.34960757731131975\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3879498877129208\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4503617098159902\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5371242788441319\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6486864465910169\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5414466836459766\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.3557476993218701\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.19885249883294298\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.06961522640084933\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.03296956370790227\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.10975411168958588\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.16142942538674743\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18852067226154717\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1913849367160967\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2284436612834566\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2997080564741171\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -20.40534706567491\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 479 lasted for 23 time steps with total reward of -15.642125987109289\n",
            "\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.47444210960127087\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.47261467085226405\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4415633777655795\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.37748290540540896\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3413025635149799\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3326124508271696\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3511780765138023\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.39694176900718114\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4640955068340986\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5522205295578846\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6651738743915165\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5430835398963224\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3561708070124482\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.19800991590803418\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.06744414463580356\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.03654341124104238\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.11481700189927141\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.16808018030830185\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.19687119239023115\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.25981250239543674\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3570346705544382\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.488821085116864\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 480 lasted for 22 time steps with total reward of -15.58764380218182\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.46362403889075077\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4652502949767685\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.49178423790437964\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5432343907400272\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6197840273950079\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6170278848568196\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4736499242415788\n",
            "output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.30037017278594\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09636647706945484\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.08033243151227165\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.23105680463456835\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.35700424596314617\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.459228118626044\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 481 lasted for 13 time steps with total reward of -17.056530151875304\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5737198922208708\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5716992902143799\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5426983518956859\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4867038468914163\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.403528658311913\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.34540815246274803\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3117575020981306\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.30216176803226336\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3163795811704184\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3543444325510453\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4161640216200845\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5021178357365386\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6126528691258961\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5774415784785917\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.39354089116521573\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.2386774545202075\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.11172554642136245\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.011700294538522893\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.062229233592689615\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.11073175296321186\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.13430895472507653\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.13329312628265477\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.10784644022615342\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.05796141744240052\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.0165376926356417\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.11599265313119955\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.2409079319537959\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.3919459668185604\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5699204105231483\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5813948289372284\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3759288546766094\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.12436272478189886\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.1720925783437265\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.442625176685286\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 482 lasted for 34 time steps with total reward of -11.731675649347826\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.5847314777353099\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5829555204603009\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6100599202806037\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6660350245416178\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6489521116089784\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5345616825550392\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.39028805298396463\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.21547013905828127\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.06833429168552729\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.052247065127881354\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.14725580517005832\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.2759527801991389\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.4387834031281\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 483 lasted for 13 time steps with total reward of -16.612850832715555\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5372646427938388\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5354613957774773\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5031641860848419\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4403620995039279\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3468695046313258\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.28140032042148233\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.24335662476808895\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.23231145002570552\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.24801248673770793\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2903833996479911\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3595232263965412\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.45570404709120294\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5793668441829383\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6688847882172424\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4882936814667789\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.27796171836272554\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.09596315560406182\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.05901813403095357\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18816922057819985\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.29253376405484555\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.431183206578943\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 484 lasted for 21 time steps with total reward of -14.386620753529064\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4833370335248097\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4816014553740271\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5048042900216375\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.552933750205068\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6261530947850295\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6411901626551137\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5006369504957198\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.33024746148481854\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.12921531283871696\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.04440088916803042\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.19190480861734482\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.31446588411242876\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.471292093063767\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 485 lasted for 13 time steps with total reward of -16.77194416357663\n",
            "\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5136126506472678\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5180879783793345\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.49510712275930635\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4407416732983849\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.41248402697142794\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.41212655693379563\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4394888177118125\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4905563269205975\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5634383844179152\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6613141795791532\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5713074303914651\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4019375228594161\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.26146810237226725\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.14885463695351642\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.06319999358583822\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.003765806289549367\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.030020469780798198\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.03856160662310065\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.02208883984945137\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.019338337698021124\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.08583119624505714\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.17766973556929122\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.29529950485082723\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.43932696282463624\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6105128426179829\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5877304620948623\n",
            "output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.3618859342395687\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.10328221759132727\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.12781336325203912\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.33305622194464984\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.572281865129977\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 486 lasted for 31 time steps with total reward of -12.045453962777394\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6389708251971755\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6392534200180786\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6096821320871639\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.550256870608187\n",
            "output tensor([[3, 2]]) selected_action: tensor([0]) rew: 0.4608016193333875\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3409690606431195\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.24933432562688607\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.18513472640743844\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.14777234917001275\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.1368199956394634\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.15202427678599972\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.19330659398556083\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2607624562624239\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.354659313667396\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.47543283237433953\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.623681290095441\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5998424741069645\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.3942423629281805\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.21763267083880855\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.06875909169845423\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.05350214102783951\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.150128983788531\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.22194382938839324\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3278716070687939\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -20.46823223475456\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 487 lasted for 25 time steps with total reward of -13.922340108553636\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5667393584537221\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5631562575790089\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.5325463340844929\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4748856172796354\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.38997653221842976\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.27745304687394634\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.12290480628795936\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.008026229545132713\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.11318177526868145\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1934540407361262\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3078387641193948\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.456702399790654\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 488 lasted for 12 time steps with total reward of -18.151541256682794\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5991146886743014\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5972947216254721\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6212821986071414\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.671066063136257\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5940437714450879\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4788739099899708\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.3336521813279708\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.1577197506152359\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.049722263119154286\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.28959253320682127\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.56290282968758\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 489 lasted for 11 time steps with total reward of -16.849170340592117\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.537232311573306\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5385731637503863\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5094513489580424\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.44987442106121056\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.41878479471208596\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.41580814630052376\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4407464395156325\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4935792249459746\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.5744625476834907\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.683725579968735\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5760774380577578\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4104647195521526\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.27159067551800664\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.1604755745144179\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.07623019677630877\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.01812459424987617\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.014405124817950532\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.02175303509022236\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.004141761296476643\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.03837764123585086\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.10592464956663522\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.19878719982830528\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.31741847435394865\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.46243222690584695\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6155448170509507\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5651778235480944\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.33584626962822095\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.0762529562775936\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.15584834609765058\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3621202591378033\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.60239623630639\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 490 lasted for 31 time steps with total reward of -11.86970152721314\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.49211817335714114\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4873881414027422\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4551017922852594\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.39523300540279116\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3043139523959185\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.17598016409780348\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.07463929780009848\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.0005588204067055735\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.050302947688686706\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.0751153271706213\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1337415608925337\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.22630430673786123\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.35308216101613\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 491 lasted for 13 time steps with total reward of -18.454330597170785\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6048218329790178\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.607793932546462\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6365225645106476\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6874855654242779\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5972885478799331\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.47750518156944044\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.32760400816590807\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.14690068303350057\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.006441597795074028\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.13360011180721398\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.23560887661661234\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.371644527992274\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 492 lasted for 12 time steps with total reward of -16.661372798101986\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.48694363481539205\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4854219312408551\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.45634823060661\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3997121439840362\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3153303971754108\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.25221622190955806\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.21215603989937148\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.19871165223387266\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.21161647315859455\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.25031201288665894\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3068441991614992\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3867449146326033\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4904347380070645\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6184962058929553\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5506822497578676\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.34636150703612645\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1709211848800024\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.023106251641785813\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.09820718209765689\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.19399692162213605\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2650851910194562\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.3121330728073768\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3356376701631476\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.33593106852696336\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.31318043776374216\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.267388848205671\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.19839657765654484\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1058828704699073\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.01063171551080938\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.15178206285946239\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3183538532451119\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5112768358013555\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6300765368872504\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.41388811442472573\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.1406230039518221\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.16944732076235547\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.453433678807404\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 493 lasted for 37 time steps with total reward of -14.70972872830156\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.4924452066799708\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.4929053827942178\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5217196766718573\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5788940166343605\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6646107401953777\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6120986354295083\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.47674362409617665\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.30261758447787845\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.09757872859083544\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.08037329569067003\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2325840523218426\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.41863035696461\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 494 lasted for 12 time steps with total reward of -16.49197410940694\n",
            "\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6474918481708377\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.6500688144429562\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6819032945504859\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6569880012483086\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5664122067580128\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4460048963186142\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.2952379088377861\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.11343094581914343\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.10023224446845813\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.28796693261390827\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.509561972374375\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 495 lasted for 11 time steps with total reward of -16.840223233310596\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5639101508516768\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5595475138522147\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5248642398449729\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4598333643329833\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3642532101700704\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2968354713608501\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.25697062044827135\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2442197182247492\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.25831840949184515\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.2991784431944061\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.36688721098825716\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.46170551298456763\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5840634876893565\n",
            "output tensor([[2, 4]]) selected_action: tensor([1]) rew: 0.6654456196010886\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.48607443265385164\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.33608296964158546\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.2143903824541823\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.12005843301725122\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.052304727486867486\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.01051114980767659\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.005771201146601512\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.0031804360990920766\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.03726115982691758\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.09653775180372409\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1812468941938975\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.29179222215407097\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.4287399525776393\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5928126042228075\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6062095400101911\n",
            "output tensor([[2, 1]]) selected_action: tensor([0]) rew: 0.39405272234083344\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: 0.1428632490854524\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.08064056786453033\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.27806559701836436\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.45091203373521\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 496 lasted for 34 time steps with total reward of -10.915237799353356\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.523894897953749\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5210793701264235\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.488687463225812\n",
            "output tensor([[4, 1]]) selected_action: tensor([0]) rew: 0.4250662818454799\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.33057223322810825\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.26389342443715136\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.22441927524446836\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.21170972916400843\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.2254988796961389\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.265696286066999\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3323864606693965\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.41617421405332966\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5200633085669645\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6484797406056433\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5282810328641616\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.3219904803586704\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.14441177405851813\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.005726237427119973\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.12956395714439628\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.22809632389393641\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.30216352264082114\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.35244560638416605\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.43743997664376\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 497 lasted for 23 time steps with total reward of -15.063130771969174\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5779836530614867\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5776372343614896\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5470427742465517\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4861974660910404\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3949231855526427\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.27287138604192573\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.11953174096293101\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.06575493211321015\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.22504394644870762\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.41801451509637\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 498 lasted for 10 time steps with total reward of -17.73262595334022\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6879596371238333\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6907937513004551\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6672452056747311\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6123090782779552\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5259846975141763\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.409552595687694\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.2625168782335219\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: 0.08422831054825725\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.12609968088137824\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.31070008850013714\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.471002825819642\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 499 lasted for 11 time steps with total reward of -16.967212440840534\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6435920512189642\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6394915561264711\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6646796844698845\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6658816042388489\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5911178461411795\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.48338512467095796\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.3395382921664255\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.16481075630830283\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.04158408526696966\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.22177637924527943\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.37715396956326\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 500 lasted for 11 time steps with total reward of -16.448017518734474\n",
            "\n",
            "\n",
            " Model saved!\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6861081408676586\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6893396443614426\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6629548875967585\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6069733370511704\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5212379722627825\n",
            "output tensor([[1, 2]]) selected_action: tensor([1]) rew: 0.4054194237423713\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2590234485887998\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.08140223645010963\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.06931713377695309\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.194307311252497\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3530142291638698\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.546015732290027\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 501 lasted for 12 time steps with total reward of -17.25019531556225\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6594820822110314\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.660360935174278\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6315175262477637\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5729570646963504\n",
            "output tensor([[2, 3]]) selected_action: tensor([1]) rew: 0.4845085986308585\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3658294689326066\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.2164131928196703\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09462349625811844\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.0005057610567857918\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.06978528742999834\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.11386288214529783\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.19165087391040025\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.30336012182656685\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.44935301249701\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 502 lasted for 14 time steps with total reward of -17.442825573895384\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.504437915951473\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5057269260224859\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5353929956417569\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5934445263265372\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6800663000462084\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6043845073429924\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.4593850574489131\n",
            "output tensor([[3, 1]]) selected_action: tensor([0]) rew: 0.28425684030552867\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.07817970651090389\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10085517291552987\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2541991087536541\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.441418320259718\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 503 lasted for 12 time steps with total reward of -16.551197826332103\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5425254120888432\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5459351432223667\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5747170251783545\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6288921620189167\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.656428410539756\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5381585879124018\n",
            "output tensor([[8, 1]]) selected_action: tensor([0]) rew: 0.390149745279895\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.21171364821113114\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.06106373084365835\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.06294819550501679\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1613262693817643\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2933647986200797\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.45952617304118\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 504 lasted for 13 time steps with total reward of -16.82758157125272\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6493420901569354\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6531867013323518\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6272387165766056\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5715202300110058\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.4858769820776303\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.3699825312498205\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2233458553342733\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.04532287266076396\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10598829092727213\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2317706099273774\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.391420118627845\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 505 lasted for 11 time steps with total reward of -17.10336304008311\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6649762825018778\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6629638407101429\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6312350602985288\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.569777543327487\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.47840303092080094\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.35675218462725733\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.20430283704932828\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: 0.07939301900980411\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.01896609517778225\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.09160901916804037\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1977251854010087\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.33766374844564917\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.51191652722717\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 506 lasted for 13 time steps with total reward of -17.510076776974426\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6116744034755802\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6145948012459838\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6433771924987781\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.660829876241073\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5702418140181922\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.4499614134644492\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2994576154216092\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: 0.11804686357461741\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.0361242330379159\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1642416146378275\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.325850416563703\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.521552284708275\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 507 lasted for 12 time steps with total reward of -17.079584569007437\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5695795218538253\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5664772544747911\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5362761963430013\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.47850761690927657\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4428206909704141\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.43527409162239183\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4556415122418078\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.49954937629949514\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5664843090362521\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6584722657665861\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5820924061561221\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.41941011574617126\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.28568996880115693\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.1799306901292509\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.10128111914058208\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.04905102732495642\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.02271778572822114\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.021930034576133572\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.046509209080574265\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.09644949179008083\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.17191649972986967\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.2732447710644713\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.40093388362968574\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5556428084172765\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.647361053866678\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4504985476911286\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.2093236268534211\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.06288465980317115\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.367379383824247\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 508 lasted for 29 time steps with total reward of -10.707198168383794\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6905938689611554\n",
            "output tensor([[ 0, 20]]) selected_action: tensor([1]) rew: 0.6939786496189558\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6674774528021484\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6042772199828386\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5113248043088269\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3882496179546515\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.23451840220567066\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10847753353118134\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.009136275780658065\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.0643421225509998\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1711867283420731\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3117519492857652\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.486535382671168\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 509 lasted for 13 time steps with total reward of -17.12578235770392\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.642047417255816\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6422729489451581\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6684973052162843\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6568599294616823\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5689155441346578\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.4511951660104122\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.30318472609734815\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.12421675312404468\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.02755344038498081\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.15329749535919862\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2540430421859704\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.388916354776708\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 510 lasted for 12 time steps with total reward of -16.766620542461453\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5192222415800699\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5226500912396659\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5544987891628649\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6094688013048091\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.688758863120183\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5783198042474854\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.43081643502132183\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.25307068776614405\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.10331561970207531\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.01958374270486607\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.11661804277056381\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.18862307157886193\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2945981341642532\n"
          ]
        }
      ],
      "source": [
        "manual_seed(SEED)\n",
        "\n",
        "if WANDB:\n",
        "    wandb.init(project='BioLCNet', entity='singularbrain', config=network_hparams)\n",
        "net = LCNet(time,**network_hparams, reward_fn=RLTasks('CartPole-v0'), wandb_active = WANDB)\n",
        "net.learn(**network_hparams)\n",
        "plot_locally_connected_weights(net.connections[('input','main1')].w, net.n_channels1, net.filter_size1,\n",
        "                                                compute_size(net.crop_size, net.filter_size1, net.stride1), net.connections[('input','main1')].locations,\n",
        "                                                net.crop_size ** 2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_locally_connected_weights(net.connections[('input','main1')].w, net.n_channels1, net.filter_size1,\n",
        "                                                compute_size(net.crop_size, net.filter_size1, net.stride1), net.connections[('input','main1')].locations,\n",
        "                                                net.crop_size ** 2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KmwqostVRPJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gym Playground"
      ],
      "metadata": {
        "id": "9M7NvCJlr0E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "intensity = 400\n",
        "crop_size = 90\n",
        "time = 100+300+50\n",
        "def frame_process(x):\n",
        "        x[x<0.99] = 2.0\n",
        "        x[x<=1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "screen = env.render(mode='rgb_array')\n",
        "screen = screen.transpose((2, 0, 1))\n",
        "_, screen_height, screen_width = screen.shape\n",
        "screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "view_width = int(screen_width * 0.6)\n",
        "world_width = env.x_threshold * 2\n",
        "scale = screen_width / world_width\n",
        "cart_location = int(env.state[0] * scale + screen_width / 2.0)\n",
        "if cart_location < view_width // 2:\n",
        "    slice_range = slice(view_width)\n",
        "elif cart_location > (screen_width - view_width // 2):\n",
        "    slice_range = slice(-view_width, None)\n",
        "else:\n",
        "    slice_range = slice(cart_location - view_width // 2,\n",
        "                        cart_location + view_width // 2)\n",
        "# Strip off the edges, so that we have a square image centered on a cart\n",
        "screen = screen[:, :, slice_range]\n",
        "# Convert to float, rescale, convert to torch tensor\n",
        "screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "screen = torch.from_numpy(screen)\n",
        "print(screen.shape)\n",
        "plt.imshow(screen.cpu().permute(\n",
        "        1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "h = screen.shape[1]\n",
        "w = screen.shape[2]\n",
        "print(h*0.75)\n",
        "resize = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize([80, 180]),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "            #transforms.Resize([80, 80]),\n",
        "            #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "            # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "            #transforms.CenterCrop((crop_size, crop_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "            transforms.Lambda(frame_process),\n",
        "            #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "            transforms.Lambda(lambda x: x * intensity),\n",
        "            transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "device = 'cuda'\n",
        "\n",
        "# Resize, and add a batch dimension (BCHW)\n",
        "screen = resize(screen)\n",
        "# print(screen.shape)\n",
        "print(int(h*0.8), int(w*0.8))\n",
        "print(screen.shape)\n",
        "# screen = screen.to(device)\n",
        "# plt.imshow(screen.cpu().permute(\n",
        "#         1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "\n",
        "# print(a.shape)\n",
        "a = screen.sum(axis=0)\n",
        "\n",
        "a = a.to(device)\n",
        "plt.imshow(a.cpu().numpy().squeeze(), cmap='gray')\n",
        "obs, reward, done, info = env.step(0)\n",
        "obs, reward, done, info "
      ],
      "metadata": {
        "id": "JDFJB9GeldHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K83DiktzRLGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BindsNet Breakout"
      ],
      "metadata": {
        "id": "9jFzOzYIFdGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/BindsNET/bindsnet.git\n"
      ],
      "metadata": {
        "id": "9Qi4v29mDF3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ],
      "metadata": {
        "id": "6ROgQ2ZWA-VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install matplotlib==2.1.1\n",
        "\n",
        "### Restart your runtime after this"
      ],
      "metadata": {
        "id": "_K7CuZmqFTny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "wONJ1L1uEGMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bindsnet.encoding import bernoulli\n",
        "from bindsnet.environment import GymEnvironment\n",
        "from bindsnet.learning import MSTDP\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes\n",
        "from bindsnet.network.topology import Connection\n",
        "from bindsnet.pipeline import EnvironmentPipeline\n",
        "from bindsnet.pipeline.action import select_softmax\n",
        "import time\n",
        "\n",
        "# Build network.\n",
        "network = Network(dt=1.0)\n",
        "\n",
        "# Layers of neurons.\n",
        "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
        "middle = LIFNodes(n=100, traces=True)\n",
        "out = LIFNodes(n=4, refrac=0, traces=True)\n",
        "\n",
        "# Connections between layers.\n",
        "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
        "middle_out = Connection(\n",
        "    source=middle,\n",
        "    target=out,\n",
        "    wmin=0,\n",
        "    wmax=1,\n",
        "    update_rule=MSTDP,\n",
        "    nu=1e-1,\n",
        "    norm=0.5 * middle.n,\n",
        ")\n",
        "\n",
        "# Add all layers and connections to the network.\n",
        "network.add_layer(inpt, name=\"Input Layer\")\n",
        "network.add_layer(middle, name=\"Hidden Layer\")\n",
        "network.add_layer(out, name=\"Output Layer\")\n",
        "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
        "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
        "\n",
        "# Load the Breakout environment.\n",
        "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
        "environment.reset()\n",
        "\n",
        "# Build pipeline from specified components.\n",
        "environment_pipeline = EnvironmentPipeline(\n",
        "    network,\n",
        "    environment,\n",
        "    encoding=bernoulli,\n",
        "    action_function=select_softmax,\n",
        "    output=\"Output Layer\",\n",
        "    time=100,\n",
        "    history_length=1,\n",
        "    delta=1,\n",
        "    plot_interval=None,\n",
        "    render_interval=None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(pipeline, episode_count):\n",
        "    for i in range(episode_count):\n",
        "        total_reward = 0\n",
        "        pipeline.reset_state_variables()\n",
        "        is_done = False\n",
        "        render_counter = 0\n",
        "        while not is_done:\n",
        "            # if render_counter % 10 == 0:\n",
        "            #     # plt.title(\"Game image\")\n",
        "            #     # plt.imshow(pipeline.env.render())\n",
        "            #     # plt.show()\n",
        "            #     a = pipeline.env.render()\n",
        "            #     print(a)\n",
        "            #     time.sleep(0.1)\n",
        "                \n",
        "            render_counter += 1\n",
        "            result = pipeline.env_step()\n",
        "            pipeline.step(result)\n",
        "\n",
        "            reward = result[1]\n",
        "            total_reward += reward\n",
        "            \n",
        "            is_done = result[2]\n",
        "        print(f\"Episode {i} total reward:{total_reward}\")\n",
        "    pipeline.env.close()\n",
        "\n",
        "print(\"Training: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)\n",
        "\n",
        "# stop MSTDP\n",
        "environment_pipeline.network.learning = False\n",
        "\n",
        "print(\"Testing: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)"
      ],
      "metadata": {
        "id": "y_jx13I5-VTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PLVXCK0Djg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7fTSvrK3T_GA",
        "ULGGHW43UksI",
        "MBKedMpIleMr",
        "8clxN_npa1WY",
        "sCXSLZZoGS4z"
      ],
      "machine_shape": "hm",
      "name": "BioLCNet_CartPole.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "87ae7d1e75b14a98f2d7b99b6b39b40721989d38e6517f9dbec64ca4d8e3011b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18fa4a7a31484692bc940427393999b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b046ca6d23b4314b0e085511e83f892",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_969408a1d3354d48a387ede9e28b72be",
              "IPY_MODEL_8e60db1d8527468d994e959139ddeede",
              "IPY_MODEL_4df8f476b64f448b85f89c98d7ebab77"
            ]
          }
        },
        "4b046ca6d23b4314b0e085511e83f892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "969408a1d3354d48a387ede9e28b72be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e641dbefe7f64b9e897bf6573ac98ac6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Episode: 511, Number of steps: 14, Episode Total Reward: -16.79:   5%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89f3a7998e5441dc93e518d516d944ba"
          }
        },
        "8e60db1d8527468d994e959139ddeede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1f5c7238957f444daa403dc39eda101b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 510,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf89107616ef445a8fd14cca56577515"
          }
        },
        "4df8f476b64f448b85f89c98d7ebab77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_76e9193de65b45d1aec85df45011ae85",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 510/10000 [1:37:01&lt;22:29:00,  8.53s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85ae4b78711f4054abe26d9cc580634b"
          }
        },
        "e641dbefe7f64b9e897bf6573ac98ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89f3a7998e5441dc93e518d516d944ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f5c7238957f444daa403dc39eda101b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf89107616ef445a8fd14cca56577515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76e9193de65b45d1aec85df45011ae85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85ae4b78711f4054abe26d9cc580634b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}