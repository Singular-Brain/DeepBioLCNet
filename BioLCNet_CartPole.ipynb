{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/DeepBioLCNet/blob/main/BioLCNet_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTSvrK3T_GA"
      },
      "source": [
        "#Notebook setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXtgP_iEPE0G"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXcXvvsXcOlv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "0QFC6xL2uAaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4l3AVRbGS4Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.image import AxesImage\n",
        "from torch.nn.modules.utils import _pair\n",
        "from matplotlib.collections import PathCollection\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from typing import Tuple, List, Optional, Sized, Dict, Union\n",
        "import math\n",
        "import random\n",
        "from torchvision.transforms.functional import crop\n",
        "# from ..utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights\n",
        "\n",
        "import gym\n",
        "import tkinter\n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFGNAecpT-Lj"
      },
      "outputs": [],
      "source": [
        "from bindsnet.network.nodes import Nodes\n",
        "import os\n",
        "### import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, Tuple, Optional, Sequence\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "from bindsnet.datasets import MNIST\n",
        "from bindsnet.encoding import PoissonEncoder\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes, IFNodes\n",
        "from bindsnet.network.topology import Connection, MaxPool2dLocalConnection\n",
        "from bindsnet.network.topology import LocalConnection, LocalConnectionOrig\n",
        "from bindsnet.network.monitors import Monitor, AbstractMonitor, TensorBoardMonitor\n",
        "from bindsnet.learning import PostPre, MSTDP, MSTDPET, WeightDependentPostPre, Hebbian\n",
        "from bindsnet.learning.reward import DynamicDopamineInjection, DopaminergicRPE, RLTasks\n",
        "from bindsnet.analysis.plotting import plot_locally_connected_weights,plot_locally_connected_weights_meh,plot_spikes,\\\n",
        "plot_LC_timepoint_spikes,plot_locally_connected_weights_meh2,plot_convergence_and_histogram,plot_locally_connected_weights_meh3\n",
        "from bindsnet.analysis.visualization import plot_weights_movie, plot_spike_trains_for_example,summary, plot_voltage\n",
        "from bindsnet.utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGGHW43UksI"
      },
      "source": [
        "## Sets up Gpu use and manual seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiUmFrpcUfmR"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device =  torch.device(\"cuda\")\n",
        "    gpu = True\n",
        "else:\n",
        "    device =  torch.device(\"cpu\")\n",
        "    gpu = False\n",
        "\n",
        "def manual_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SEED = 2045 # The Singularity is Near!\n",
        "manual_seed(SEED)\n",
        "WANDB = False\n",
        "\n",
        "torch.set_num_threads(os.cpu_count() - 1)\n",
        "print(\"Running on Device = \", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKedMpIleMr"
      },
      "source": [
        "# Custom Monitors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqpsr2a1WV"
      },
      "source": [
        "## Reward Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M44GJ65GleMs"
      },
      "outputs": [],
      "source": [
        "class RewardMonitor(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: None,\n",
        "        batch_size: int = 1,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.time = time\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # if time is not specified the monitor variable accumulate the logs\n",
        "        if self.time is None:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        self.recording = []\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if \"reward\" in kwargs:\n",
        "            self.recording.append(kwargs[\"reward\"])\n",
        "        # remove the oldest element (first in the list)\n",
        "        # if self.time is not None:\n",
        "        #     self.recording.pop(0)\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clxN_npa1WY"
      },
      "source": [
        "## Plot Eligibility trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SshGlRwpa1WZ"
      },
      "outputs": [],
      "source": [
        "class PlotET(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        i,\n",
        "        j,\n",
        "        source,\n",
        "        target,\n",
        "        connection,\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.connection = connection\n",
        "\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if hasattr(self.connection.update_rule, 'p_plus'):\n",
        "            self.recording['spikes_i'].append(self.source.s.ravel()[self.i].item())\n",
        "            self.recording['spikes_j'].append(self.target.s.ravel()[self.j].item())\n",
        "            self.recording['p_plus'].append(self.connection.update_rule.p_plus[self.i].item())\n",
        "            self.recording['p_minus'].append(self.connection.update_rule.p_minus[self.j].item())\n",
        "            self.recording['eligibility'].append(self.connection.update_rule.eligibility[self.i,self.j].item())\n",
        "            self.recording['eligibility_trace'].append(self.connection.update_rule.eligibility_trace[self.i,self.j].item())\n",
        "            self.recording['w'].append(self.connection.w[self.i,self.j].item())\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        fig, axs  = plt.subplots(7)\n",
        "        fig.set_size_inches(10, 20)\n",
        "        for i, (name, p) in enumerate(self.recording.items()):\n",
        "            axs[i].plot(p[-250:])\n",
        "            axs[i].set_title(name)\n",
        "    \n",
        "        fig.show()\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = {\n",
        "        'spikes_i': [],\n",
        "        'spikes_j': [],\n",
        "        'p_plus':[],\n",
        "        'p_minus':[],\n",
        "        'eligibility':[],\n",
        "        'eligibility_trace':[],\n",
        "        'w': [],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YGE1XjvIkZ"
      },
      "source": [
        "## Kernel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-4hp2V46vOUv"
      },
      "outputs": [],
      "source": [
        "class AbstractKernel(ABC):\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Base class for generating image filter kernels such as Gabor, DoG, etc. Each subclass should override :attr:`__call__` function.\n",
        "        Instantiates a ``Filter Kernel`` object.\n",
        "        :param window_size : The size of the kernel (int)\n",
        "        \"\"\"\n",
        "        self.window_size = kernel_size\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PL2L6_ABwBH4"
      },
      "outputs": [],
      "source": [
        "class DoGKernel(AbstractKernel):\n",
        "    def __init__(self, kernel_size: Union[int, Tuple[int, int]], sigma1 : float, sigma2 : float):\n",
        "        \"\"\"\n",
        "        Generates DoG filter kernels.\n",
        "        :param kernel_size: Horizontal and vertical size of DOG kernels.(If pass int, we consider it as a square filter) \n",
        "        :param sigma1 : The sigma parameter for the first Gaussian function.\n",
        "        :param sigma2 : The sigma parameter for the second Gaussian function.\n",
        "        \"\"\"\n",
        "        super(DoGKernel, self).__init__(kernel_size)\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        \n",
        "    def __call__(self):\n",
        "        k = self.window_size//2\n",
        "        x, y = np.mgrid[-k:k+1:1, -k:k+1:1]\n",
        "        a = 1.0 / (2 * math.pi)\n",
        "        prod = x*x + y*y\n",
        "        f1 = (1/(self.sigma1*self.sigma1)) * np.exp(-0.5 * (1/(self.sigma1*self.sigma1)) * (prod))\n",
        "        f2 = (1/(self.sigma2*self.sigma2)) * np.exp(-0.5 * (1/(self.sigma2*self.sigma2)) * (prod))\n",
        "        dog = a * (f1-f2)\n",
        "        dog_mean = np.mean(dog)\n",
        "        dog = dog - dog_mean\n",
        "        dog_max = np.max(dog)\n",
        "        dog = dog / dog_max\n",
        "        dog_tensor = torch.from_numpy(dog)\n",
        "        # returns a 2d tensor corresponding to the requested DoG filter\n",
        "        return dog_tensor.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zBUT0IUZDXxW"
      },
      "outputs": [],
      "source": [
        "class Filter():\n",
        "    \"\"\"\n",
        "    Applies a filter transform. Each filter contains a sequence of :attr:`FilterKernel` objects.\n",
        "    The result of each filter kernel will be passed through a given threshold (if not :attr:`None`).\n",
        "    Args:\n",
        "        filter_kernels (sequence of FilterKernels): The sequence of filter kernels.\n",
        "        padding (int, optional): The size of the padding for the convolution of filter kernels. Default: 0\n",
        "        thresholds (sequence of floats, optional): The threshold for each filter kernel. Default: None\n",
        "        use_abs (boolean, optional): To compute the absolute value of the outputs or not. Default: False\n",
        "    .. note::\n",
        "        The size of the compund filter kernel tensor (stack of individual filter kernels) will be equal to the \n",
        "        greatest window size among kernels. All other smaller kernels will be zero-padded with an appropriate \n",
        "        amount.\n",
        "    \"\"\"\n",
        "    # filter_kernels must be a list of filter kernels\n",
        "    # thresholds must be a list of thresholds for each kernel\n",
        "    def __init__(self, filter_kernels, padding=0, thresholds=None, use_abs=False):\n",
        "        tensor_list = []\n",
        "        self.max_window_size = 0\n",
        "        for kernel in filter_kernels:\n",
        "            if isinstance(kernel, torch.Tensor):\n",
        "                tensor_list.append(kernel)\n",
        "                self.max_window_size = max(self.max_window_size, kernel.size(-1))\n",
        "            else:\n",
        "                tensor_list.append(kernel().unsqueeze(0))\n",
        "                self.max_window_size = max(self.max_window_size, kernel.window_size)\n",
        "        for i in range(len(tensor_list)):\n",
        "            p = (self.max_window_size - filter_kernels[i].window_size)//2\n",
        "            tensor_list[i] = fn.pad(tensor_list[i], (p,p,p,p))\n",
        "\n",
        "        self.kernels = torch.stack(tensor_list)\n",
        "        self.number_of_kernels = len(filter_kernels)\n",
        "        self.padding = padding\n",
        "        if isinstance(thresholds, list):\n",
        "            self.thresholds = thresholds.clone().detach()\n",
        "            self.thresholds.unsqueeze_(0).unsqueeze_(2).unsqueeze_(3)\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "        self.use_abs = use_abs\n",
        "\n",
        "    # returns a 4d tensor containing the flitered versions of the input image\n",
        "    # input is a 4d tensor. dim: (minibatch=1, filter_kernels, height, width)\n",
        "    def __call__(self, input):\n",
        "\n",
        "        # if input.dim() == 3:\n",
        "        #     input2 = torch.unsqueeze(input, 0)\n",
        "        input.unsqueeze_(0)\n",
        "        output = fn.conv2d(input, self.kernels, padding = self.padding).float()\n",
        "        if not(self.thresholds is None):\n",
        "            output = torch.where(output < self.thresholds, torch.tensor(0.0, device=output.device), output)\n",
        "        if self.use_abs:\n",
        "            torch.abs_(output)\n",
        "        return output.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXSLZZoGS4z"
      },
      "source": [
        "# Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3DIAdG1mGS4z"
      },
      "outputs": [],
      "source": [
        "def plot_LC_timepoint_spikes(spikes: torch.Tensor,\n",
        "    timepoint: int,\n",
        "    n_filters: int,\n",
        "    in_chans: int,\n",
        "    slice_to_plot: int,\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    im: Optional[AxesImage] = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (10, 10),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(n_filters)))\n",
        "    sel_slice = spikes[timepoint].view(in_chans, n_filters, conv_size, conv_size).cpu()\n",
        "    sel_slice = sel_slice[slice_to_plot, ...].view(n_filters, conv_size, conv_size)\n",
        "    spikes_ = np.zeros((n_sqrt*conv_size, n_sqrt*conv_size))\n",
        "    filt_counter = 0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for n1 in range(n_sqrt):\n",
        "        for n2 in range(n_sqrt):\n",
        "            filter_ = sel_slice[filt_counter, :, :].view(conv_size, conv_size)\n",
        "            spikes_[n1 * conv_size : (n1 + 1) * conv_size, n2 * conv_size : (n2 + 1) * conv_size] = filter_\n",
        "            filt_counter += 1\n",
        "            ax.axhline((n1 + 1) * conv_size, color=\"g\", linestyle=\"-\")\n",
        "            ax.axvline((n2 + 1) * conv_size, color=\"g\", linestyle=\"--\")\n",
        "    ax.imshow(spikes_, cmap='Greys')\n",
        "    return spikes_\n",
        "    \n",
        "def plot_FC_response_map(lc: object,\n",
        "    fc: object,\n",
        "    ind_neuron_in_group: int,\n",
        "    label: int,\n",
        "    n_per_action: int,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param fc: FC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input\n",
        "    :param scale_factor: determines intensity of activation map  \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\t\n",
        "    ind_neuron = label * n_per_action + ind_neuron_in_group\n",
        "    w = fc.w[:,ind_neuron].view(reshaped.shape[0]//lc.kernel_size[0],reshaped.shape[1]//lc.kernel_size[1])\n",
        "    w = w.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*w, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_activation_map(lc : object,\n",
        "    spikes: torch.tensor,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot an activation map of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param scale_factor: determines intensity of activation map \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "    spikes = spikes.sum(0).squeeze().view(lc.conv_size[0]*int(np.sqrt(lc.out_channels)),lc.conv_size[1]*int(np.sqrt(lc.out_channels)))\n",
        "    x = scale_factor * spikes / torch.max(spikes)\n",
        "    x = x.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*x, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def reshape_LC_weights(\n",
        "    w: torch.Tensor,\n",
        "    n_filters: int,\n",
        "    kernel_size: Union[int, Tuple[int, int]],\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    input_sqrt: Union[int, Tuple[int, int]],\n",
        ") -> torch.Tensor:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Get the weights from a locally connected layer and reshape them to be two-dimensional and square.\n",
        "    :param w: Weights from a locally connected layer.\n",
        "    :param n_filters: No. of neuron filters.\n",
        "    :param kernel_size: Side length(s) of convolutional kernel.\n",
        "    :param conv_size: Side length(s) of convolution population.\n",
        "    :param input_sqrt: Sides length(s) of input neurons.\n",
        "    :return: Locally connected weights reshaped as a collection of spatially ordered square grids.\n",
        "    \"\"\"\n",
        "    k1, k2 = kernel_size\n",
        "    c1, c2 = conv_size\n",
        "    i1, i2 = input_sqrt\n",
        "    c1sqrt, c2sqrt = int(math.ceil(math.sqrt(c1))), int(math.ceil(math.sqrt(c2)))\n",
        "    fs = int(math.ceil(math.sqrt(n_filters)))\n",
        "\n",
        "    w_ = torch.zeros((n_filters * k1, k2 * c1 * c2))\n",
        "\n",
        "    for n1 in range(c1):\n",
        "        for n2 in range(c2):\n",
        "            for feature in range(n_filters):\n",
        "                n = n1 * c2 + n2\n",
        "                filter_ = w[feature, n1, n2, :, :\n",
        "                ].view(k1, k2)\n",
        "                w_[feature * k1 : (feature + 1) * k1, n * k2 : (n + 1) * k2] = filter_\n",
        "\n",
        "    if c1 == 1 and c2 == 1:\n",
        "        square = torch.zeros((i1 * fs, i2 * fs))\n",
        "\n",
        "        for n in range(n_filters):\n",
        "            square[\n",
        "                (n // fs) * i1 : ((n // fs) + 1) * i2,\n",
        "                (n % fs) * i2 : ((n % fs) + 1) * i2,\n",
        "            ] = w_[n * i1 : (n + 1) * i2]\n",
        "\n",
        "        return square\n",
        "    else:\n",
        "        square = torch.zeros((k1 * fs * c1, k2 * fs * c2))\n",
        "\n",
        "        for n1 in range(c1):\n",
        "            for n2 in range(c2):\n",
        "                for f1 in range(fs):\n",
        "                    for f2 in range(fs):\n",
        "                        if f1 * fs + f2 < n_filters:\n",
        "                            square[\n",
        "                                k1 * (n1 * fs + f1) : k1 * (n1 * fs + f1 + 1),\n",
        "                                k2 * (n2 * fs + f2) : k2 * (n2 * fs + f2 + 1),\n",
        "                            ] = w_[\n",
        "                                (f1 * fs + f2) * k1 : (f1 * fs + f2 + 1) * k1,\n",
        "                                (n1 * c2 + n2) * k2 : (n1 * c2 + n2 + 1) * k2,\n",
        "                            ]\n",
        "\n",
        "        return square\n",
        "\n",
        "def plot_semantic_pooling(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_weights(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        #print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            lc.kernel_size[0],#n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt*lc.conv_size[0] * lc.kernel_size[0],#n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            lc.kernel_size[0],#,n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt*lc.conv_size[1] * lc.kernel_size[1],#n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color=color, linestyle=\"--\")\n",
        "            \n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywXyWP0I83Au"
      },
      "source": [
        "# Design network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PcU9FSsVi4Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7099b91f-7b3e-451c-8ee6-169f573a5558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 65.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "WANDB = True\n",
        "if WANDB:\n",
        "    !pip install -q wandb\n",
        "    !wandb login\n",
        "    import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8bZpJmlrJDa9"
      },
      "outputs": [],
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "def convergence(c):\n",
        "    if c.norm is None:\n",
        "        return 1-torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2\n",
        "    else:\n",
        "        mean_norm_factor = c.norm / c.w.shape[-1]\n",
        "        return  1-(torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2)\n",
        "\n",
        "        \n",
        "class LCNet(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: int,\n",
        "        n_actions: int,\n",
        "        neuron_per_action: int,\n",
        "        in_channels : int,\n",
        "        n_channels1: int,\n",
        "        n_channels2: int,\n",
        "        filter_size1: int,\n",
        "        filter_size2: int,\n",
        "        stride1: int,\n",
        "        stride2: int,\n",
        "        maxPool1: bool,\n",
        "        maxPool2: bool,\n",
        "        online: bool,\n",
        "        deep: bool,\n",
        "        reward_fn,\n",
        "        n_neurons: int,\n",
        "        pre_observation: bool,\n",
        "        has_decision_period: bool,\n",
        "        local_rewarding: bool,\n",
        "        nu_LC: Union[float, Tuple[float, float]],\n",
        "        nu_LC2: Union[float, Tuple[float, float]],\n",
        "        nu_Output: float,\n",
        "        dt: float,\n",
        "        crop_size:int ,\n",
        "        nu_inh_LC: float,\n",
        "        nu_inh: float,\n",
        "        inh_type,\n",
        "        inh_LC: bool,\n",
        "        inh_LC2: bool,\n",
        "        inh_factor_LC: float,\n",
        "        inh_factor_LC2: float,\n",
        "        inh_factor:float,\n",
        "        single_output_layer:bool,\n",
        "        NodesType_LC,\n",
        "        NodesType_Output, \n",
        "        update_rule_LC,\n",
        "        update_rule_LC2,\n",
        "        update_rule_Output,\n",
        "        update_rule_inh,\n",
        "        update_rule_inh_LC,\n",
        "        wmin: float,\n",
        "        wmax: float ,\n",
        "        soft_bound,\n",
        "        theta_plus: float,\n",
        "        tc_theta_decay: float,\n",
        "        tc_trace:int,\n",
        "        normal_init:bool,\n",
        "        mu: float,\n",
        "        std:float,\n",
        "        norm_factor_inh_LC: bool,\n",
        "        norm_factor_LC,\n",
        "        norm_factor_LC2,\n",
        "        norm_factor_out,\n",
        "        norm_factor_inh,\n",
        "        trace_additive,\n",
        "        load_path,\n",
        "        save_path,\n",
        "        LC_weights_path,\n",
        "        LC2_weights_path,\n",
        "        confusion_matrix,\n",
        "        lc_weights_vis,\n",
        "        out_weights_vis,\n",
        "        lc_convergence_vis,\n",
        "        out_convergence_vis,\n",
        "        thresh_LC,\n",
        "        thresh_FC,\n",
        "        num_episodes,\n",
        "        max_steps,\n",
        "        wandb_active = False,\n",
        "        batch_size=1,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructor for class ``BioLCNet``.\n",
        "\n",
        "        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n",
        "        :param n_neurons: Number of excitatory, inhibitory neurons.\n",
        "        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n",
        "        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n",
        "        :param dt: Simulation time step.\n",
        "        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n",
        "            respectively.\n",
        "        :param reduction: Method for reducing parameter updates along the minibatch\n",
        "            dimension.\n",
        "        :param wmin: Minimum allowed weight on input to excitatory synapses.\n",
        "        :param wmax: Maximum allowed weight on input to excitatory synapses.\n",
        "        :param norm: Input to excitatory layer connection weights normalization\n",
        "            constant.\n",
        "        :param theta_plus: On-spike increment of ``(adaptive)LIFNodes`` membrane\n",
        "            threshold potential.\n",
        "        :param tc_theta_decay: Time constant of ``(adaptive)LIFNodes`` threshold\n",
        "            potential decay.\n",
        "        :param inpt_shape: The dimensionality of the input layer.\n",
        "        \"\"\"\n",
        "        manual_seed(SEED)\n",
        "        super().__init__(dt=dt, reward_fn = None, online=online)\n",
        "        kwargs['single_output_layer'] = single_output_layer\n",
        "        kwargs['dt'] = dt\n",
        "        kwargs['n_labels'] = n_actions\n",
        "        kwargs['neuron_per_actionn'] = neuron_per_action\n",
        "        self.true_label = 0\n",
        "        self.dt = dt\n",
        "        self.intensity = kwargs['intensity']\n",
        "        self.reward_fn = reward_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_fn.network = self\n",
        "        self.reward_fn.dt = self.dt\n",
        "        self.n_actions = n_actions\n",
        "        self.neuron_per_action = neuron_per_action\n",
        "        self.n_classes = n_actions\n",
        "        self.neuron_per_class = neuron_per_action\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "        self.deep = deep\n",
        "        self.maxPool1 = maxPool1\n",
        "        self.maxPool2 = maxPool2\n",
        "        self.time = time\n",
        "        self.crop_size = crop_size\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.clamp_intensity = kwargs.get('clamp_intensity',None)\n",
        "        self.single_output_layer = single_output_layer\n",
        "        self.pre_observation = pre_observation\n",
        "        self.has_decision_period = has_decision_period\n",
        "        self.local_rewarding = local_rewarding\n",
        "        self.soft_bound = soft_bound\n",
        "        self.confusion_matrix = confusion_matrix\n",
        "        self.lc_weights_vis = lc_weights_vis\n",
        "        self.out_weights_vis = out_weights_vis\n",
        "        self.lc_convergence_vis = lc_convergence_vis\n",
        "        self.out_convergence_vis = out_convergence_vis\n",
        "        self.frame_analysis = frame_analysis\n",
        "        self.in_channels = in_channels\n",
        "        self.n_channels1 = n_channels1\n",
        "        self.n_channels2 = n_channels2\n",
        "        self.stride1 = stride1\n",
        "        self.stride2 = stride2\n",
        "        self.convergences = {}\n",
        "        self.norm_factor_LC = norm_factor_LC\n",
        "        self.norm_factor_LC2 = norm_factor_LC2\n",
        "        self.norm_factor_out = norm_factor_out\n",
        "        self.wmin = wmin \n",
        "        self.wmax = wmax\n",
        "        self.wandb_active = wandb_active\n",
        "        self.epochs_trained = 0\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_steps= max_steps\n",
        "        self.rew = 0.0\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.env.reset()\n",
        "\n",
        "        self.time_analysis = kwargs.get('time_analysis', False)\n",
        "        if kwargs['variant'] == 'scalar':\n",
        "            assert self.has_decision_period == True, ''\n",
        "\n",
        "        if self.online == False:\n",
        "            assert self.has_decision_period == True, ''\n",
        "        \n",
        "        if self.has_decision_period == True:\n",
        "            assert self.online == False, \"Decision period is not compatible with online learning.\"\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.decision_period = kwargs['decision_period']\n",
        "            assert self.decision_period > 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period - self.decision_period\n",
        "\n",
        "        elif self.pre_observation == True:\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period\n",
        "            self.decision_period = self.time - self.observation_period\n",
        "\n",
        "        else:\n",
        "            self.observation_period = 0\n",
        "            self.decision_period = self.time\n",
        "            self.learning_period = self.time\n",
        "\n",
        "        ### nodes\n",
        "        inp = Input(shape= [in_channels,crop_size,crop_size], traces=True, tc_trace=tc_trace,traces_additive = trace_additive)\n",
        "        self.add_layer(inp, name=\"input\")\n",
        "\n",
        "        ## First hidden layer\n",
        "        conv_size1 = compute_size(crop_size, filter_size1, stride1)\n",
        "        main1 = NodesType_LC(shape= [n_channels1, conv_size1, conv_size1], thresh = thresh_LC, traces=True, tc_trace=tc_trace,\n",
        "                             traces_additive = trace_additive,tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "        \n",
        "        self.add_layer(main1, name=\"main1\")\n",
        "\n",
        "        ### connections \n",
        "        LC1 = LocalConnectionOrig(inp, main1, filter_size1, stride1, n_channels1,\\\n",
        "                              nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, norm = norm_factor_LC)\n",
        "\n",
        "        # LC1 = LocalConnection(inp, main1, filter_size1, stride1, in_channels, n_channels1,input_shape=(crop_size,crop_size),\\\n",
        "        #                      nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC)\n",
        "\n",
        "\n",
        "        if LC_weights_path:\n",
        "            a = torch.load(LC_weights_path)\n",
        "            LC1.w.data = a['state_dict']['input_to_main1.w']\n",
        "            print(\"Weights loaded ...\")\n",
        "        \n",
        "        elif normal_init:\n",
        "            w_lc_init = torch.normal(mu,std, size = (in_channels, n_channels1 * compute_size(crop_size, filter_size1, stride1)**2, filter_size1**2))\n",
        "            LC1.w.data = w_lc_init\n",
        "       \n",
        "        self.add_connection(LC1, \"input\", \"main1\")\n",
        "        self.convergences['lc1'] = []\n",
        "\n",
        "        if inh_LC:\n",
        "            main_width = compute_size(crop_size, filter_size1, stride1)\n",
        "            w_inh_LC = torch.zeros(n_channels1,main_width,main_width,n_channels1,main_width,main_width)\n",
        "            for c in range(n_channels1):\n",
        "                for w1 in range(main_width):\n",
        "                    for w2 in range(main_width):\n",
        "                        w_inh_LC[c,w1,w2,:,w1,w2] = - inh_factor_LC\n",
        "                        w_inh_LC[c,w1,w2,c,w1,w2] = 0\n",
        "        \n",
        "            w_inh_LC = w_inh_LC.reshape(main1.n,main1.n)\n",
        "                                                             \n",
        "            LC_recurrent_inhibition = Connection(\n",
        "                source=main1,\n",
        "                target=main1,\n",
        "                w=w_inh_LC,\n",
        "            )\n",
        "            self.add_connection(LC_recurrent_inhibition, \"main1\", \"main1\")\n",
        "        \n",
        "        \n",
        "        self.final_connection_source_name = 'main1'\n",
        "        self.final_connection_source = main1\n",
        "\n",
        "        self.hidden2 = main1\n",
        "        self.hidden2_name = 'main1'\n",
        "        if maxPool1:\n",
        "            maxPool_kernel = 2\n",
        "            maxPool_stride = 2\n",
        "            \n",
        "            conv_size1 =compute_size(conv_size1, maxPool_kernel, maxPool_stride)\n",
        "            self.final_connection_source_name = 'maxpool1'\n",
        "            \n",
        "            maxpool1 = LIFNodes(shape= [self.n_channels1, conv_size1, conv_size1], refrac = 0)\n",
        "            self.add_layer(maxpool1, name=\"maxpool1\")\n",
        "            self.final_connection_source = maxpool1\n",
        "            \n",
        "            maxPoolConnection = MaxPool2dLocalConnection(main1, maxpool1, maxPool_kernel, maxPool_stride)\n",
        "            self.add_connection(maxPoolConnection, \"main1\", 'maxpool1')\n",
        "            \n",
        "            self.hidden2 = maxpool1\n",
        "            self.hidden2_name = 'maxpool1'\n",
        "\n",
        "        if deep:\n",
        "            # # Second hidden layer\n",
        "            conv_size2 = compute_size(conv_size1, filter_size2, stride2)\n",
        "\n",
        "            main2 = NodesType_LC(shape= [n_channels2, conv_size2, conv_size2],traces=True, tc_trace=tc_trace,traces_additive = trace_additive,\n",
        "                                            tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "            \n",
        "            self.add_layer(main2, name=\"main2\")\n",
        "\n",
        "            ### connections \n",
        "            lc2_input_shape = (conv_size1,conv_size1)\n",
        "            LC2 = LocalConnection(self.hidden2, main2, filter_size2, stride2, n_channels1, n_channels2, input_shape= lc2_input_shape,\n",
        "            nu = _pair(nu_LC2), update_rule = update_rule_LC2, wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC2)\n",
        "\n",
        "            self.add_connection(LC2,  self.hidden2_name, \"main2\")\n",
        "            self.convergences['lc2'] = []\n",
        "            if LC2_weights_path:\n",
        "                a = torch.load(LC2_weights_path)\n",
        "                LC2.w.data = a['state_dict']['main1_to_main2.w']\n",
        "                print(\"Weights loaded ...\")\n",
        "            \n",
        "            elif normal_init:\n",
        "                w_lc_init = torch.normal(mu,std, size = (n_channels1, n_channels2 * compute_size(conv_size1, filter_size2, stride2)**2, filter_size2**2))\n",
        "                LC2.w.data = w_lc_init\n",
        "\n",
        "            self.final_connection_source_name = 'main2'\n",
        "            self.final_connection_source = main2\n",
        "\n",
        "            if inh_LC2:\n",
        "                main_width = conv_size2\n",
        "                w_inh_LC2 = torch.zeros(n_channels2,main_width,main_width,n_channels2,main_width,main_width)\n",
        "                for c in range(n_channels2):\n",
        "                    for w1 in range(main_width):\n",
        "                        for w2 in range(main_width):\n",
        "                            w_inh_LC2[c,w1,w2,:,w1,w2] = - inh_factor_LC2\n",
        "                            w_inh_LC2[c,w1,w2,c,w1,w2] = 0\n",
        "            \n",
        "                w_inh_LC2 = w_inh_LC2.reshape(main2.n,main2.n)\n",
        "                                                                \n",
        "                LC_recurrent_inhibition2 = Connection(\n",
        "                    source=main2,\n",
        "                    target=main2,\n",
        "                    w=w_inh_LC2,\n",
        "                )\n",
        "                self.add_connection(LC_recurrent_inhibition2, \"main2\", \"main2\")\n",
        "\n",
        "\n",
        "            if maxPool2:\n",
        "                maxPool_kernel = 2\n",
        "                maxPool_stride = 2\n",
        "                conv_size2 =compute_size(conv_size2, maxPool_kernel, maxPool_stride)\n",
        "                self.final_connection_source_name = 'maxpool2'\n",
        "                maxpool2 = LIFNodes(shape= [self.n_channels2, conv_size2, conv_size2], refrac = 0)\n",
        "                self.final_connection_source = maxpool2\n",
        "                maxPoolConnection2 = MaxPool2dLocalConnection(main2, maxpool2, maxPool_kernel, maxPool_stride)\n",
        "\n",
        "                self.add_layer(maxpool2, name=\"maxpool2\")\n",
        "                self.add_connection(maxPoolConnection2, \"main2\", 'maxpool2')\n",
        "\n",
        "\n",
        "        ### main2 to output\n",
        "        out = NodesType_Output(n= n_neurons, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "        self.add_layer(out, \"output\")\n",
        "\n",
        "        last_main_out = Connection(self.final_connection_source, out, nu = nu_Output, update_rule = update_rule_Output, wmin = wmin, wmax= wmax, norm = norm_factor_out)\n",
        "\n",
        "        if normal_init:\n",
        "            w_last_main_init = torch.normal(mu,std,size = (self.final_connection_source.n,out.n)) \n",
        "            last_main_out.w.data = w_last_main_init\n",
        "\n",
        "        self.add_connection(last_main_out, self.final_connection_source_name, \"output\")\n",
        "        self.convergences['last_main_out'] = []\n",
        "        ### Inhibitory:\n",
        "        if inh_type == 'between_layers':\n",
        "            w = -inh_factor * torch.ones(out.n, out.n)\n",
        "            for c in range(n_actions):\n",
        "                ind = slice(c*neuron_per_action,(c+1)*neuron_per_action)\n",
        "                w[ind, ind] = 0\n",
        "\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        elif inh_type == 'one_2_all':\n",
        "            w = -inh_factor * (torch.ones(out.n, out.n) - torch.eye(out.n, out.n))\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        # Diehl and Cook\n",
        "        elif inh_type == 'DC':\n",
        "            raise NotImplementedError('Diehl and cook not implemented yet fo r 10 classes')\n",
        "\n",
        "        # Directs network to GPU\n",
        "        if gpu:\n",
        "            self.to(\"cuda\")\n",
        "\n",
        "    def frame_process(self, x):\n",
        "        x[x<1.0] = 2.0\n",
        "        x[x==1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "\n",
        "\n",
        "    def get_state_spiking(self):\n",
        "        intensity = self.intensity\n",
        "        screen = self.env.render(mode='rgb_array')\n",
        "        screen = screen.transpose((2, 0, 1))\n",
        "        _, screen_height, screen_width = screen.shape\n",
        "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "        view_width = int(screen_width * 0.6)\n",
        "        world_width = self.env.x_threshold * 2\n",
        "        scale = screen_width / world_width\n",
        "        cart_location = int(self.env.state[0] * scale + screen_width / 2.0)\n",
        "        if cart_location < view_width // 2:\n",
        "            slice_range = slice(view_width)\n",
        "        elif cart_location > (screen_width - view_width // 2):\n",
        "            slice_range = slice(-view_width, None)\n",
        "        else:\n",
        "            slice_range = slice(cart_location - view_width // 2,\n",
        "                                cart_location + view_width // 2)\n",
        "            \n",
        "        # Strip off the edges, so that we have a square image centered on a cart\n",
        "        screen = screen[:, :, slice_range]\n",
        "\n",
        "        # Convert to float, rescale, convert to torch tensor\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        h = screen.shape[1]\n",
        "        w = screen.shape[2]\n",
        "        resize = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 120, self.crop_size, self.crop_size)),\n",
        "                    # transforms.Resize([80, 80]),\n",
        "                    #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "                    # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "                    #transforms.CenterCrop((crop_size, crop_size)),\n",
        "                    transforms.Grayscale(),\n",
        "                    transforms.ToTensor(),\n",
        "                    #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "                    transforms.Lambda(self.frame_process),\n",
        "                    #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "                    transforms.Lambda(lambda x: x * intensity),\n",
        "                    transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "        screen = resize(screen)\n",
        "        if self.frame_analysis:\n",
        "            f = screen.sum(axis=0)\n",
        "            f = f.to(device)\n",
        "            plt.imshow(f.cpu().numpy().squeeze(), cmap='gray')\n",
        "            plt.show()\n",
        "        return screen\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        hparams = None,\n",
        "        online_validate = True,\n",
        "        running_window_length = 250,\n",
        "        verbose = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        manual_seed(SEED)\n",
        "        if self.wandb_active:\n",
        "            wandb.watch(self)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "\n",
        "        self.add_monitor(reward_monitor, name=\"reward\")\n",
        "\n",
        "        reward_hist = collections.deque([], running_window_length)\n",
        "\n",
        "        self.spikes = {}\n",
        "        for layer in set(self.layers):\n",
        "            self.spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=None)\n",
        "            self.add_monitor(self.spikes[layer], name=\"%s_spikes\" % layer)\n",
        "            self.dopaminergic_layers = self.layers[\"output\"]\n",
        "       \n",
        "        self.episode = 0\n",
        "        rew = 0.0\n",
        "        tot_rew = 0.0\n",
        "\n",
        "\n",
        "        reward_history = []\n",
        "        if self.load_path:\n",
        "            \n",
        "            self.model_params = torch.load(self.load_path)\n",
        "            self.load_state_dict(torch.load(self.load_path)['state_dict'])\n",
        "            self.env = self.model_params['env']\n",
        "            self.episode =  self.model_params['episode']\n",
        "            hparams = self.model_params['hparams']\n",
        "            reward_hist = self.model_params['reward_hist']\n",
        "            print(f'Previous model loaded! Resuming training from episode {iteration}...\\n') if self.verbose else None\n",
        "        else:\n",
        "            self.env = gym.make('CartPole-v0')\n",
        "            self.env.reset()\n",
        "            print(f'Previous model not found! Training from the beginning...\\n') if self.verbose else None\n",
        "\n",
        "        pbar = tqdm(total=self.num_episodes)\n",
        "        \n",
        "        if self.time_analysis:\n",
        "            self.sample_spikes = {'input': [], 'main1': [], 'output': []}\n",
        "\n",
        "        for ep in range(self.num_episodes):\n",
        "            # print(f\"episode: {ep+1}\")\n",
        "            self.reset_state_variables()\n",
        "            self.env.reset()\n",
        "            done = False\n",
        "            tot_rew = 0.0\n",
        "            success = False\n",
        "            failure = False\n",
        "            num_steps = 0\n",
        "            for t in count():\n",
        "                \n",
        "                if t != 0:\n",
        "                    print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                    if self.time_analysis:\n",
        "                        self.sample_spikes['input'].append(self.spikes['input'].get('s'))\n",
        "                        self.sample_spikes['main1'].append(self.spikes['main1'].get('s'))\n",
        "                        self.sample_spikes['output'].append(self.spikes['output'].get('s').view(self.time, self.batch_size, n_actions, neuron_per_action))\n",
        "\n",
        "                        # w_lc1 = self.connections[('input', 'main1')].w\n",
        "                        # w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "                \n",
        "                image = self.get_state_spiking()\n",
        "                if gpu:\n",
        "                    inputs = {\"input\": image.cuda().view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "                else:\n",
        "                    inputs = {\"input\": image.view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "\n",
        "                clamp = {}\n",
        "                if self.clamp_intensity is not None:\n",
        "                    encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "                    clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "                # if done:\n",
        "                #     failure = True\n",
        "\n",
        "                if t >= self.max_steps:\n",
        "                    success = True\n",
        "                    done = True\n",
        "                elif done:\n",
        "                    failure = True\n",
        "\n",
        "\n",
        "                self.run(inputs=inputs, \n",
        "                        time = self.time,\n",
        "                        one_step=False,\n",
        "                        clamp = clamp,\n",
        "                        env = self.env,\n",
        "                        success = success,\n",
        "                        failure = failure,\n",
        "                        **kwargs,\n",
        "                        )\n",
        "                \n",
        "                rew = float(reward_monitor.get()[0])\n",
        "                tot_rew += rew\n",
        "                \n",
        "                lc_spikes1 = self.spikes['main1'].get('s')\n",
        "                #lc_spikes2 = self.spikes['main2'].get('s')\n",
        "                out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, self.n_actions, self.neuron_per_action)\n",
        "                sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "                selected_action = torch.argmax(sum_spikes, dim=1)\n",
        "\n",
        "                self.spikes['main1'].reset_state_variables()\n",
        "                self.spikes[\"output\"].reset_state_variables()\n",
        "                reward_monitor.reset_state_variables()\n",
        "                #self.reset_state_variables()\n",
        "\n",
        "                if done:\n",
        "                    if success == True:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\Successful episode!')\n",
        "                        num_steps = t+1\n",
        "                    else:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\nEpisode not successful!')\n",
        "                        num_steps = t+1\n",
        "                    break\n",
        "\n",
        "                obs, reward, done, _ = self.env.step(int(selected_action[0]))         \n",
        "                # Get voltage recording.\n",
        "                #main_voltage = main_monitor.get(\"v\")\n",
        "\n",
        "                #tensorboard.update(step= i)\n",
        "\n",
        "\n",
        "            if self.lc_weights_vis:\n",
        "                plot_locally_connected_weights(self.connections[('input','main1')].w, self.n_channels1, self.filter_size1,\n",
        "                                                compute_size(self.crop_size, self.filter_size1, self.stride1), self.connections[('input','main1')].locations,\n",
        "                                                self.crop_size ** 2)\n",
        "                plt.show()\n",
        "\n",
        "            if self.wandb_active:\n",
        "                wandb.log({\n",
        "                        **{'reward': tot_rew},\n",
        "                        **{' to '.join(name) + ' std': c.w.std().item() for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                        #**{name + ' spikes': monitor.get('s').sum().item() for name, monitor in self.spikes.items()},\n",
        "                        **{' to '.join(name) + \" gradients\": wandb.Histogram(c.w.cpu()) for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                    },\n",
        "                    step = self.episode)\n",
        "\n",
        "\n",
        "            #self.reward_fn.update() \n",
        "            #Plot_et.plot()    \n",
        "            #self.reset_state_variables()  # Reset state variables.\n",
        "            \n",
        "            self.episode += 1\n",
        "            print(f'\\nEpisode {self.episode} lasted for {num_steps} time steps with total reward of {tot_rew}\\n')\n",
        "            pbar.set_description_str(\"Episode: \"+str(self.episode)+\", Episode Total Reward: \" + \"{:.2f}\".format(tot_rew))\n",
        "            pbar.update()\n",
        "\n",
        "    \n",
        "    def single_trial(self):\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        image = self.get_state_spiking()\n",
        "\n",
        "        if gpu:\n",
        "            inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "        else:\n",
        "            inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step = False,\n",
        "                mode = 'RL',\n",
        "                env = 'CartPole'\n",
        "                )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqAFucAUDb8"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3TerGeJoFdzg"
      },
      "outputs": [],
      "source": [
        "n_neurons = 2 #100\n",
        "n_actions = 2\n",
        "neuron_per_action = int(n_neurons/n_actions)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "crop_size = 120\n",
        "intensity = 128\n",
        "frame_analysis = True\n",
        "lc_weights_vis = True\n",
        "\n",
        "obs = 50\n",
        "dec = 255\n",
        "learn = 50\n",
        "time = obs+dec+learn\n",
        "\n",
        "max_steps = 100\n",
        "num_episodes = 1000\n",
        "\n",
        "filter_size1 = 60\n",
        "stride1 = 20\n",
        "n_channels1 = 25\n",
        "\n",
        "learning_rate = 0.01\n",
        "norm_factor = 0.25\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': crop_size,\n",
        "    'intensity': intensity,\n",
        "    'round_input': False,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': n_channels1,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': filter_size1,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': stride1,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_actions': n_actions,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'max_steps': max_steps,\n",
        "    'num_episodes': num_episodes,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': True,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': obs,\n",
        "    'decision_period': dec,\n",
        "    'time_analysis': False,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': PostPre,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDPET,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.0001,0.01),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output': learning_rate,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': True,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': 'between_layers',\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': norm_factor*filter_size1*filter_size1,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': None,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'LC_weights_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25_weights.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': lc_weights_vis,\n",
        "    'out_weights_vis': False,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': False,\n",
        "    'frame_analysis': False,\n",
        "\n",
        "    ## reward\n",
        "    'n_labels': n_actions,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': 1.,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0005,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokdidkrV2Z5"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Venb2KhSYrT_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715,
          "referenced_widgets": [
            "bfd70ec59c0f49cea07cec2fffda6473",
            "f69f0b80672a4bc8901a3a041f77b334",
            "3fcc43a21c9d41b6bc14c31aeb828a8d",
            "79b401d15b7d46a5aa0aa14238704e37",
            "5b683361d797490aaa4ac8f54a97638d",
            "412efd28b8e0420ca700ccde9583c7ff",
            "5f9ddd728e5348a6a0806268b907d9f3",
            "58aca235d6cf45adb1156d6dba7c6cf1",
            "ed08e75f1cea4ca6ac8dbea2cc7cc64d",
            "8d606534b16d43f1865acc8769a1c09e",
            "a4047ecd039343fd8e0521c8dd8a5e23"
          ]
        },
        "id": "oThYyYvHJzeP",
        "outputId": "d4a8d9b1-9ace-4210-bad8-74fcda9c8181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/singularbrain/biolcnet/runs/219e3jg8\" target=\"_blank\">treasured-flower-111</a></strong> to <a href=\"https://wandb.ai/singularbrain/biolcnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous model not found! Training from the beginning...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfd70ec59c0f49cea07cec2fffda6473",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d4e4239b3965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BioLCNet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'singularbrain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLCNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRLTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_active\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWANDB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-c32147a28590>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, hparams, online_validate, running_window_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m                         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                         \u001b[0mfailure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfailure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m                         )\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/network.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, time, one_step, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                         self.connections[c].update(\n\u001b[0;32m--> 465\u001b[0;31m                             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m                             )\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/topology.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mCompute\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/topology.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlearning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_rule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/learning/learning.py\u001b[0m in \u001b[0;36m_connection_update\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m             self.connection.w += (\n\u001b[1;32m   1051\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meligibility_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m             ).to(self.connection.w.device)*(self.soft_bound_decay().to(self.connection.w.device))\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;31m# Update P^+ and P^- values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "manual_seed(SEED)\n",
        "\n",
        "if WANDB:\n",
        "    wandb.init(project='BioLCNet', entity='singularbrain', config=network_hparams)\n",
        "net = LCNet(time,**network_hparams, reward_fn=RLTasks('CartPole-v0'), wandb_active = WANDB)\n",
        "net.learn(**network_hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gym Playground"
      ],
      "metadata": {
        "id": "9M7NvCJlr0E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "intensity = 400\n",
        "crop_size = 90\n",
        "time = 100+300+50\n",
        "def frame_process(x):\n",
        "        x[x<1.0] = 2.0\n",
        "        x[x==1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "screen = env.render(mode='rgb_array')\n",
        "screen = screen.transpose((2, 0, 1))\n",
        "_, screen_height, screen_width = screen.shape\n",
        "screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "view_width = int(screen_width * 0.6)\n",
        "world_width = env.x_threshold * 2\n",
        "scale = screen_width / world_width\n",
        "cart_location = int(env.state[0] * scale + screen_width / 2.0)\n",
        "if cart_location < view_width // 2:\n",
        "    slice_range = slice(view_width)\n",
        "elif cart_location > (screen_width - view_width // 2):\n",
        "    slice_range = slice(-view_width, None)\n",
        "else:\n",
        "    slice_range = slice(cart_location - view_width // 2,\n",
        "                        cart_location + view_width // 2)\n",
        "# Strip off the edges, so that we have a square image centered on a cart\n",
        "screen = screen[:, :, slice_range]\n",
        "# Convert to float, rescale, convert to torch tensor\n",
        "screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "screen = torch.from_numpy(screen)\n",
        "plt.imshow(screen.cpu().permute(\n",
        "        1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "h = screen.shape[1]\n",
        "w = screen.shape[2]\n",
        "print(h*0.75)\n",
        "resize = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 120, 120, 120)),\n",
        "            # transforms.Resize([80, 80]),\n",
        "            #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "            # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "            #transforms.CenterCrop((crop_size, crop_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "            transforms.Lambda(frame_process),\n",
        "            #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "            transforms.Lambda(lambda x: x * intensity),\n",
        "            transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "device = 'cuda'\n",
        "\n",
        "# Resize, and add a batch dimension (BCHW)\n",
        "screen = resize(screen)\n",
        "# print(screen.shape)\n",
        "print(int(h*0.8), int(w*0.8))\n",
        "print(screen.shape)\n",
        "# screen = screen.to(device)\n",
        "# plt.imshow(screen.cpu().permute(\n",
        "#         1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "\n",
        "# print(a.shape)\n",
        "a = screen.sum(axis=0)\n",
        "\n",
        "a = a.to(device)\n",
        "plt.imshow(a.cpu().numpy().squeeze(), cmap='gray')\n",
        "obs, reward, done, info = env.step(0)\n",
        "obs, reward, done, info "
      ],
      "metadata": {
        "id": "dXvH3s8Yr2dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BindsNet Breakout"
      ],
      "metadata": {
        "id": "9jFzOzYIFdGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/BindsNET/bindsnet.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Qi4v29mDF3i",
        "outputId": "e2115a1b-3d7a-403a-e763-57ccd12ddab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/BindsNET/bindsnet.git\n",
            "  Cloning https://github.com/BindsNET/bindsnet.git to /tmp/pip-req-build-mtcdoh5t\n",
            "  Running command git clone -q https://github.com/BindsNET/bindsnet.git /tmp/pip-req-build-mtcdoh5t\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision==0.11.1 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (0.11.1+cu111)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image<0.19.0,>=0.18.3 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (0.18.3)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (1.10.0+cu111)\n",
            "Collecting gym<0.11.0,>=0.10.4\n",
            "  Downloading gym-0.10.11.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 55.4 MB/s \n",
            "\u001b[?25hCollecting pandas<2.0.0,>=1.3.2\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 64.0 MB/s \n",
            "\u001b[?25hCollecting scipy<2.0.0,>=1.7.1\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (1.19.5)\n",
            "Collecting tensorboardX==2.2\n",
            "  Downloading tensorboardX-2.2-py2.py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 79.5 MB/s \n",
            "\u001b[?25hCollecting scikit-build<0.13.0,>=0.12.0\n",
            "  Downloading scikit_build-0.12.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.62.2 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (4.62.3)\n",
            "Collecting opencv-python<5.0.0,>=4.5.3\n",
            "  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.4 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting foolbox<4.0.0,>=3.3.1\n",
            "  Downloading foolbox-3.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 66.4 MB/s \n",
            "\u001b[?25hCollecting matplotlib<4.0.0,>=3.4.3\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 96.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (0.29.26)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.2->bindsnet==0.3.0) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->bindsnet==0.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1->bindsnet==0.3.0) (7.1.2)\n",
            "Collecting GitPython>=3.0.7\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting eagerpy==0.29.0\n",
            "  Downloading eagerpy-0.29.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (57.4.0)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym<0.11.0,>=0.10.4->bindsnet==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.11.0,>=0.10.4->bindsnet==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (3.0.6)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
            "\u001b[K     |████████████████████████████████| 890 kB 91.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.3.2->bindsnet==0.3.0) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym<0.11.0,>=0.10.4->bindsnet==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (2.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (2.10)\n",
            "Requirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.7/dist-packages (from scikit-build<0.13.0,>=0.12.0->bindsnet==0.3.0) (0.37.1)\n",
            "Collecting distro\n",
            "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (2.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.2->bindsnet==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.2->bindsnet==0.3.0) (3.0.0)\n",
            "Building wheels for collected packages: bindsnet, gym\n",
            "  Building wheel for bindsnet (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bindsnet: filename=bindsnet-0.3.0-py3-none-any.whl size=114807 sha256=028eda52acfd9f56b6fda6aeddbd155258705a105369030f2fff2a96ea21e814\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ankwpzvk/wheels/85/c3/10/2e6bb871a97d7e6fdc9a0c047652b4895c0825d19646be7adf\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-py3-none-any.whl size=1588313 sha256=dbcb356b91d48b050a080f54dc6bd9d5ecf6b87ed287da5013fd2d3b4863ea6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/c9/3a/068c5b80305e89c8de8b0a412e67ef2986cbad74895cfb9551\n",
            "Successfully built bindsnet gym\n",
            "Installing collected packages: smmap, gitdb, fonttools, scipy, requests, matplotlib, GitPython, eagerpy, distro, tensorboardX, scikit-learn, scikit-build, pandas, opencv-python, gym, foolbox, bindsnet\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.26 bindsnet-0.3.0 distro-1.6.0 eagerpy-0.29.0 fonttools-4.28.5 foolbox-3.3.1 gitdb-4.0.9 gym-0.10.11 matplotlib-3.5.1 opencv-python-4.5.5.62 pandas-1.3.5 requests-2.27.1 scikit-build-0.12.0 scikit-learn-0.24.2 scipy-1.7.3 smmap-5.0.0 tensorboardX-2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ROgQ2ZWA-VT",
        "outputId": "98adc726-879e-48d6-8ae4-2cfd47b0b044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-16 14:47:08--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11128004 (11M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar’\n",
            "\n",
            "Roms.rar            100%[===================>]  10.61M   486KB/s    in 23s     \n",
            "\n",
            "2022-01-16 14:47:32 (469 KB/s) - ‘Roms.rar’ saved [11128004/11128004]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/Roms.rar\n",
            "\n",
            "Extracting  /content/ROM/HC ROMS.zip                                     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  /content/ROM/ROMS.zip                                        \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install matplotlib==2.1.1\n",
        "\n",
        "### Restart your runtime after this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "_K7CuZmqFTny",
        "outputId": "736e4d9f-ec5b-4ea6-cb87-2c444d87957e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==2.1.1\n",
            "  Downloading matplotlib-2.1.1.tar.gz (36.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (3.0.6)\n",
            "Building wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-2.1.1-cp37-cp37m-linux_x86_64.whl size=10243757 sha256=18b68a6b31696664021b1bb9df2061292918bfcc3217241b860d9366de83d8c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/7a/d6/021782cff10d8257e030d4a766ca5ed9667fd8758606fbbeff\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.1\n",
            "    Uninstalling matplotlib-3.5.1:\n",
            "      Successfully uninstalled matplotlib-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "seaborn 0.11.2 requires matplotlib>=2.2, but you have matplotlib 2.1.1 which is incompatible.\n",
            "plotnine 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.1.1 which is incompatible.\n",
            "mizani 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.1.1 which is incompatible.\n",
            "bindsnet 0.3.0 requires matplotlib<4.0.0,>=3.4.3, but you have matplotlib 2.1.1 which is incompatible.\n",
            "arviz 0.11.4 requires matplotlib>=3.0, but you have matplotlib 2.1.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-2.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "wONJ1L1uEGMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bindsnet.encoding import bernoulli\n",
        "from bindsnet.environment import GymEnvironment\n",
        "from bindsnet.learning import MSTDP\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes\n",
        "from bindsnet.network.topology import Connection\n",
        "from bindsnet.pipeline import EnvironmentPipeline\n",
        "from bindsnet.pipeline.action import select_softmax\n",
        "import time\n",
        "\n",
        "# Build network.\n",
        "network = Network(dt=1.0)\n",
        "\n",
        "# Layers of neurons.\n",
        "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
        "middle = LIFNodes(n=100, traces=True)\n",
        "out = LIFNodes(n=4, refrac=0, traces=True)\n",
        "\n",
        "# Connections between layers.\n",
        "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
        "middle_out = Connection(\n",
        "    source=middle,\n",
        "    target=out,\n",
        "    wmin=0,\n",
        "    wmax=1,\n",
        "    update_rule=MSTDP,\n",
        "    nu=1e-1,\n",
        "    norm=0.5 * middle.n,\n",
        ")\n",
        "\n",
        "# Add all layers and connections to the network.\n",
        "network.add_layer(inpt, name=\"Input Layer\")\n",
        "network.add_layer(middle, name=\"Hidden Layer\")\n",
        "network.add_layer(out, name=\"Output Layer\")\n",
        "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
        "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
        "\n",
        "# Load the Breakout environment.\n",
        "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
        "environment.reset()\n",
        "\n",
        "# Build pipeline from specified components.\n",
        "environment_pipeline = EnvironmentPipeline(\n",
        "    network,\n",
        "    environment,\n",
        "    encoding=bernoulli,\n",
        "    action_function=select_softmax,\n",
        "    output=\"Output Layer\",\n",
        "    time=100,\n",
        "    history_length=1,\n",
        "    delta=1,\n",
        "    plot_interval=None,\n",
        "    render_interval=None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(pipeline, episode_count):\n",
        "    for i in range(episode_count):\n",
        "        total_reward = 0\n",
        "        pipeline.reset_state_variables()\n",
        "        is_done = False\n",
        "        render_counter = 0\n",
        "        while not is_done:\n",
        "            # if render_counter % 10 == 0:\n",
        "            #     # plt.title(\"Game image\")\n",
        "            #     # plt.imshow(pipeline.env.render())\n",
        "            #     # plt.show()\n",
        "            #     a = pipeline.env.render()\n",
        "            #     print(a)\n",
        "            #     time.sleep(0.1)\n",
        "                \n",
        "            render_counter += 1\n",
        "            result = pipeline.env_step()\n",
        "            pipeline.step(result)\n",
        "\n",
        "            reward = result[1]\n",
        "            total_reward += reward\n",
        "            \n",
        "            is_done = result[2]\n",
        "        print(f\"Episode {i} total reward:{total_reward}\")\n",
        "    pipeline.env.close()\n",
        "\n",
        "print(\"Training: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)\n",
        "\n",
        "# stop MSTDP\n",
        "environment_pipeline.network.learning = False\n",
        "\n",
        "print(\"Testing: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)"
      ],
      "metadata": {
        "id": "y_jx13I5-VTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f32146c8-3f59-47fc-d959-3a66975bf6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: \n",
            "Episode 0 total reward:2.0\n",
            "Episode 1 total reward:0.0\n",
            "Episode 2 total reward:0.0\n",
            "Episode 3 total reward:1.0\n",
            "Episode 4 total reward:1.0\n",
            "Episode 5 total reward:1.0\n",
            "Episode 6 total reward:1.0\n",
            "Episode 7 total reward:3.0\n",
            "Episode 8 total reward:1.0\n",
            "Episode 9 total reward:2.0\n",
            "Episode 10 total reward:2.0\n",
            "Episode 11 total reward:2.0\n",
            "Episode 12 total reward:0.0\n",
            "Episode 13 total reward:1.0\n",
            "Episode 14 total reward:0.0\n",
            "Episode 15 total reward:2.0\n",
            "Episode 16 total reward:0.0\n",
            "Episode 17 total reward:0.0\n",
            "Episode 18 total reward:2.0\n",
            "Episode 19 total reward:0.0\n",
            "Episode 20 total reward:2.0\n",
            "Episode 21 total reward:0.0\n",
            "Episode 22 total reward:0.0\n",
            "Episode 23 total reward:0.0\n",
            "Episode 24 total reward:0.0\n",
            "Episode 25 total reward:1.0\n",
            "Episode 26 total reward:1.0\n",
            "Episode 27 total reward:1.0\n",
            "Episode 28 total reward:1.0\n",
            "Episode 29 total reward:0.0\n",
            "Episode 30 total reward:2.0\n",
            "Episode 31 total reward:3.0\n",
            "Episode 32 total reward:2.0\n",
            "Episode 33 total reward:0.0\n",
            "Episode 34 total reward:2.0\n",
            "Episode 35 total reward:4.0\n",
            "Episode 36 total reward:1.0\n",
            "Episode 37 total reward:2.0\n",
            "Episode 38 total reward:1.0\n",
            "Episode 39 total reward:2.0\n",
            "Episode 40 total reward:2.0\n",
            "Episode 41 total reward:2.0\n",
            "Episode 42 total reward:0.0\n",
            "Episode 43 total reward:2.0\n",
            "Episode 44 total reward:1.0\n",
            "Episode 45 total reward:0.0\n",
            "Episode 46 total reward:1.0\n",
            "Episode 47 total reward:2.0\n",
            "Episode 48 total reward:1.0\n",
            "Episode 49 total reward:0.0\n",
            "Episode 50 total reward:3.0\n",
            "Episode 51 total reward:1.0\n",
            "Episode 52 total reward:0.0\n",
            "Episode 53 total reward:2.0\n",
            "Episode 54 total reward:1.0\n",
            "Episode 55 total reward:3.0\n",
            "Episode 56 total reward:0.0\n",
            "Episode 57 total reward:2.0\n",
            "Episode 58 total reward:0.0\n",
            "Episode 59 total reward:3.0\n",
            "Episode 60 total reward:1.0\n",
            "Episode 61 total reward:3.0\n",
            "Episode 62 total reward:0.0\n",
            "Episode 63 total reward:0.0\n",
            "Episode 64 total reward:0.0\n",
            "Episode 65 total reward:1.0\n",
            "Episode 66 total reward:2.0\n",
            "Episode 67 total reward:2.0\n",
            "Episode 68 total reward:0.0\n",
            "Episode 69 total reward:0.0\n",
            "Episode 70 total reward:1.0\n",
            "Episode 71 total reward:2.0\n",
            "Episode 72 total reward:2.0\n",
            "Episode 73 total reward:0.0\n",
            "Episode 74 total reward:0.0\n",
            "Episode 75 total reward:0.0\n",
            "Episode 76 total reward:0.0\n",
            "Episode 77 total reward:1.0\n",
            "Episode 78 total reward:2.0\n",
            "Episode 79 total reward:3.0\n",
            "Episode 80 total reward:2.0\n",
            "Episode 81 total reward:0.0\n",
            "Episode 82 total reward:2.0\n",
            "Episode 83 total reward:1.0\n",
            "Episode 84 total reward:1.0\n",
            "Episode 85 total reward:1.0\n",
            "Episode 86 total reward:3.0\n",
            "Episode 87 total reward:2.0\n",
            "Episode 88 total reward:2.0\n",
            "Episode 89 total reward:0.0\n",
            "Episode 90 total reward:0.0\n",
            "Episode 91 total reward:5.0\n",
            "Episode 92 total reward:2.0\n",
            "Episode 93 total reward:2.0\n",
            "Episode 94 total reward:2.0\n",
            "Episode 95 total reward:0.0\n",
            "Episode 96 total reward:1.0\n",
            "Episode 97 total reward:0.0\n",
            "Episode 98 total reward:0.0\n",
            "Episode 99 total reward:1.0\n",
            "Testing: \n",
            "Episode 0 total reward:2.0\n",
            "Episode 1 total reward:2.0\n",
            "Episode 2 total reward:0.0\n",
            "Episode 3 total reward:3.0\n",
            "Episode 4 total reward:3.0\n",
            "Episode 5 total reward:2.0\n",
            "Episode 6 total reward:1.0\n",
            "Episode 7 total reward:1.0\n",
            "Episode 8 total reward:0.0\n",
            "Episode 9 total reward:0.0\n",
            "Episode 10 total reward:0.0\n",
            "Episode 11 total reward:2.0\n",
            "Episode 12 total reward:0.0\n",
            "Episode 13 total reward:2.0\n",
            "Episode 14 total reward:1.0\n",
            "Episode 15 total reward:1.0\n",
            "Episode 16 total reward:0.0\n",
            "Episode 17 total reward:2.0\n",
            "Episode 18 total reward:1.0\n",
            "Episode 19 total reward:0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2feff05f12ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-2feff05f12ee>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(pipeline, episode_count)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mrender_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/pipeline/base_pipeline.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecursive_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mstep_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/pipeline/environment_pipeline.py\u001b[0m in \u001b[0;36mstep_\u001b[0;34m(self, gym_batch, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Run the network on the spike train-encoded inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/network.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, time, one_step, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mcurrent_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mone_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mcurrent_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/network.py\u001b[0m in \u001b[0;36m_get_inputs\u001b[0;34m(self, layers)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PLVXCK0Djg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7fTSvrK3T_GA",
        "ULGGHW43UksI",
        "MBKedMpIleMr",
        "8clxN_npa1WY",
        "sCXSLZZoGS4z"
      ],
      "machine_shape": "hm",
      "name": "BioLCNet_CartPole.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "87ae7d1e75b14a98f2d7b99b6b39b40721989d38e6517f9dbec64ca4d8e3011b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bfd70ec59c0f49cea07cec2fffda6473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f69f0b80672a4bc8901a3a041f77b334",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3fcc43a21c9d41b6bc14c31aeb828a8d",
              "IPY_MODEL_79b401d15b7d46a5aa0aa14238704e37",
              "IPY_MODEL_5b683361d797490aaa4ac8f54a97638d"
            ]
          }
        },
        "f69f0b80672a4bc8901a3a041f77b334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3fcc43a21c9d41b6bc14c31aeb828a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_412efd28b8e0420ca700ccde9583c7ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f9ddd728e5348a6a0806268b907d9f3"
          }
        },
        "79b401d15b7d46a5aa0aa14238704e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58aca235d6cf45adb1156d6dba7c6cf1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed08e75f1cea4ca6ac8dbea2cc7cc64d"
          }
        },
        "5b683361d797490aaa4ac8f54a97638d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d606534b16d43f1865acc8769a1c09e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1000 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4047ecd039343fd8e0521c8dd8a5e23"
          }
        },
        "412efd28b8e0420ca700ccde9583c7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f9ddd728e5348a6a0806268b907d9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58aca235d6cf45adb1156d6dba7c6cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed08e75f1cea4ca6ac8dbea2cc7cc64d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d606534b16d43f1865acc8769a1c09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4047ecd039343fd8e0521c8dd8a5e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}