{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/DeepBioLCNet/blob/main/BioLCNet_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTSvrK3T_GA"
      },
      "source": [
        "#Notebook setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lXtgP_iEPE0G"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KXcXvvsXcOlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711e1c73-f34b-4fd8-cc4e-2daaafe637c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepBioLCNet' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Singular-Brain/DeepBioLCNet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "0QFC6xL2uAaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d4cf9a-f7f3-4bff-8e8c-9eb65a7edd42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f42aed34390>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K4l3AVRbGS4Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.image import AxesImage\n",
        "from torch.nn.modules.utils import _pair\n",
        "from matplotlib.collections import PathCollection\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from typing import Tuple, List, Optional, Sized, Dict, Union\n",
        "import math\n",
        "import random\n",
        "from torchvision.transforms.functional import crop\n",
        "# from ..utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights\n",
        "\n",
        "import gym\n",
        "import tkinter\n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BFGNAecpT-Lj"
      },
      "outputs": [],
      "source": [
        "from bindsnet.network.nodes import Nodes\n",
        "import os\n",
        "### import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, Tuple, Optional, Sequence\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "from bindsnet.datasets import MNIST\n",
        "from bindsnet.encoding import PoissonEncoder\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes, IFNodes\n",
        "from bindsnet.network.topology import Connection, MaxPool2dLocalConnection\n",
        "from bindsnet.network.topology import LocalConnection, LocalConnectionOrig\n",
        "from bindsnet.network.monitors import Monitor, AbstractMonitor, TensorBoardMonitor\n",
        "from bindsnet.learning import PostPre, MSTDP, MSTDPET, WeightDependentPostPre, Hebbian\n",
        "from bindsnet.learning.reward import DynamicDopamineInjection, DopaminergicRPE, RLTasks\n",
        "from bindsnet.analysis.plotting import plot_locally_connected_weights,plot_locally_connected_weights_meh,plot_spikes,\\\n",
        "plot_LC_timepoint_spikes,plot_locally_connected_weights_meh2,plot_convergence_and_histogram,plot_locally_connected_weights_meh3\n",
        "from bindsnet.analysis.visualization import plot_weights_movie, plot_spike_trains_for_example,summary, plot_voltage\n",
        "from bindsnet.utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGGHW43UksI"
      },
      "source": [
        "## Sets up Gpu use and manual seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LiUmFrpcUfmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e83bcda-0edf-4a46-adbe-4d71b143ee29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Device =  cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device =  torch.device(\"cuda\")\n",
        "    gpu = True\n",
        "else:\n",
        "    device =  torch.device(\"cpu\")\n",
        "    gpu = False\n",
        "\n",
        "def manual_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SEED = 2045 # The Singularity is Near!\n",
        "manual_seed(SEED)\n",
        "WANDB = False\n",
        "\n",
        "torch.set_num_threads(os.cpu_count() - 1)\n",
        "print(\"Running on Device = \", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKedMpIleMr"
      },
      "source": [
        "# Custom Monitors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqpsr2a1WV"
      },
      "source": [
        "## Reward Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M44GJ65GleMs"
      },
      "outputs": [],
      "source": [
        "class RewardMonitor(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: None,\n",
        "        batch_size: int = 1,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.time = time\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # if time is not specified the monitor variable accumulate the logs\n",
        "        if self.time is None:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        self.recording = []\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if \"reward\" in kwargs:\n",
        "            self.recording.append(kwargs[\"reward\"])\n",
        "        # remove the oldest element (first in the list)\n",
        "        # if self.time is not None:\n",
        "        #     self.recording.pop(0)\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clxN_npa1WY"
      },
      "source": [
        "## Plot Eligibility trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SshGlRwpa1WZ"
      },
      "outputs": [],
      "source": [
        "class PlotET(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        i,\n",
        "        j,\n",
        "        source,\n",
        "        target,\n",
        "        connection,\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.connection = connection\n",
        "\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if hasattr(self.connection.update_rule, 'p_plus'):\n",
        "            self.recording['spikes_i'].append(self.source.s.ravel()[self.i].item())\n",
        "            self.recording['spikes_j'].append(self.target.s.ravel()[self.j].item())\n",
        "            self.recording['p_plus'].append(self.connection.update_rule.p_plus[self.i].item())\n",
        "            self.recording['p_minus'].append(self.connection.update_rule.p_minus[self.j].item())\n",
        "            self.recording['eligibility'].append(self.connection.update_rule.eligibility[self.i,self.j].item())\n",
        "            self.recording['eligibility_trace'].append(self.connection.update_rule.eligibility_trace[self.i,self.j].item())\n",
        "            self.recording['w'].append(self.connection.w[self.i,self.j].item())\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        fig, axs  = plt.subplots(7)\n",
        "        fig.set_size_inches(10, 20)\n",
        "        for i, (name, p) in enumerate(self.recording.items()):\n",
        "            axs[i].plot(p[-250:])\n",
        "            axs[i].set_title(name)\n",
        "    \n",
        "        fig.show()\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = {\n",
        "        'spikes_i': [],\n",
        "        'spikes_j': [],\n",
        "        'p_plus':[],\n",
        "        'p_minus':[],\n",
        "        'eligibility':[],\n",
        "        'eligibility_trace':[],\n",
        "        'w': [],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YGE1XjvIkZ"
      },
      "source": [
        "## Kernel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-4hp2V46vOUv"
      },
      "outputs": [],
      "source": [
        "class AbstractKernel(ABC):\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Base class for generating image filter kernels such as Gabor, DoG, etc. Each subclass should override :attr:`__call__` function.\n",
        "        Instantiates a ``Filter Kernel`` object.\n",
        "        :param window_size : The size of the kernel (int)\n",
        "        \"\"\"\n",
        "        self.window_size = kernel_size\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PL2L6_ABwBH4"
      },
      "outputs": [],
      "source": [
        "class DoGKernel(AbstractKernel):\n",
        "    def __init__(self, kernel_size: Union[int, Tuple[int, int]], sigma1 : float, sigma2 : float):\n",
        "        \"\"\"\n",
        "        Generates DoG filter kernels.\n",
        "        :param kernel_size: Horizontal and vertical size of DOG kernels.(If pass int, we consider it as a square filter) \n",
        "        :param sigma1 : The sigma parameter for the first Gaussian function.\n",
        "        :param sigma2 : The sigma parameter for the second Gaussian function.\n",
        "        \"\"\"\n",
        "        super(DoGKernel, self).__init__(kernel_size)\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        \n",
        "    def __call__(self):\n",
        "        k = self.window_size//2\n",
        "        x, y = np.mgrid[-k:k+1:1, -k:k+1:1]\n",
        "        a = 1.0 / (2 * math.pi)\n",
        "        prod = x*x + y*y\n",
        "        f1 = (1/(self.sigma1*self.sigma1)) * np.exp(-0.5 * (1/(self.sigma1*self.sigma1)) * (prod))\n",
        "        f2 = (1/(self.sigma2*self.sigma2)) * np.exp(-0.5 * (1/(self.sigma2*self.sigma2)) * (prod))\n",
        "        dog = a * (f1-f2)\n",
        "        dog_mean = np.mean(dog)\n",
        "        dog = dog - dog_mean\n",
        "        dog_max = np.max(dog)\n",
        "        dog = dog / dog_max\n",
        "        dog_tensor = torch.from_numpy(dog)\n",
        "        # returns a 2d tensor corresponding to the requested DoG filter\n",
        "        return dog_tensor.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zBUT0IUZDXxW"
      },
      "outputs": [],
      "source": [
        "class Filter():\n",
        "    \"\"\"\n",
        "    Applies a filter transform. Each filter contains a sequence of :attr:`FilterKernel` objects.\n",
        "    The result of each filter kernel will be passed through a given threshold (if not :attr:`None`).\n",
        "    Args:\n",
        "        filter_kernels (sequence of FilterKernels): The sequence of filter kernels.\n",
        "        padding (int, optional): The size of the padding for the convolution of filter kernels. Default: 0\n",
        "        thresholds (sequence of floats, optional): The threshold for each filter kernel. Default: None\n",
        "        use_abs (boolean, optional): To compute the absolute value of the outputs or not. Default: False\n",
        "    .. note::\n",
        "        The size of the compund filter kernel tensor (stack of individual filter kernels) will be equal to the \n",
        "        greatest window size among kernels. All other smaller kernels will be zero-padded with an appropriate \n",
        "        amount.\n",
        "    \"\"\"\n",
        "    # filter_kernels must be a list of filter kernels\n",
        "    # thresholds must be a list of thresholds for each kernel\n",
        "    def __init__(self, filter_kernels, padding=0, thresholds=None, use_abs=False):\n",
        "        tensor_list = []\n",
        "        self.max_window_size = 0\n",
        "        for kernel in filter_kernels:\n",
        "            if isinstance(kernel, torch.Tensor):\n",
        "                tensor_list.append(kernel)\n",
        "                self.max_window_size = max(self.max_window_size, kernel.size(-1))\n",
        "            else:\n",
        "                tensor_list.append(kernel().unsqueeze(0))\n",
        "                self.max_window_size = max(self.max_window_size, kernel.window_size)\n",
        "        for i in range(len(tensor_list)):\n",
        "            p = (self.max_window_size - filter_kernels[i].window_size)//2\n",
        "            tensor_list[i] = fn.pad(tensor_list[i], (p,p,p,p))\n",
        "\n",
        "        self.kernels = torch.stack(tensor_list)\n",
        "        self.number_of_kernels = len(filter_kernels)\n",
        "        self.padding = padding\n",
        "        if isinstance(thresholds, list):\n",
        "            self.thresholds = thresholds.clone().detach()\n",
        "            self.thresholds.unsqueeze_(0).unsqueeze_(2).unsqueeze_(3)\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "        self.use_abs = use_abs\n",
        "\n",
        "    # returns a 4d tensor containing the flitered versions of the input image\n",
        "    # input is a 4d tensor. dim: (minibatch=1, filter_kernels, height, width)\n",
        "    def __call__(self, input):\n",
        "\n",
        "        # if input.dim() == 3:\n",
        "        #     input2 = torch.unsqueeze(input, 0)\n",
        "        input.unsqueeze_(0)\n",
        "        output = fn.conv2d(input, self.kernels, padding = self.padding).float()\n",
        "        if not(self.thresholds is None):\n",
        "            output = torch.where(output < self.thresholds, torch.tensor(0.0, device=output.device), output)\n",
        "        if self.use_abs:\n",
        "            torch.abs_(output)\n",
        "        return output.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXSLZZoGS4z"
      },
      "source": [
        "# Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3DIAdG1mGS4z"
      },
      "outputs": [],
      "source": [
        "def plot_LC_timepoint_spikes(spikes: torch.Tensor,\n",
        "    timepoint: int,\n",
        "    n_filters: int,\n",
        "    in_chans: int,\n",
        "    slice_to_plot: int,\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    im: Optional[AxesImage] = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (10, 10),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(n_filters)))\n",
        "    sel_slice = spikes[timepoint].view(in_chans, n_filters, conv_size, conv_size).cpu()\n",
        "    sel_slice = sel_slice[slice_to_plot, ...].view(n_filters, conv_size, conv_size)\n",
        "    spikes_ = np.zeros((n_sqrt*conv_size, n_sqrt*conv_size))\n",
        "    filt_counter = 0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for n1 in range(n_sqrt):\n",
        "        for n2 in range(n_sqrt):\n",
        "            filter_ = sel_slice[filt_counter, :, :].view(conv_size, conv_size)\n",
        "            spikes_[n1 * conv_size : (n1 + 1) * conv_size, n2 * conv_size : (n2 + 1) * conv_size] = filter_\n",
        "            filt_counter += 1\n",
        "            ax.axhline((n1 + 1) * conv_size, color=\"g\", linestyle=\"-\")\n",
        "            ax.axvline((n2 + 1) * conv_size, color=\"g\", linestyle=\"--\")\n",
        "    ax.imshow(spikes_, cmap='Greys')\n",
        "    return spikes_\n",
        "    \n",
        "def plot_FC_response_map(lc: object,\n",
        "    fc: object,\n",
        "    ind_neuron_in_group: int,\n",
        "    label: int,\n",
        "    n_per_action: int,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param fc: FC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input\n",
        "    :param scale_factor: determines intensity of activation map  \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\t\n",
        "    ind_neuron = label * n_per_action + ind_neuron_in_group\n",
        "    w = fc.w[:,ind_neuron].view(reshaped.shape[0]//lc.kernel_size[0],reshaped.shape[1]//lc.kernel_size[1])\n",
        "    w = w.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*w, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_activation_map(lc : object,\n",
        "    spikes: torch.tensor,\n",
        "    input_channel: int = 0,\n",
        "    scale_factor: float = 1.0,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r'\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot an activation map of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param scale_factor: determines intensity of activation map \n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "    spikes = spikes.sum(0).squeeze().view(lc.conv_size[0]*int(np.sqrt(lc.out_channels)),lc.conv_size[1]*int(np.sqrt(lc.out_channels)))\n",
        "    x = scale_factor * spikes / torch.max(spikes)\n",
        "    x = x.clip(lc.wmin,lc.wmax).repeat_interleave(lc.kernel_size[0], dim=0).repeat_interleave(lc.kernel_size[1], dim=1).cpu()\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    sel_slice = sel_slice[input_channel, ...]\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu()*x, cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def reshape_LC_weights(\n",
        "    w: torch.Tensor,\n",
        "    n_filters: int,\n",
        "    kernel_size: Union[int, Tuple[int, int]],\n",
        "    conv_size: Union[int, Tuple[int, int]],\n",
        "    input_sqrt: Union[int, Tuple[int, int]],\n",
        ") -> torch.Tensor:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Get the weights from a locally connected layer and reshape them to be two-dimensional and square.\n",
        "    :param w: Weights from a locally connected layer.\n",
        "    :param n_filters: No. of neuron filters.\n",
        "    :param kernel_size: Side length(s) of convolutional kernel.\n",
        "    :param conv_size: Side length(s) of convolution population.\n",
        "    :param input_sqrt: Sides length(s) of input neurons.\n",
        "    :return: Locally connected weights reshaped as a collection of spatially ordered square grids.\n",
        "    \"\"\"\n",
        "    k1, k2 = kernel_size\n",
        "    c1, c2 = conv_size\n",
        "    i1, i2 = input_sqrt\n",
        "    c1sqrt, c2sqrt = int(math.ceil(math.sqrt(c1))), int(math.ceil(math.sqrt(c2)))\n",
        "    fs = int(math.ceil(math.sqrt(n_filters)))\n",
        "\n",
        "    w_ = torch.zeros((n_filters * k1, k2 * c1 * c2))\n",
        "\n",
        "    for n1 in range(c1):\n",
        "        for n2 in range(c2):\n",
        "            for feature in range(n_filters):\n",
        "                n = n1 * c2 + n2\n",
        "                filter_ = w[feature, n1, n2, :, :\n",
        "                ].view(k1, k2)\n",
        "                w_[feature * k1 : (feature + 1) * k1, n * k2 : (n + 1) * k2] = filter_\n",
        "\n",
        "    if c1 == 1 and c2 == 1:\n",
        "        square = torch.zeros((i1 * fs, i2 * fs))\n",
        "\n",
        "        for n in range(n_filters):\n",
        "            square[\n",
        "                (n // fs) * i1 : ((n // fs) + 1) * i2,\n",
        "                (n % fs) * i2 : ((n % fs) + 1) * i2,\n",
        "            ] = w_[n * i1 : (n + 1) * i2]\n",
        "\n",
        "        return square\n",
        "    else:\n",
        "        square = torch.zeros((k1 * fs * c1, k2 * fs * c2))\n",
        "\n",
        "        for n1 in range(c1):\n",
        "            for n2 in range(c2):\n",
        "                for f1 in range(fs):\n",
        "                    for f2 in range(fs):\n",
        "                        if f1 * fs + f2 < n_filters:\n",
        "                            square[\n",
        "                                k1 * (n1 * fs + f1) : k1 * (n1 * fs + f1 + 1),\n",
        "                                k2 * (n2 * fs + f2) : k2 * (n2 * fs + f2 + 1),\n",
        "                            ] = w_[\n",
        "                                (f1 * fs + f2) * k1 : (f1 * fs + f2 + 1) * k1,\n",
        "                                (n1 * c2 + n2) * k2 : (n1 * c2 + n2 + 1) * k2,\n",
        "                            ]\n",
        "\n",
        "        return square\n",
        "\n",
        "def plot_semantic_pooling(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            ax.axhline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i - 0.5, color=color, linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n",
        "\n",
        "def plot_LC_weights(lc : object,\n",
        "    input_channel: int = 0,\n",
        "    output_channel: int = None,\n",
        "    lines: bool = True,\n",
        "    figsize: Tuple[int, int] = (5, 5),\n",
        "    cmap: str = \"hot_r\",\n",
        "    color: str='r',\n",
        "    ) -> AxesImage:\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n",
        "    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n",
        "    :param lc: LC connection object of LCNet\n",
        "    :param input_channel: indicates weights which connected to this channel of input \n",
        "    :param output_channel: indicates weights of specific channel in the output layer\n",
        "    :param lines: Whether or not to draw horizontal and vertical lines separating input regions.\n",
        "    :param figsize: Horizontal, vertical figure size in inches.\n",
        "    :param cmap: Matplotlib colormap.\n",
        "    :return: Used for re-drawing the weights plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n_sqrt = int(np.ceil(np.sqrt(lc.out_channels)))\n",
        "    sel_slice = lc.w.view(lc.in_channels, lc.out_channels, lc.conv_size[0], lc.conv_size[1], lc.kernel_size[0], lc.kernel_size[1]).cpu()\n",
        "    input_size = _pair(int(np.sqrt(lc.source.n)))\n",
        "    \n",
        "    if output_channel is None:\n",
        "        sel_slice = sel_slice[input_channel, ...]\n",
        "        reshaped = reshape_LC_weights(sel_slice, lc.out_channels, lc.kernel_size, lc.conv_size, input_size)\n",
        "    else:\n",
        "        sel_slice = sel_slice[input_channel, output_channel, ...]\n",
        "        sel_slice = sel_slice.unsqueeze(0)\n",
        "        reshaped = reshape_LC_weights(sel_slice, 1, lc.kernel_size, lc.conv_size, input_size)\n",
        "        #print(reshaped.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=lc.wmin, vmax=lc.wmax)\n",
        "    div = make_axes_locatable(ax)\n",
        "    cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "    if lines and  output_channel is None:\n",
        "        for i in range(\n",
        "            lc.kernel_size[0],#n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt*lc.conv_size[0] * lc.kernel_size[0],#n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            lc.kernel_size[0],#,n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color=color, linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt*lc.conv_size[1] * lc.kernel_size[1],#n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            lc.kernel_size[1],#n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color=color, linestyle=\"--\")\n",
        "            \n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "            n_sqrt * lc.conv_size[0] * lc.kernel_size[0],\n",
        "            n_sqrt * lc.kernel_size[0],\n",
        "        ):\n",
        "            #print(i)\n",
        "            ax.axhline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "        for i in range(\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "            n_sqrt * lc.conv_size[1] * lc.kernel_size[1],\n",
        "            n_sqrt * lc.kernel_size[1],\n",
        "        ):\n",
        "            ax.axvline(i, color='b', linestyle=\"--\")\n",
        "\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_aspect(\"auto\")\n",
        "\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return im\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywXyWP0I83Au"
      },
      "source": [
        "# Design network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PcU9FSsVi4Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bba7e89-4026-47e5-b4ff-ad801f6b3b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ],
      "source": [
        "WANDB = True\n",
        "if WANDB:\n",
        "    !pip install -q wandb\n",
        "    !wandb login\n",
        "    import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8bZpJmlrJDa9"
      },
      "outputs": [],
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "def convergence(c):\n",
        "    if c.norm is None:\n",
        "        return 1-torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2\n",
        "    else:\n",
        "        mean_norm_factor = c.norm / c.w.shape[-1]\n",
        "        return  1-(torch.mean((c.w-c.wmin)*(c.wmax-c.w))/((c.wmax-c.wmin)/2)**2)\n",
        "\n",
        "        \n",
        "class LCNet(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: int,\n",
        "        n_actions: int,\n",
        "        neuron_per_action: int,\n",
        "        in_channels : int,\n",
        "        n_channels1: int,\n",
        "        n_channels2: int,\n",
        "        filter_size1: int,\n",
        "        filter_size2: int,\n",
        "        stride1: int,\n",
        "        stride2: int,\n",
        "        maxPool1: bool,\n",
        "        maxPool2: bool,\n",
        "        online: bool,\n",
        "        deep: bool,\n",
        "        reward_fn,\n",
        "        n_neurons: int,\n",
        "        pre_observation: bool,\n",
        "        has_decision_period: bool,\n",
        "        local_rewarding: bool,\n",
        "        nu_LC: Union[float, Tuple[float, float]],\n",
        "        nu_LC2: Union[float, Tuple[float, float]],\n",
        "        nu_Output: float,\n",
        "        dt: float,\n",
        "        crop_size:int ,\n",
        "        nu_inh_LC: float,\n",
        "        nu_inh: float,\n",
        "        inh_type,\n",
        "        inh_LC: bool,\n",
        "        inh_LC2: bool,\n",
        "        inh_factor_LC: float,\n",
        "        inh_factor_LC2: float,\n",
        "        inh_factor:float,\n",
        "        single_output_layer:bool,\n",
        "        NodesType_LC,\n",
        "        NodesType_Output, \n",
        "        update_rule_LC,\n",
        "        update_rule_LC2,\n",
        "        update_rule_Output,\n",
        "        update_rule_inh,\n",
        "        update_rule_inh_LC,\n",
        "        wmin: float,\n",
        "        wmax: float ,\n",
        "        soft_bound,\n",
        "        theta_plus: float,\n",
        "        tc_theta_decay: float,\n",
        "        tc_trace:int,\n",
        "        normal_init:bool,\n",
        "        mu: float,\n",
        "        std:float,\n",
        "        norm_factor_inh_LC: bool,\n",
        "        norm_factor_LC,\n",
        "        norm_factor_LC2,\n",
        "        norm_factor_out,\n",
        "        norm_factor_inh,\n",
        "        trace_additive,\n",
        "        load_path,\n",
        "        save_path,\n",
        "        LC_weights_path,\n",
        "        LC2_weights_path,\n",
        "        confusion_matrix,\n",
        "        lc_weights_vis,\n",
        "        out_weights_vis,\n",
        "        lc_convergence_vis,\n",
        "        out_convergence_vis,\n",
        "        thresh_LC,\n",
        "        thresh_FC,\n",
        "        num_episodes,\n",
        "        max_steps,\n",
        "        wandb_active = False,\n",
        "        batch_size=1,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructor for class ``BioLCNet``.\n",
        "\n",
        "        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n",
        "        :param n_neurons: Number of excitatory, inhibitory neurons.\n",
        "        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n",
        "        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n",
        "        :param dt: Simulation time step.\n",
        "        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n",
        "            respectively.\n",
        "        :param reduction: Method for reducing parameter updates along the minibatch\n",
        "            dimension.\n",
        "        :param wmin: Minimum allowed weight on input to excitatory synapses.\n",
        "        :param wmax: Maximum allowed weight on input to excitatory synapses.\n",
        "        :param norm: Input to excitatory layer connection weights normalization\n",
        "            constant.\n",
        "        :param theta_plus: On-spike increment of ``(adaptive)LIFNodes`` membrane\n",
        "            threshold potential.\n",
        "        :param tc_theta_decay: Time constant of ``(adaptive)LIFNodes`` threshold\n",
        "            potential decay.\n",
        "        :param inpt_shape: The dimensionality of the input layer.\n",
        "        \"\"\"\n",
        "        manual_seed(SEED)\n",
        "        super().__init__(dt=dt, reward_fn = None, online=online)\n",
        "        kwargs['single_output_layer'] = single_output_layer\n",
        "        kwargs['dt'] = dt\n",
        "        kwargs['n_labels'] = n_actions\n",
        "        kwargs['neuron_per_actionn'] = neuron_per_action\n",
        "        self.true_label = 0\n",
        "        self.dt = dt\n",
        "        self.intensity = kwargs['intensity']\n",
        "        self.reward_fn = reward_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_fn.network = self\n",
        "        self.reward_fn.dt = self.dt\n",
        "        self.n_actions = n_actions\n",
        "        self.neuron_per_action = neuron_per_action\n",
        "        self.n_classes = n_actions\n",
        "        self.neuron_per_class = neuron_per_action\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "        self.deep = deep\n",
        "        self.maxPool1 = maxPool1\n",
        "        self.maxPool2 = maxPool2\n",
        "        self.time = time\n",
        "        self.crop_size = crop_size\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.clamp_intensity = kwargs.get('clamp_intensity',None)\n",
        "        self.single_output_layer = single_output_layer\n",
        "        self.pre_observation = pre_observation\n",
        "        self.has_decision_period = has_decision_period\n",
        "        self.local_rewarding = local_rewarding\n",
        "        self.soft_bound = soft_bound\n",
        "        self.confusion_matrix = confusion_matrix\n",
        "        self.lc_weights_vis = lc_weights_vis\n",
        "        self.out_weights_vis = out_weights_vis\n",
        "        self.lc_convergence_vis = lc_convergence_vis\n",
        "        self.out_convergence_vis = out_convergence_vis\n",
        "        self.frame_analysis = frame_analysis\n",
        "        self.in_channels = in_channels\n",
        "        self.n_channels1 = n_channels1\n",
        "        self.n_channels2 = n_channels2\n",
        "        self.stride1 = stride1\n",
        "        self.stride2 = stride2\n",
        "        self.convergences = {}\n",
        "        self.norm_factor_LC = norm_factor_LC\n",
        "        self.norm_factor_LC2 = norm_factor_LC2\n",
        "        self.norm_factor_out = norm_factor_out\n",
        "        self.wmin = wmin \n",
        "        self.wmax = wmax\n",
        "        self.wandb_active = wandb_active\n",
        "        self.epochs_trained = 0\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_steps= max_steps\n",
        "        self.rew = 0.0\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.env.reset()\n",
        "\n",
        "        self.time_analysis = kwargs.get('time_analysis', False)\n",
        "        if kwargs['variant'] == 'scalar':\n",
        "            assert self.has_decision_period == True, ''\n",
        "\n",
        "        if self.online == False:\n",
        "            assert self.has_decision_period == True, ''\n",
        "        \n",
        "        if self.has_decision_period == True:\n",
        "            assert self.online == False, \"Decision period is not compatible with online learning.\"\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.decision_period = kwargs['decision_period']\n",
        "            assert self.decision_period > 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period - self.decision_period\n",
        "\n",
        "        elif self.pre_observation == True:\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period\n",
        "            self.decision_period = self.time - self.observation_period\n",
        "\n",
        "        else:\n",
        "            self.observation_period = 0\n",
        "            self.decision_period = self.time\n",
        "            self.learning_period = self.time\n",
        "\n",
        "        ### nodes\n",
        "        inp = Input(shape= [in_channels,crop_size,crop_size], traces=True, tc_trace=tc_trace,traces_additive = trace_additive)\n",
        "        self.add_layer(inp, name=\"input\")\n",
        "\n",
        "        ## First hidden layer\n",
        "        conv_size1 = compute_size(crop_size, filter_size1, stride1)\n",
        "        main1 = NodesType_LC(shape= [n_channels1, conv_size1, conv_size1], thresh = thresh_LC, traces=True, tc_trace=tc_trace,\n",
        "                             traces_additive = trace_additive,tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "        \n",
        "        self.add_layer(main1, name=\"main1\")\n",
        "\n",
        "        ### connections \n",
        "        LC1 = LocalConnectionOrig(inp, main1, filter_size1, stride1, n_channels1,\\\n",
        "                              nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, norm = norm_factor_LC)\n",
        "\n",
        "        # LC1 = LocalConnection(inp, main1, filter_size1, stride1, in_channels, n_channels1,input_shape=(crop_size,crop_size),\\\n",
        "        #                      nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC)\n",
        "\n",
        "\n",
        "        if LC_weights_path:\n",
        "            a = torch.load(LC_weights_path)\n",
        "            LC1.w.data = a['state_dict']['input_to_main1.w']\n",
        "            print(\"Weights loaded ...\")\n",
        "        \n",
        "        elif normal_init:\n",
        "            w_lc_init = torch.normal(mu,std, size = (in_channels, n_channels1 * compute_size(crop_size, filter_size1, stride1)**2, filter_size1**2))\n",
        "            LC1.w.data = w_lc_init\n",
        "       \n",
        "        self.add_connection(LC1, \"input\", \"main1\")\n",
        "        self.convergences['lc1'] = []\n",
        "\n",
        "        if inh_LC:\n",
        "            main_width = compute_size(crop_size, filter_size1, stride1)\n",
        "            w_inh_LC = torch.zeros(n_channels1,main_width,main_width,n_channels1,main_width,main_width)\n",
        "            for c in range(n_channels1):\n",
        "                for w1 in range(main_width):\n",
        "                    for w2 in range(main_width):\n",
        "                        w_inh_LC[c,w1,w2,:,w1,w2] = - inh_factor_LC\n",
        "                        w_inh_LC[c,w1,w2,c,w1,w2] = 0\n",
        "        \n",
        "            w_inh_LC = w_inh_LC.reshape(main1.n,main1.n)\n",
        "                                                             \n",
        "            LC_recurrent_inhibition = Connection(\n",
        "                source=main1,\n",
        "                target=main1,\n",
        "                w=w_inh_LC,\n",
        "            )\n",
        "            self.add_connection(LC_recurrent_inhibition, \"main1\", \"main1\")\n",
        "        \n",
        "        \n",
        "        self.final_connection_source_name = 'main1'\n",
        "        self.final_connection_source = main1\n",
        "\n",
        "        self.hidden2 = main1\n",
        "        self.hidden2_name = 'main1'\n",
        "        if maxPool1:\n",
        "            maxPool_kernel = 2\n",
        "            maxPool_stride = 2\n",
        "            \n",
        "            conv_size1 =compute_size(conv_size1, maxPool_kernel, maxPool_stride)\n",
        "            self.final_connection_source_name = 'maxpool1'\n",
        "            \n",
        "            maxpool1 = LIFNodes(shape= [self.n_channels1, conv_size1, conv_size1], refrac = 0)\n",
        "            self.add_layer(maxpool1, name=\"maxpool1\")\n",
        "            self.final_connection_source = maxpool1\n",
        "            \n",
        "            maxPoolConnection = MaxPool2dLocalConnection(main1, maxpool1, maxPool_kernel, maxPool_stride)\n",
        "            self.add_connection(maxPoolConnection, \"main1\", 'maxpool1')\n",
        "            \n",
        "            self.hidden2 = maxpool1\n",
        "            self.hidden2_name = 'maxpool1'\n",
        "\n",
        "        if deep:\n",
        "            # # Second hidden layer\n",
        "            conv_size2 = compute_size(conv_size1, filter_size2, stride2)\n",
        "\n",
        "            main2 = NodesType_LC(shape= [n_channels2, conv_size2, conv_size2],traces=True, tc_trace=tc_trace,traces_additive = trace_additive,\n",
        "                                            tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "            \n",
        "            self.add_layer(main2, name=\"main2\")\n",
        "\n",
        "            ### connections \n",
        "            lc2_input_shape = (conv_size1,conv_size1)\n",
        "            LC2 = LocalConnection(self.hidden2, main2, filter_size2, stride2, n_channels1, n_channels2, input_shape= lc2_input_shape,\n",
        "            nu = _pair(nu_LC2), update_rule = update_rule_LC2, wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC2)\n",
        "\n",
        "            self.add_connection(LC2,  self.hidden2_name, \"main2\")\n",
        "            self.convergences['lc2'] = []\n",
        "            if LC2_weights_path:\n",
        "                a = torch.load(LC2_weights_path)\n",
        "                LC2.w.data = a['state_dict']['main1_to_main2.w']\n",
        "                print(\"Weights loaded ...\")\n",
        "            \n",
        "            elif normal_init:\n",
        "                w_lc_init = torch.normal(mu,std, size = (n_channels1, n_channels2 * compute_size(conv_size1, filter_size2, stride2)**2, filter_size2**2))\n",
        "                LC2.w.data = w_lc_init\n",
        "\n",
        "            self.final_connection_source_name = 'main2'\n",
        "            self.final_connection_source = main2\n",
        "\n",
        "            if inh_LC2:\n",
        "                main_width = conv_size2\n",
        "                w_inh_LC2 = torch.zeros(n_channels2,main_width,main_width,n_channels2,main_width,main_width)\n",
        "                for c in range(n_channels2):\n",
        "                    for w1 in range(main_width):\n",
        "                        for w2 in range(main_width):\n",
        "                            w_inh_LC2[c,w1,w2,:,w1,w2] = - inh_factor_LC2\n",
        "                            w_inh_LC2[c,w1,w2,c,w1,w2] = 0\n",
        "            \n",
        "                w_inh_LC2 = w_inh_LC2.reshape(main2.n,main2.n)\n",
        "                                                                \n",
        "                LC_recurrent_inhibition2 = Connection(\n",
        "                    source=main2,\n",
        "                    target=main2,\n",
        "                    w=w_inh_LC2,\n",
        "                )\n",
        "                self.add_connection(LC_recurrent_inhibition2, \"main2\", \"main2\")\n",
        "\n",
        "\n",
        "            if maxPool2:\n",
        "                maxPool_kernel = 2\n",
        "                maxPool_stride = 2\n",
        "                conv_size2 =compute_size(conv_size2, maxPool_kernel, maxPool_stride)\n",
        "                self.final_connection_source_name = 'maxpool2'\n",
        "                maxpool2 = LIFNodes(shape= [self.n_channels2, conv_size2, conv_size2], refrac = 0)\n",
        "                self.final_connection_source = maxpool2\n",
        "                maxPoolConnection2 = MaxPool2dLocalConnection(main2, maxpool2, maxPool_kernel, maxPool_stride)\n",
        "\n",
        "                self.add_layer(maxpool2, name=\"maxpool2\")\n",
        "                self.add_connection(maxPoolConnection2, \"main2\", 'maxpool2')\n",
        "\n",
        "\n",
        "        ### main2 to output\n",
        "        out = NodesType_Output(n= n_neurons, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "        self.add_layer(out, \"output\")\n",
        "\n",
        "        last_main_out = Connection(self.final_connection_source, out, nu = nu_Output, update_rule = update_rule_Output, wmin = wmin, wmax= wmax, norm = norm_factor_out)\n",
        "\n",
        "        if normal_init:\n",
        "            w_last_main_init = torch.normal(mu,std,size = (self.final_connection_source.n,out.n)) \n",
        "            last_main_out.w.data = w_last_main_init\n",
        "\n",
        "        self.add_connection(last_main_out, self.final_connection_source_name, \"output\")\n",
        "        self.convergences['last_main_out'] = []\n",
        "        ### Inhibitory:\n",
        "        if inh_type == 'between_layers':\n",
        "            w = -inh_factor * torch.ones(out.n, out.n)\n",
        "            for c in range(n_actions):\n",
        "                ind = slice(c*neuron_per_action,(c+1)*neuron_per_action)\n",
        "                w[ind, ind] = 0\n",
        "\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        elif inh_type == 'one_2_all':\n",
        "            w = -inh_factor * (torch.ones(out.n, out.n) - torch.eye(out.n, out.n))\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        # Diehl and Cook\n",
        "        elif inh_type == 'DC':\n",
        "            raise NotImplementedError('Diehl and cook not implemented yet fo r 10 classes')\n",
        "\n",
        "        # Directs network to GPU\n",
        "        if gpu:\n",
        "            self.to(\"cuda\")\n",
        "\n",
        "    def frame_process(self, x):\n",
        "        x[x<1.0] = 2.0\n",
        "        x[x==1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "\n",
        "\n",
        "    def get_state_spiking(self):\n",
        "        intensity = self.intensity\n",
        "        screen = self.env.render(mode='rgb_array')\n",
        "        screen = screen.transpose((2, 0, 1))\n",
        "        _, screen_height, screen_width = screen.shape\n",
        "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "        view_width = int(screen_width * 0.6)\n",
        "        world_width = self.env.x_threshold * 2\n",
        "        scale = screen_width / world_width\n",
        "        cart_location = int(self.env.state[0] * scale + screen_width / 2.0)\n",
        "        if cart_location < view_width // 2:\n",
        "            slice_range = slice(view_width)\n",
        "        elif cart_location > (screen_width - view_width // 2):\n",
        "            slice_range = slice(-view_width, None)\n",
        "        else:\n",
        "            slice_range = slice(cart_location - view_width // 2,\n",
        "                                cart_location + view_width // 2)\n",
        "            \n",
        "        # Strip off the edges, so that we have a square image centered on a cart\n",
        "        screen = screen[:, :, slice_range]\n",
        "\n",
        "        # Convert to float, rescale, convert to torch tensor\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        h = screen.shape[1]\n",
        "        w = screen.shape[2]\n",
        "        resize = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize([80, 180]),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "                    transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "                    # transforms.Resize([80, 80]),\n",
        "                    #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "                    # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "                    #transforms.CenterCrop((crop_size, crop_size)),\n",
        "                    transforms.Grayscale(),\n",
        "                    transforms.ToTensor(),\n",
        "                    #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "                    transforms.Lambda(self.frame_process),\n",
        "                    #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "                    transforms.Lambda(lambda x: x * intensity),\n",
        "                    transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "        screen = resize(screen)\n",
        "        if self.frame_analysis:\n",
        "            f = screen.sum(axis=0)\n",
        "            f = f.to(device)\n",
        "            plt.imshow(f.cpu().numpy().squeeze(), cmap='gray')\n",
        "            plt.show()\n",
        "        return screen\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        hparams = None,\n",
        "        online_validate = True,\n",
        "        running_window_length = 250,\n",
        "        verbose = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        manual_seed(SEED)\n",
        "        if self.wandb_active:\n",
        "            wandb.watch(self)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "\n",
        "        self.add_monitor(reward_monitor, name=\"reward\")\n",
        "\n",
        "        reward_hist = collections.deque([], running_window_length)\n",
        "\n",
        "        self.spikes = {}\n",
        "        for layer in set(self.layers):\n",
        "            self.spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=None)\n",
        "            self.add_monitor(self.spikes[layer], name=\"%s_spikes\" % layer)\n",
        "            self.dopaminergic_layers = self.layers[\"output\"]\n",
        "       \n",
        "        self.episode = 0\n",
        "        rew = 0.0\n",
        "        tot_rew = 0.0\n",
        "\n",
        "\n",
        "        reward_history = []\n",
        "        if self.load_path:\n",
        "            \n",
        "            self.model_params = torch.load(self.load_path)\n",
        "            self.load_state_dict(torch.load(self.load_path)['state_dict'])\n",
        "            self.env = self.model_params['env']\n",
        "            self.episode =  self.model_params['episode']\n",
        "            hparams = self.model_params['hparams']\n",
        "            reward_hist = self.model_params['reward_hist']\n",
        "            print(f'Previous model loaded! Resuming training from episode {iteration}...\\n') if self.verbose else None\n",
        "        else:\n",
        "            self.env = gym.make('CartPole-v0')\n",
        "            self.env.reset()\n",
        "            print(f'Previous model not found! Training from the beginning...\\n') if self.verbose else None\n",
        "\n",
        "        pbar = tqdm(total=self.num_episodes)\n",
        "        \n",
        "        if self.time_analysis:\n",
        "            self.sample_spikes = {'input': [], 'main1': [], 'output': []}\n",
        "\n",
        "        for ep in range(self.num_episodes):\n",
        "            # print(f\"episode: {ep+1}\")\n",
        "            self.reset_state_variables()\n",
        "            self.env.reset()\n",
        "            done = False\n",
        "            tot_rew = 0.0\n",
        "            success = False\n",
        "            failure = False\n",
        "            num_steps = 0\n",
        "            for t in count():\n",
        "                \n",
        "                if t != 0:\n",
        "                    print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                    if self.time_analysis:\n",
        "                        self.sample_spikes['input'].append(self.spikes['input'].get('s'))\n",
        "                        self.sample_spikes['main1'].append(self.spikes['main1'].get('s'))\n",
        "                        self.sample_spikes['output'].append(self.spikes['output'].get('s').view(self.time, self.batch_size, n_actions, neuron_per_action))\n",
        "\n",
        "                        # w_lc1 = self.connections[('input', 'main1')].w\n",
        "                        # w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "                \n",
        "                image = self.get_state_spiking()\n",
        "                if gpu:\n",
        "                    inputs = {\"input\": image.cuda().view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "                else:\n",
        "                    inputs = {\"input\": image.view(self.time, self.batch_size, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "\n",
        "                clamp = {}\n",
        "                if self.clamp_intensity is not None:\n",
        "                    encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "                    clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "                # if done:\n",
        "                #     failure = True\n",
        "\n",
        "                if t >= self.max_steps:\n",
        "                    success = True\n",
        "                    done = True\n",
        "                elif done:\n",
        "                    failure = True\n",
        "\n",
        "\n",
        "                self.run(inputs=inputs, \n",
        "                        time = self.time,\n",
        "                        one_step=False,\n",
        "                        clamp = clamp,\n",
        "                        env = self.env,\n",
        "                        success = success,\n",
        "                        failure = failure,\n",
        "                        **kwargs,\n",
        "                        )\n",
        "                \n",
        "                rew = float(reward_monitor.get()[0])\n",
        "                tot_rew += rew\n",
        "                \n",
        "                lc_spikes1 = self.spikes['main1'].get('s')\n",
        "                #lc_spikes2 = self.spikes['main2'].get('s')\n",
        "                out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, self.batch_size, self.n_actions, self.neuron_per_action)\n",
        "                sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(2)\n",
        "                selected_action = torch.argmax(sum_spikes, dim=1)\n",
        "\n",
        "                self.spikes['main1'].reset_state_variables()\n",
        "                self.spikes[\"output\"].reset_state_variables()\n",
        "                reward_monitor.reset_state_variables()\n",
        "                #self.reset_state_variables()\n",
        "\n",
        "                if done:\n",
        "                    if success == True:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\Successful episode!')\n",
        "                        num_steps = t+1\n",
        "                    else:\n",
        "                        print(\"output\", sum_spikes, 'selected_action:',  selected_action, 'rew:', rew)\n",
        "                        print('\\nEpisode not successful!')\n",
        "                        num_steps = t+1\n",
        "                    break\n",
        "\n",
        "                obs, reward, done, _ = self.env.step(int(selected_action[0]))         \n",
        "                # Get voltage recording.\n",
        "                #main_voltage = main_monitor.get(\"v\")\n",
        "\n",
        "                #tensorboard.update(step= i)\n",
        "\n",
        "\n",
        "            if self.lc_weights_vis:\n",
        "                plot_locally_connected_weights(self.connections[('input','main1')].w, self.n_channels1, self.filter_size1,\n",
        "                                                compute_size(self.crop_size, self.filter_size1, self.stride1), self.connections[('input','main1')].locations,\n",
        "                                                self.crop_size ** 2)\n",
        "                plt.show()\n",
        "\n",
        "            if self.wandb_active:\n",
        "                wandb.log({\n",
        "                        **{'reward': tot_rew},\n",
        "                        **{' to '.join(name) + ' std': c.w.std().item() for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                        #**{name + ' spikes': monitor.get('s').sum().item() for name, monitor in self.spikes.items()},\n",
        "                        **{' to '.join(name) + \" gradients\": wandb.Histogram(c.w.cpu()) for name, c in self.connections.items() if name[0]!=name[1]},\n",
        "                    },\n",
        "                    step = self.episode)\n",
        "\n",
        "\n",
        "            #self.reward_fn.update() \n",
        "            #Plot_et.plot()    \n",
        "            # self.reset_state_variables()  # Reset state variables.\n",
        "            \n",
        "            self.episode += 1\n",
        "            print(f'\\nEpisode {self.episode} lasted for {num_steps} time steps with total reward of {tot_rew}\\n')\n",
        "            pbar.set_description_str(\"Episode: \"+str(self.episode)+ \"Number of steps: \" + str(num_steps) +\", Episode Total Reward: \" + \"{:.2f}\".format(tot_rew))\n",
        "            pbar.update()\n",
        "\n",
        "    \n",
        "    def single_trial(self):\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        image = self.get_state_spiking()\n",
        "\n",
        "        if gpu:\n",
        "            inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "        else:\n",
        "            inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step = False,\n",
        "                mode = 'RL',\n",
        "                env = 'CartPole'\n",
        "                )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqAFucAUDb8"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TerGeJoFdzg"
      },
      "outputs": [],
      "source": [
        "n_neurons = 2 #100\n",
        "n_actions = 2\n",
        "neuron_per_action = int(n_neurons/n_actions)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "crop_size = 40\n",
        "intensity = 128\n",
        "frame_analysis = False\n",
        "lc_weights_vis = False\n",
        "\n",
        "obs = 100\n",
        "dec = 250\n",
        "learn = 100\n",
        "time = obs+dec+learn\n",
        "\n",
        "max_steps = 100\n",
        "num_episodes = 1000\n",
        "\n",
        "filter_size1 = 24\n",
        "stride1 = 8\n",
        "n_channels1 = 50\n",
        "\n",
        "learning_rate = 0.1\n",
        "norm_factor = 0.25\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': crop_size,\n",
        "    'intensity': intensity,\n",
        "    'round_input': False,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': n_channels1,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': filter_size1,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': stride1,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_actions': n_actions,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'max_steps': max_steps,\n",
        "    'num_episodes': num_episodes,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': True,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': obs,\n",
        "    'decision_period': dec,\n",
        "    'time_analysis': False,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': PostPre,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDP,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.0001,0.01),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output': learning_rate,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': True,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': 'between_layers',\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': norm_factor*filter_size1*filter_size1,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': None,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25.pth',\n",
        "    'LC_weights_path': None,#'/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f15_s4_inh100_norm25_ch100_inh25_weights.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': lc_weights_vis,\n",
        "    'out_weights_vis': False,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': False,\n",
        "    'frame_analysis': False,\n",
        "\n",
        "    ## reward\n",
        "    'n_labels': n_actions,\n",
        "    'neuron_per_action': neuron_per_action,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': 1.,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0005,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokdidkrV2Z5"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Venb2KhSYrT_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f03e9dec1a8a4f4b8e78b35eefadd80e",
            "6549daa3fd7b4faebac94fe86774cd10",
            "dd1292e0c6f142f78421e85a772086f4",
            "6c5f35e306bc419a9a3eb618afdbee31",
            "8d7be4c0e9634af99e65b9eb046688fa",
            "3abda04055e8418c9d28a3f3ffd75faa",
            "b38071ca750840b2bdc034d9e7ad32d9",
            "67e5c7681d7946bdad74b62eb1f03977",
            "cf857a0b26f64096884d245b33114f44",
            "57b505ce9a4749b5928802d63663edaa",
            "8b63bb2c290047b38b401ee45062853f"
          ]
        },
        "id": "oThYyYvHJzeP",
        "outputId": "67f53297-dc0b-42d2-cf58-cda0a8655918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingularbrain\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/singularbrain/biolcnet/runs/227635dt\" target=\"_blank\">visionary-fog-119</a></strong> to <a href=\"https://wandb.ai/singularbrain/biolcnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous model not found! Training from the beginning...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f03e9dec1a8a4f4b8e78b35eefadd80e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5197284103843568\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.5187662006894782\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4904885457677084\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4348888358644979\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3517870189724218\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.24083456268496628\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.08914146667134665\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.09916753412467894\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.32054503765463893\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.575879497308158\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 1 lasted for 10 time steps with total reward of -18.3499570280527\n",
            "\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6631875507304001\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6642700352129531\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6936526948428906\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6455121630959106\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5561872431324494\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.43696925582964363\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.28733937993114134\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.10662622247161696\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.10597808974580253\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.35139622503653833\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.630637250378253\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 2 lasted for 11 time steps with total reward of -17.03426701991359\n",
            "\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6514152643953557\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.6522782563059265\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.680583667294213\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.6269630984184285\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5470132717324466\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4403817795733138\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2997877843356457\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.1190358959252032\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.09365034894324975\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.33919286848801\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 3 lasted for 10 time steps with total reward of -16.415384199450727\n",
            "\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5875806168146247\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.5859632024522222\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5540962690131044\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.4919700634205846\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3993996483903951\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.27602994164803063\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.12134452009976077\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -0.06532126451749387\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -0.284762050896354\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.537878123308168\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 4 lasted for 10 time steps with total reward of -17.871577176883292\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6904355468002393\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6917068845521173\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6666219095160458\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6135368566228511\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5256745079869004\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4077016775640314\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.25911342243111024\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.0792521414998163\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -0.13267618254659208\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.37758009206891\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 5 lasted for 10 time steps with total reward of -16.576213327642392\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.6171260393358635\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.6202187026373032\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5932453611587403\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5362264629081906\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.44900646419640755\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.33125816218689264\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.1824906146852362\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.002061142675479799\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.21080781705522444\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.45700603642502\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 6 lasted for 10 time steps with total reward of -17.336180903696132\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5832663750504037\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.582359953281499\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.6071410981089621\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6576061444324117\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6107527522274879\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.49463674216040854\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3485605851121856\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.1718585213725844\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.03627516212229204\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.2767659484413732\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.550632478608158\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 7 lasted for 11 time steps with total reward of -16.80749141742588\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6050270758657373\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6003389266714149\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.624662936903406\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6779700659591316\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6395917743987084\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5277014226146285\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3858711599048146\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.21345686062674507\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.00967234338945544\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.2263911033714578\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.495738898966437\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 8 lasted for 11 time steps with total reward of -16.437837436003853\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6873447950668071\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6891212913785584\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6645438498511433\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6136239032848008\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.532534478396064\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.41534026083736364\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2675481444832498\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.0885037341535056\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.12258422991919982\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.36662213132765\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 9 lasted for 10 time steps with total reward of -16.530645903795357\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6283497925193128\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6279468928339822\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5975962773397262\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5372943786202465\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.446861881959149\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.32594846812524014\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.17404113351908557\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.009523378154396855\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.22554123282180882\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.474918255398627\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 10 lasted for 10 time steps with total reward of -17.37194404145809\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5815238481569132\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5831127558096459\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.613554700360973\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6728603575621275\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6387831983766631\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.52101676599988\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3733162844089335\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.19500362680513883\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.01473828361134144\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.25684501967207485\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.532343713084966\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 11 lasted for 11 time steps with total reward of -16.62475547888811\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5028134121168952\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5070302834554544\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5396408518639366\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6006750305416094\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6903387733228312\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5909902410356005\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4427697295985049\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.264303280159892\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.0547549927793044\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.18683067934162478\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.461502719376515\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 12 lasted for 11 time steps with total reward of -16.45501680384411\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5306835876074887\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5312630549392419\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5571917810758567\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.6084746815526635\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6604368266404488\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5445045812834565\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3989190416193731\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.223005448102634\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.015948150803410666\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.22318983746316845\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.495440982658142\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 13 lasted for 11 time steps with total reward of -16.648203666496737\n",
            "\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5958167142996074\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.5964908085793175\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6260966785602771\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6837430453306547\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.6276953869834998\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5105559795525408\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.3634216151137454\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.1856184193206818\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.023666461285655027\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.26536434677229825\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.54049801771092\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 14 lasted for 11 time steps with total reward of -16.64009017802855\n",
            "\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.6396760176366894\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.6389305442193459\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6115070602854296\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.5542393378421191\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.463370339511631\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.34203127182458093\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.18970784596930468\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.005735566346552035\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.21068291764888936\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.46045455199782\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 15 lasted for 10 time steps with total reward of -17.225939486011058\n",
            "\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.6431369929182909\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.6417290085114458\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.613703595488072\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.5590520031558718\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.47749191458018037\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.3558056989916061\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.20318073252589186\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.018948758332378857\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.19769130441957783\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.447650127041236\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 16 lasted for 10 time steps with total reward of -17.132292726957076\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.457476076961726\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.45492653167544084\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.48041731740031257\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5339314297554303\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.608302830056053\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6741590996241956\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.534931386890444\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3660305972541922\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.16665344852025799\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.06413138869213936\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.32736006394831035\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.624143265322672\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 17 lasted for 12 time steps with total reward of -16.73880599982507\n",
            "\n",
            "output tensor([[ 1, 15]]) selected_action: tensor([1]) rew: 0.6151420101194355\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.6152576170348517\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6411955293010944\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.6864023423321747\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5991731660473779\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4823481371586328\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.33541335725301624\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.15770081834277277\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.051596308205609676\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.29340298900533096\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.568735120701604\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 18 lasted for 11 time steps with total reward of -16.781101440323187\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6345735139214714\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6320188693564356\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6027494947593535\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5428021769720712\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.45026629445895316\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.32719320856440803\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.1730597162045014\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.012806520478134187\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.23120920924979854\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.483059670103096\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 19 lasted for 10 time steps with total reward of -17.364412125593834\n",
            "\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.5806896476891336\n",
            "output tensor([[16,  0]]) selected_action: tensor([0]) rew: 0.5764131293316496\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5451307069433987\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.48681842708440437\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4012786986727759\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.2772903879397286\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.12035387091910427\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.0686421245384613\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.29050141829852366\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.5461309875663\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 20 lasted for 10 time steps with total reward of -17.91729966182309\n",
            "\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.6089828056779824\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6134442439169936\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6437448566843406\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.660228475314942\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5679736697057053\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.44601876936504603\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.29382327465533886\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.11069492131751213\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.10419413462817789\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.35178553740652724\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.633106219787145\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 21 lasted for 11 time steps with total reward of -17.14417487518399\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5745638018041646\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5792774032051539\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6096093185508198\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6655878367761418\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6067472070378477\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.486435729313328\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.33611012631541737\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.15507763095463623\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.05749156633293906\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.3025432113267245\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.581111269735086\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 22 lasted for 11 time steps with total reward of -16.92773699343724\n",
            "\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.5480327106118081\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5448631318023281\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5112307042719695\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.44711630202427344\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.3523261881787676\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2264973807436389\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.0691069309340605\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.12051432332349254\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.34316271083735195\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.599736399626988\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 23 lasted for 10 time steps with total reward of -18.364240085220985\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.527427841157042\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5302989827940575\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5058837590315463\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4542022348306233\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.37510050992911703\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.26825511176095684\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.12208755004144634\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.06064911772010756\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.27628539689271814\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.525697327456733\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 24 lasted for 10 time steps with total reward of -18.07937585252477\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5436446810521887\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5408239830215179\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5108055932622556\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4535702686498214\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3689253548031167\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.24967598509343814\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.0918305122654356\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.09824066214062843\n",
            "output tensor([[2, 1]]) selected_action: tensor([0]) rew: -0.32133414256730697\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.578348404654434\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 25 lasted for 10 time steps with total reward of -18.238646831214595\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6097248792123847\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6108463253773765\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6378146849634948\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6717333797534424\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5839246004169796\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4664810574719842\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3188845973595187\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.14046340632569732\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.06959250907646819\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3122104027482105\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.588407299322668\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 26 lasted for 11 time steps with total reward of -16.930337280266468\n",
            "\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4481912405013341\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.44573025913297437\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4122475623984686\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.34772942082884084\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.2519886592277101\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.12467025330491321\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.03473884454121284\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.22689509697001603\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -20.452578189241244\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 27 lasted for 9 time steps with total reward of -18.683654735358232\n",
            "\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5769428468718708\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.5773639313655086\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.5508152447120318\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.4973021625167594\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4166558234715013\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.30853766139265104\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.16648493866481395\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.016959194573306635\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -0.23307666007587424\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.48276065906748\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 28 lasted for 10 time steps with total reward of -17.638693904721524\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.6305485028147336\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6340223851451714\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.666681390653192\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6639866820124285\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5801765765792571\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4591236900061987\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.30775849830412594\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.12539490946687726\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.08878887055123741\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3357285205538994\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.616445049412373\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 29 lasted for 11 time steps with total reward of -16.973269805535526\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6761801287843189\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6753790066683225\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6810157372828172\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6234808990122164\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5363083778789334\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4191566334888336\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.27151959451931684\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.09273804300841199\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.11798441896771128\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.3615598245905887\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.638987612226725\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 30 lasted for 11 time steps with total reward of -17.142753435141852\n",
            "\n",
            "output tensor([[ 0, 19]]) selected_action: tensor([1]) rew: 0.5953492728778372\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5989662935933309\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6316122031725677\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6799947025876588\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.599468472375509\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.49257743672191934\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.34447447699723877\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.16292663349291936\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.050280072174615054\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.2960838223577792\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.575510348523924\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 31 lasted for 11 time steps with total reward of -16.816504751237336\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5975124311706355\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6005763453883679\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6326076977124329\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6936252065102101\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6161761684981547\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4964296920612672\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.34660439595446757\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.16601624260768594\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.04615628004567529\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.2908511086265092\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.56909495718234\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 32 lasted for 11 time steps with total reward of -16.756554165951304\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6031140237709871\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6052426259538887\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5804871261242635\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5288604291913392\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4436052047974828\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.324424244536507\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.17412195881894416\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.00794948590950445\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.22257143599206958\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.4706355885059\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 33 lasted for 10 time steps with total reward of -17.44130089721406\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5904595986995383\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.590643228376346\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6165846255193094\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6682856893517642\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5930239353699372\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.47594430868131365\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.328812512143159\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.15095856973623267\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.05842648901395886\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.3002700593117026\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.575590887402452\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 34 lasted for 11 time steps with total reward of -16.919574967850515\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6552899794005121\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6505750324722273\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6193363552488802\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5615462459998642\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4770028156662631\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3607042024640388\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2056076902634183\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.018908733126978883\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.2002083824100409\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.452666925860274\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 35 lasted for 10 time steps with total reward of -17.10390425362813\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5020212308356241\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5045417669741384\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.47637468276141914\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4175339220598041\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.327858641692201\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.20701798888953693\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.05451984064529031\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.13027598621479874\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.34813928111540676\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.599944786132774\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 36 lasted for 10 time steps with total reward of -18.588491979604964\n",
            "\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5319914583261705\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.5322377533633682\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.5019776455447044\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.4412108407453692\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.3497621460973305\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.2272864582366042\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.0732776418099147\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.11291816967437274\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: -0.33208405712165745\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: -20.585107609464025\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 37 lasted for 10 time steps with total reward of -18.372365892136592\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6634532513769079\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.662185569426128\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6344185032584518\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5801454922683418\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4981410076125772\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.37753369301649575\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.2261138353871076\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.0432137204232344\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.1719682555088572\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.420344770365222\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 38 lasted for 10 time steps with total reward of -16.907107953104834\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6397744827082563\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6369026058601596\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6040884156638239\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5413144563674563\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.44838777609046776\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3249449384059604\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.17046064298045072\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.015739488001517976\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.23446113932698526\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -20.48661754084253\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 39 lasted for 10 time steps with total reward of -17.37094485009446\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.46653336521995015\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4710808812235522\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5037538935894049\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5645829631066385\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6537745670254018\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6282930738462976\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4810768388046748\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3038787647559753\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.09586012307628011\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.14393945454975182\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.416576623147588\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 40 lasted for 11 time steps with total reward of -16.391681607049165\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6606630124195045\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6625632403851199\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6379200302437481\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5867469270832187\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4997430354345712\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3823570332228925\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.23419192459193094\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.05459680401024053\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.15721512271248295\n",
            "output tensor([[1, 1]]) selected_action: tensor([0]) rew: -20.402144350255863\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 41 lasted for 10 time steps with total reward of -16.84057746557712\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6168899005408295\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.618211832563851\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5928207690077189\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5407247453388597\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4617571802807977\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.35558122671090997\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.21358172984251123\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.0316933426018185\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.18260603582706364\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.43021545721279\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 42 lasted for 10 time steps with total reward of -17.181560766152558\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4812308710528935\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4768045731810612\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4980783506962356\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5429651808036294\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6128876361883016\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6587072468915264\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5208818600497742\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.353326481318893\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1552491216514853\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.07427105336856699\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.3362606395699208\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.63182189042934\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 43 lasted for 12 time steps with total reward of -16.742222261534025\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.61814042674616\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6202256024040291\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5923167747342553\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5344293792894649\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.44640284214649695\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3279049758536041\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.17843993343117615\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.00263977631169221\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2161164380423627\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.46288446482338\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 44 lasted for 10 time steps with total reward of -17.363780744572246\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.4780557049300186\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.47736482926741686\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.44576680633928534\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3832601189250475\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2896692584382037\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.16464996941262278\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.007698623623687395\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.1818337421811903\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.40472193551875\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 45 lasted for 9 time steps with total reward of -18.340090366763658\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.47385820383677135\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.47581605939138183\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4468754945718829\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3870479669331113\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.29617067258589136\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1739114898130273\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.019778015302281626\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.1668687943375713\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.386795602450373\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 46 lasted for 9 time steps with total reward of -18.280206494353596\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5609818640128191\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.565602785570705\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5398225088145954\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4836690153175919\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3969947061847726\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.27948073938926443\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.13064512922019306\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.050144907358485424\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.26365560085312356\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.510763319266957\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 47 lasted for 10 time steps with total reward of -17.867367078968623\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5246146866533205\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5207307734595764\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.48955170501824086\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.43105268245990913\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.34503578987464634\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.23113532536947767\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.07223741806164152\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.11932112051328209\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.344024165011035\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 48 lasted for 9 time steps with total reward of -17.848986904627505\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6707630407544428\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6746503698104381\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.648903126429123\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5935484275331715\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5084368076660579\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3932462255552739\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.24748940127941965\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.07052496421699228\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.13842683290981572\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.38026285969658113\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.655969220166135\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 49 lasted for 11 time steps with total reward of -17.367096549527613\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5819326839526283\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.578292524542487\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6035462124128093\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6576683374079658\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6575920093647458\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5467028313497556\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4043801023553798\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.23157359021402213\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.027493177791206413\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.20877422391973666\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.478239081657946\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 50 lasted for 11 time steps with total reward of -16.397831836186683\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6450879425906152\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6483538607674083\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6808614422775672\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6573687740749489\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5661401528293465\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4450846158942964\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.29367052509654756\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.11121434557364701\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.10310309281876368\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.350214712095488\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.631138554096793\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 51 lasted for 11 time steps with total reward of -17.03667469990667\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6033848180672955\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6009318261955654\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6275496837102461\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.683224038318367\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6143734031856534\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5125232601502708\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.37293328774210777\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.19772013846473624\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.0090003268157407\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.24814455458564416\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.520722481408182\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 52 lasted for 11 time steps with total reward of -16.565226906975326\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6681107870013241\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6681721237471203\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6632852243396884\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6042223521801942\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5154858313826503\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.39672710799451394\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.2474335528309941\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.06694006139820691\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.14555471145453913\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.39096647971668\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 53 lasted for 10 time steps with total reward of -16.706144150296527\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5664470878429333\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5688489811324345\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5441201297093535\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.4888411201104451\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.39995875824218585\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.28021102044349877\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.1291044214108073\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.05400425651059859\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.2698902220994722\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.519436901424225\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 54 lasted for 10 time steps with total reward of -17.865799861142637\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6110665769996486\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.606647949591864\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6312700162726277\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.684906342850847\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6322925936061057\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5200031622665705\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.37773534677564313\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.20484299682369111\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.0005383103541937162\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.23608847682880507\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.50604307149264\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 55 lasted for 11 time steps with total reward of -16.472828252780253\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4667098646278638\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.46290732173390936\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4314265021101633\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.37224585264104304\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.28517145606669003\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.16216316830647054\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.0018707799999228558\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1911265732761951\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.417615552071616\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 56 lasted for 9 time steps with total reward of -18.426247179861747\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5671476543252515\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.566723597383236\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5951165076546422\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6523223706373636\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.653433957218204\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5459639665375045\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4006000526558875\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.22472832575103463\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.01754204036186735\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.22188631081371346\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.494578779586845\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 57 lasted for 11 time steps with total reward of -16.49288661787557\n",
            "\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.5438529926443256\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.5446867905237737\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.5149828173155278\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.45474869847275745\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.36381707639442906\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.24185044609191486\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.0883498800728576\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.09733183844787696\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.3159718910580572\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.568452941430223\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 58 lasted for 10 time steps with total reward of -18.229467969420572\n",
            "\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.6029180529829135\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.6056708594570471\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5782703371637463\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.5207316536536087\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.432894195788168\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.3144260500242474\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.16483213191390284\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -0.016533530266340135\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2304505830891434\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.47780923179479\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 59 lasted for 10 time steps with total reward of -17.50505006416664\n",
            "\n",
            "output tensor([[ 0, 21]]) selected_action: tensor([1]) rew: 0.5818418811974699\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5859922030608586\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.563095484475476\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.508741169048974\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.42269469179283736\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3059054312928212\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.15789101706546138\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -0.021982212661453393\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.2344820835664841\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.480487552469498\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 60 lasted for 10 time steps with total reward of -17.610789970763538\n",
            "\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.47402363428319727\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.47091515497378644\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.44017643330003897\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.38179152467481436\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.2955718887610158\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.17757805653634856\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.01880181062907399\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.17260708577702988\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.39743297062001\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 61 lasted for 9 time steps with total reward of -18.311181553238765\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6500308213814285\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6502769158718597\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6766212626663762\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6313292900711596\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5424538524649732\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.42369650488062127\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.27454175731331043\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.09432125493271248\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.11777024081971654\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.36265330138705104\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.641335593457388\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 62 lasted for 11 time steps with total reward of -17.178487476081713\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6098367206523243\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6110395263397844\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6412619337074907\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6993628362267506\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6110175083400855\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4929743125458418\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.34483321270158196\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.16591756100783034\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.04458735741743158\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2876134297030898\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.5641822537336\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 63 lasted for 11 time steps with total reward of -16.72013942933243\n",
            "\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.4887312011485976\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4857443636612555\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.45516577326418306\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.39618964149594826\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.30034977747732006\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.17308800214794928\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.013886331561939524\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -0.177917302486297\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: -20.403109130215864\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 64 lasted for 9 time steps with total reward of -18.267871341944968\n",
            "\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5038113132746282\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.5047523379869926\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.47820777502389145\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.4241827640470568\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.336057061410053\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2130249104099743\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.058244020616844805\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.12893176803523543\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.349276648686654\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 65 lasted for 9 time steps with total reward of -17.95992823395245\n",
            "\n",
            "output tensor([[15,  1]]) selected_action: tensor([0]) rew: 0.45730994336425745\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.46056532540136563\n",
            "output tensor([[15,  0]]) selected_action: tensor([0]) rew: 0.48867325428977504\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.5416531006934839\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.6196987888964571\n",
            "output tensor([[14,  0]]) selected_action: tensor([0]) rew: 0.6197190685260885\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4748754516401419\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.30013415445055514\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.09466438079829076\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.1424879266392065\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.412375273547227\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 66 lasted for 11 time steps with total reward of -16.497569732126017\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.47673979590903204\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.47318608634640447\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.44198577033931596\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3831196863288837\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.2934904908776126\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1659549173974289\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.0064185888611596464\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.1857810034120972\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.411429819887005\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 67 lasted for 9 time steps with total reward of -18.356315487239264\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.608476460439079\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6113375905794641\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6432356891819628\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6898215156917873\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6056017684497875\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.485773624766484\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3357932763411625\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.15497626251541347\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.057498830398156564\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.30256908665243815\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.581259338343685\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 68 lasted for 11 time steps with total reward of -16.80631106742914\n",
            "\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.449839675881783\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4491277066173729\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4174064091437959\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.35467294369694946\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.2607507637646631\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.13529495085313326\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.022198256757759993\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.21237720552704353\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -20.436015049457\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 69 lasted for 9 time steps with total reward of -18.60349806178411\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5852890886731497\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5807961011509754\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6019396859937656\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6486936679303886\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6426147454661708\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5302320348332439\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3880277766776671\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2153549125366584\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.011423929415685097\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.22467786469076462\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.493960819620426\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 70 lasted for 11 time steps with total reward of -16.514266741633485\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5941615393058084\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5942774058946151\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.620101217530298\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6716347465932616\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6124767265233313\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.495576546721356\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.34867470270930456\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.17110101081978002\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.0379539994396495\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2794186573897818\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.554313246416697\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 71 lasted for 11 time steps with total reward of -16.763682007148372\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5154928588283408\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5142426492919112\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.48239539826266553\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4199465210893021\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.32671679940542653\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.20235751391537726\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.046359531136058896\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.14193311662899444\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3633043600373859\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.61864095781148\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 72 lasted for 10 time steps with total reward of -18.616367162548777\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.602122433598983\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6039004965830155\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5756172199013281\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5172815612622305\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4287266672680311\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.30961443078035056\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.15944365976563113\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.022437627843565622\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2368150001681727\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.48458451554334\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 73 lasted for 10 time steps with total reward of -17.547130674395508\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6110689710729916\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6099627193307292\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5787468878659888\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5174132849038324\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4257782829854807\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.30348774498952236\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.15002563094260096\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.03527317131226576\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.25320498555204213\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.504673461797882\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 74 lasted for 10 time steps with total reward of -17.596668096571044\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6268234080679685\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6284375482751525\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6000781215165693\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5417547503872245\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4533011162366908\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3343794582996312\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.18448863298326912\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.0029762460211245068\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2109443604699618\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: -20.458171281561874\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 75 lasted for 10 time steps with total reward of -17.296876360244205\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5210314783736106\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5220154620146371\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5482964545049263\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5998804898274247\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6674542495361037\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.551795368974191\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.40653144532872976\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.23098653834639915\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.024343660023213032\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21433577693998973\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.486086226129967\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 76 lasted for 11 time steps with total reward of -16.628086856140722\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6645644745798548\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6672681273932322\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6434362280050209\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5881647140312632\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5019855704347724\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.3855493647101459\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.23836225872689254\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.059777591358443904\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.1509878289099602\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -20.394831752969495\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 77 lasted for 10 time steps with total reward of -16.79671125263983\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6089974560272927\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6072324968857807\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6312825983685634\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6811348742535112\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6040108209559678\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.48944729041282153\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.34483133903764196\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.1695057133038418\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.03732703076291277\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2765843167755915\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.54927690374937\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 78 lasted for 11 time steps with total reward of -16.726745662042454\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.49525002421815045\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.49201593373128827\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4580325269191827\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3932798893650471\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.29756412176524794\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.17052290502713507\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.011635148294072473\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.17976471596527965\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.40446697506626\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 79 lasted for 9 time steps with total reward of -18.265931141711416\n",
            "\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.46989035259954237\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4747614209576455\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.45198680763526045\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.40159522728331143\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3234420129647274\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.21212973414627911\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.060771725641753316\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.12302857229961361\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.340026136862843\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 80 lasted for 9 time steps with total reward of -18.06847742793394\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5978421985169748\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5960774284894139\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5641775421941844\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5021310002457761\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4097509298098442\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.28668013302355444\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.132399799207908\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.053757530419367094\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.2725899351603821\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.52500202784845\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 81 lasted for 10 time steps with total reward of -17.762290461940545\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6640078155502763\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6682354944367154\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6670866042220017\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6100796756737451\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5266405483617402\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.41639721565956767\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.265663211749236\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.08087402178106778\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.136021398824135\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.38595497506204\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 82 lasted for 10 time steps with total reward of -16.622991786451827\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5358333862059691\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5408676411634384\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5744733349188937\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6366818788045236\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6722992649774683\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.552091024815487\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.40215006147212873\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.22177956603673343\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.010144333221577151\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.2337088542086097\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: -20.51082436559667\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 83 lasted for 11 time steps with total reward of -16.59821272818906\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5964827334866277\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5978933170873889\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5692015074789942\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5104149404708329\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4213655220154605\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.30171403319696155\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.15095837321222133\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.031554042624865886\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.24660894494138685\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.495101949755473\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 84 lasted for 10 time steps with total reward of -17.62523451037324\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5821606764676298\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5840745635359028\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6115695703017312\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6646592706481382\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6332714244322242\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5149716409494733\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.3667519115314327\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.18793144346279844\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.022309339769235437\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.26490831098014594\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.540894661574313\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 85 lasted for 11 time steps with total reward of -16.682721810994366\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.48277970608244614\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4840782393525368\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4577641069043996\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4038431814378325\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.31347226236005377\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.1898120621252678\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.03428004095837395\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.15376746741687458\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.37510122686037\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 86 lasted for 9 time steps with total reward of -18.162839095056334\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6146300188129158\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6106279511336837\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5798436754454211\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5222552303160927\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.43766665647793557\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3206940254952796\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.16537742834267588\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.02176972728897686\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.24155444925796377\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.494889583161033\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 87 lasted for 10 time steps with total reward of -17.50711877368397\n",
            "\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.4852062900178651\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.48325834059653505\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.4537816657866053\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.396766178510903\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.31202884970771794\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.1992188671581765\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.048228025540115405\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.14164348296320467\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.364805039522906\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 88 lasted for 9 time steps with total reward of -18.127960305168195\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6646115738484545\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6615795181966946\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6879729989085455\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6437824523898743\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.568664191172836\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4556222319125497\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3099915513634306\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.133329742426625\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.07515039214077518\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.31635408374067897\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -20.591277113831357\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 89 lasted for 11 time steps with total reward of -16.8572273294938\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6119628762393625\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6092921265798984\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5764962144445396\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5135594751791509\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4202909751993962\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2963295979268984\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.14115283828615843\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.04591013889592899\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.26566036082168015\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.51900482909106\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 90 lasted for 10 time steps with total reward of -17.661491224953263\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5941069590341853\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5955163169930727\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6259218328173751\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6853319872435548\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6158277073291748\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5079221501735403\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3597077687283178\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.1807497558528084\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.029765694891476957\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.272769990838138\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.549284824758786\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 91 lasted for 11 time steps with total reward of -16.68673603231637\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.649230084078033\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6500256354452983\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6800876981915233\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6605783289070155\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5717907023376594\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4531957385253126\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.3042754232609437\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.1243587541997424\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.08736238694624643\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3318118230472258\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.61000100857946\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 92 lasted for 11 time steps with total reward of -16.935632853627403\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5431588307640973\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.54430442771817\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5176781589768574\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4585578396276534\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.368734641696952\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2478732446567441\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.09547679771706635\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.0890999935638262\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.30663259587964353\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.558002227067064\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 93 lasted for 10 time steps with total reward of -18.177950875352995\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.46415577670284713\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4624279361532735\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4330450385122353\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.37599838226216764\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.2911065791165134\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.17802075644970217\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.02645035933882789\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.16395071368320308\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.387754568366624\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 94 lasted for 9 time steps with total reward of -18.32050045351426\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5895024810473001\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5862817641552218\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6087706207490231\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6569476466335862\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6228632306897383\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5099621316152089\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.36714758504116585\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.1937688821663881\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -0.01096629686579309\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.24797225047867855\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.51825930480619\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 95 lasted for 11 time steps with total reward of -16.64195351005303\n",
            "\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.4881453934101798\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.49098493877441507\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.46299893398854286\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.40420618274609565\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.3144509235500135\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.19340756976329643\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: 0.04058948151058678\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.1446377299452628\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.363037600342746\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 96 lasted for 9 time steps with total reward of -18.11289190654488\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.47927502916465947\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4743204393746496\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4384718282953449\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.3717015557649803\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.27380851723455246\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.14442398027963432\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -0.016978389542305128\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.21106773734376688\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.43863564112635\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 97 lasted for 9 time steps with total reward of -18.484680417898602\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5722401431710387\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5689121929645128\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5944624937735793\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6488685903302944\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6503971178207778\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5509634751741642\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.41237503969060996\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.23930692632551054\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.03496415383236706\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.201568459512109\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.471303138314525\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 98 lasted for 11 time steps with total reward of -16.40038146474378\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.5800020869385111\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.579157376828476\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5512903228317343\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4963954464630945\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4086149824678901\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.28572435739937463\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.1315042657775939\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.05470619679728361\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.27369822235876606\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.526369024972\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 99 lasted for 10 time steps with total reward of -17.822084605421374\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6232641367815345\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6203480075372251\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5906539917138882\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5324336828836996\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4393413121987009\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.3156420574377615\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.16081122553401372\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.02582450818499149\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.24506911802623055\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.49783320500648\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 100 lasted for 10 time steps with total reward of -17.48623241713088\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6217220843936434\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6179610008175808\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6400477190145306\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6879583028540546\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6121641562617159\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4989911202740297\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.35577566031656105\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.1818700121271254\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.02351471774483771\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2612886360770093\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.532456771890278\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 101 lasted for 11 time steps with total reward of -16.60077006965288\n",
            "\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4981866954224399\n",
            "output tensor([[13,  0]]) selected_action: tensor([0]) rew: 0.4937304853537803\n",
            "output tensor([[12,  0]]) selected_action: tensor([0]) rew: 0.4584909782808376\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: 0.3924420762180779\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: 0.29538391182640866\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: 0.1669485752482106\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.006609960309585161\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.18630169591614953\n",
            "output tensor([[9, 0]]) selected_action: tensor([0]) rew: -20.41257975870864\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 102 lasted for 9 time steps with total reward of -18.28708877196545\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5416348975239063\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5439534577155006\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.574938278768877\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6346068388875378\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6597642601056677\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5543699393736254\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.41149442456445096\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.2334724265482243\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.024167646478544025\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21736309817268268\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -20.492155495174146\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 103 lasted for 11 time steps with total reward of -16.531116423380496\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.517907944815571\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5180592869003684\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4908175558569865\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.43618306370050264\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.3499625100724446\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.22646289138956233\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.07128780061030415\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1162140664607294\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.3368221633720501\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.53314596306352\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 104 lasted for 10 time steps with total reward of -18.37550113955056\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5459846116757734\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5415489780146551\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5625594509253152\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6089873256542513\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6666964227886801\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5558955053150032\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4154744677863118\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.24478751795399478\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.043045229482332026\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.1906667701363055\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.45736298851715\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 105 lasted for 11 time steps with total reward of -16.46305024905714\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6736465397877357\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6740645736214391\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6480087777748299\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5914733041739066\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5029192110332622\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.38412575223794254\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.23458527727300282\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.05363827228379159\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.15951002384254787\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.40576726462155\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 106 lasted for 10 time steps with total reward of -16.802815580278185\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6180223351059643\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6155304814003476\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5830417334161605\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5205395400434196\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.42783191077091864\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.30455644222233713\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.15018898239523343\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.035943511639740355\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.2546451054522904\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.506827003459335\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 107 lasted for 10 time steps with total reward of -17.577704195196986\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.643333318024306\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6389035477799087\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.6066309297444913\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5430680004617258\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.44934938200529373\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.32510437454746277\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.16980084835073095\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.01724178863912812\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2368346040061312\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.4898951214225\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 108 lasted for 10 time steps with total reward of -17.36778111315384\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6623961024649134\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6574064086017268\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6817767599031169\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6645230044739914\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5813462183434452\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.46837322067809084\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3251183615729448\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.15094065349638175\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.054941184707845925\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.29342831809542\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.565514912871926\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 109 lasted for 11 time steps with total reward of -16.72200368614058\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.4691011641163725\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.46484499201658924\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.42968812558802205\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3636066152667268\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.26640298004940255\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.13771196334587305\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.022989527662158782\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21636772891752876\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.443211756139256\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 110 lasted for 9 time steps with total reward of -18.551213172335956\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4986872979737693\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5030316295415292\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.47986782014472995\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4292223126806963\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3485152789666336\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.22924202133337046\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.07827925366502003\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10500201092031014\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.32136133661152366\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.57166449008293\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 111 lasted for 10 time steps with total reward of -18.431182223309015\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6163541184019312\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6183035096398757\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6493693484421171\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6771206191637777\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5975422040274866\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4817358855554903\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.33234507074318886\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.1520746574723984\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.0598902788565055\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.304480721935895\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.582715850673413\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 112 lasted for 11 time steps with total reward of -16.822241438019546\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.45297786896358694\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.45088735946615843\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.41776268537005967\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.35359341149748824\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2581956190723915\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.13121743292790122\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -0.0278512231477282\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21966409498123646\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.444998492960117\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 113 lasted for 9 time steps with total reward of -18.627879433791495\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5320413002844371\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5307746939808888\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4989746673535975\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.436632184555664\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.34356353361864833\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2194155183440385\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.06367457809384813\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.12431961807926462\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.34535529854374797\n",
            "output tensor([[10,  0]]) selected_action: tensor([0]) rew: -20.60032345918345\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 114 lasted for 10 time steps with total reward of -18.44492189957534\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5232226728614517\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5213490260598129\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5465963632509762\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5952382370950363\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6692284253145622\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6009914814966909\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.45903995436210276\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.28699151064756045\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.08404144618083403\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.1507395510987819\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.41838069915282\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 115 lasted for 11 time steps with total reward of -16.282421132982577\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6363080839106742\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6406108147889427\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6741380861404573\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6535200731759447\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5703794360525687\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4487596822962959\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.29630386297252675\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.11277983544407999\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.10263832529939854\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.350889100277854\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 116 lasted for 10 time steps with total reward of -16.420727550795764\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6877741054846328\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6878777513791301\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6616192574269573\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.608999233226935\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5222921737629578\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.40322368050596025\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.2535073428432276\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.07247996134952689\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.14065727431146774\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.386816624208535\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 117 lasted for 10 time steps with total reward of -16.629700392540677\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6775558366746453\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6823444682488815\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6574519110007321\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6029062827759959\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5185591474423061\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.40408952383444696\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2590112488468005\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.08268417372981407\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.12567005206677712\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.366946636833177\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 118 lasted for 10 time steps with total reward of -16.60801409634633\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6788369213682909\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6758545390131796\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.68666670525306\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6367569257206983\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5560868478197076\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.44053930670393504\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2945389086211332\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.11743513198546818\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.09156080416804557\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.3333547511302709\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.55059356786979\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 119 lasted for 11 time steps with total reward of -16.888793836682634\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6651754062251151\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6657084041991191\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6871874316877725\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6275624617303637\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5383458442871273\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4191851504466102\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.26956387799800285\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.08881299856456848\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.12387289096222653\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -20.369413805524783\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 120 lasted for 10 time steps with total reward of -16.53174512134833\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6512577322797013\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6496771717283657\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6742136497538661\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6346320601273253\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5484266055775414\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4323626439355347\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.28593769083901677\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.10849535501903734\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10075906152122721\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.34273747468346183\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.61844132811055\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 121 lasted for 11 time steps with total reward of -17.07693495505485\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6085820802813806\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6097800853588966\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6400363007563344\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6915060572487232\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6120779037418775\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.49391216866290755\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.34562759986548586\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.16655156450895986\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.044126890036634225\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.28733619440603564\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.564094859433425\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 122 lasted for 11 time steps with total reward of -16.72748418345153\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.608849485970151\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6087397951403174\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.581790162736384\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5280023158201114\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.44683684558654824\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.32637047691946863\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.17478380863296988\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.008580509643498713\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.22451232575934266\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.473910518772442\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 123 lasted for 10 time steps with total reward of -17.43163046336933\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.636377278428067\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6376411376223773\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6681628101305537\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6523333885409255\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.572221136755208\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.46374630245442505\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.31435419021050515\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.13395194443671987\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.07827209047414213\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.32324466404510266\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.601979371872748\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 124 lasted for 11 time steps with total reward of -16.92470793781321\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.47927753462952427\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4797750165734822\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.452655922315798\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3979248970159688\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.3154135536931647\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.19389428987191426\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.03871844325356388\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14898234771604923\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -20.369979551669857\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 125 lasted for 9 time steps with total reward of -18.16130224203249\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5469803912217185\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5499158579037656\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5815532894441126\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6419114935495941\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6547518333238886\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5494078610994552\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4018859232647627\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.223014016474274\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.012817119567413493\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2296480656013215\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.50541670086985\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 126 lasted for 11 time steps with total reward of -16.57282698062219\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6004485902995228\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6033895888790892\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6353731522211593\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6815185946054961\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6011940907628762\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.49335942666967747\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.34328428125686494\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.1623731461079405\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.05019436840031832\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.29535446198132126\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.515711732001314\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 127 lasted for 11 time steps with total reward of -16.740319691580325\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5354173751572006\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5360685029171053\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5061938178538223\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.44579654805045676\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3547049952424176\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2325774583614253\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.078911050009153\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1069450678011789\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.32577123984621525\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.520120824331368\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 128 lasted for 10 time steps with total reward of -18.26316738438718\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5436041258110469\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5411427666364189\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5638224837881948\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.611811077179716\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6852687498314961\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5804052741312962\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.43805449143418995\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2654956095770783\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.06192776820783419\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.17357401291564023\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.442033865979504\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 129 lasted for 11 time steps with total reward of -16.324075532297872\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5632729338797547\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5637549904709385\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.589723238282877\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6411823238256351\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6497909305685179\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5332734734455907\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3869660269567674\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.21019473497814112\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.0021455388436446565\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.23811590452958942\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.511617926628382\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 130 lasted for 11 time steps with total reward of -16.609429639906104\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6169101510678238\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6213072032444749\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6515746167575936\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.658700827718115\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.566488484214737\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.44454668180906765\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.29233564842401605\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.10916391483624022\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.10579544917953226\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.3534829908068517\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.63492428663852\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 131 lasted for 11 time steps with total reward of -17.133175198552834\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.4942108519461428\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.49104204267090446\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.46031102043101724\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.39805499821490853\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3014283218619387\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.1733999973089802\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.013449000968677072\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.17908964047988274\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.40500488784107\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 132 lasted for 9 time steps with total reward of -18.252198294918383\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5539097040590168\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5526732599165183\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5243001001535331\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4687851028986427\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.38594935715557555\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2754449937655754\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.1213555882984893\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.0650521052703058\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2843493253867449\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.479067492069557\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 133 lasted for 10 time steps with total reward of -17.946050816479257\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.49784590533405504\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.4966607990412676\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5205115594247673\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.56937469720142\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6434212507885992\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.626471228097815\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.48512705814146306\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.31384456577124076\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.1118157651725053\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.1218928487572295\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.38831604239197\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 134 lasted for 11 time steps with total reward of -16.245136062176066\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5679456464754024\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5705805667577977\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6020134540631491\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6622618571171172\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6458953263497091\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5298474913497935\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.38130908200324876\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.20218103068752402\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.008359344127205681\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.2512527412265155\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.527530837134847\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 135 lasted for 11 time steps with total reward of -16.625108467684825\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6901444719216002\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6874098935674241\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6583330631326476\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6028966343067949\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5147775223604322\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.39280030080664186\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.2401260134883756\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.056076663376685154\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.16015945351018185\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -20.40950497763779\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 136 lasted for 10 time steps with total reward of -16.72709986818737\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5988385840011679\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5964497348979587\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6197957552843454\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6688641371626681\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6163979033468479\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5014101620872659\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.35644904224831697\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.18085648413578248\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.026166161259070175\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.26553812587043174\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.53827282490376\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 137 lasted for 11 time steps with total reward of -16.69091530886891\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.458155993908236\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4612553378295666\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4334102719807885\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.3746387825365748\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.28478458816724916\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.16352195874274472\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.010364638469512577\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.17532062483994837\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.394295180157442\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 138 lasted for 9 time steps with total reward of -18.38348423336272\n",
            "\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6084739719683482\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6121097407358133\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6415481648078871\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6753927113223114\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5846473864575543\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4642593870633043\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.3136943833742818\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.13226559024094597\n",
            "output tensor([[1, 1]]) selected_action: tensor([0]) rew: -0.08085033624010368\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.32659187375955534\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.54757926702298\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 139 lasted for 11 time steps with total reward of -16.922630141052192\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6787088156805713\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6791784599367422\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6912980360675278\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6325606700795282\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5442021784117428\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.42587330160587356\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.2770606119329766\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.097098040343923\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -0.11481698934233348\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.359601815332258\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 140 lasted for 10 time steps with total reward of -16.448438690615706\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5184986586920199\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5220688047787133\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5508962645007651\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6050026120804062\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6686402905004277\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5508838407091449\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4035020917385689\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.22580585005070564\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: 0.016966194412786018\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.2239656603551725\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.498032128243356\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 141 lasted for 11 time steps with total reward of -16.65973318113499\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.661568433992053\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6643147979314052\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6700299201979915\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6140821006878837\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5317282003921338\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4226036748646259\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2773046460508696\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.09417395918663102\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.12101044730912258\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.369176435192536\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 142 lasted for 10 time steps with total reward of -16.554381149198065\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5192331728097437\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5166266407751067\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5392051756496432\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5869527435297034\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.660028550392883\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6015116116558699\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4601899216235612\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.2888040767136776\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.08655277540004058\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14749054630032654\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.41435323034437\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 143 lasted for 11 time steps with total reward of -16.302739108094467\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5924066245438089\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5924678383176538\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6215192930568272\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6795590928030396\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6234645258969083\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.516525135592218\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.36978968669210366\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.19236484343162497\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.016555759756053\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2578975614518049\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.532678207114497\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 144 lasted for 11 time steps with total reward of -16.61903448798817\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5082615017612264\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5039801941830956\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.46899055138952217\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.40326941811293604\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.30661963104673573\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.17867563001486608\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.01891313633871078\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1733365322156093\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.398866690557508\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 145 lasted for 9 time steps with total reward of -18.183493159926023\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.676922163212441\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6811498371234244\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6644786055922819\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.600753171469772\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5073039265077371\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.38375584017893094\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.22957138107949104\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.044062800294344895\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1735908005592689\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.42432058929808\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 146 lasted for 10 time steps with total reward of -16.809913664398927\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6145911011037649\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6122863912935028\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5831693250103596\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5272272698230243\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.44027825959352684\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.31737105248090935\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.16331593879389483\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.02255625375170317\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.24104566546921286\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.49305946396373\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 147 lasted for 10 time steps with total reward of -17.49842204508566\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6720834526202896\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6698209197970707\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.683276366847013\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6328642163434944\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5560857992128397\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.44379414514802196\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.29710506191467867\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.11930793475174367\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.09038904626249944\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -0.3328943194728177\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.550862293055385\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 148 lasted for 11 time steps with total reward of -16.89980776215555\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5269249942689885\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5286334966118242\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5555780814626357\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6077706531324256\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6853979859945306\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5716554069091753\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.42557930716886494\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.24925558108086743\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.04186228369418543\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19754383669711173\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.470001567049422\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 149 lasted for 11 time steps with total reward of -16.474887613423036\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5982566878480066\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5962962936228846\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6200502200804929\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6695057464104968\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.624322782548376\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5099129066159371\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3655511144777802\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.19057940478428292\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.01580094513278396\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.254509435855541\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.52656002168113\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 150 lasted for 11 time steps with total reward of -16.6223952462812\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5132425140672048\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5129374710377888\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4819540629109037\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4202907521410847\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3277714809267629\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.2040507841696999\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.048622896583943365\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1391646117037948\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.360091877913426\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 151 lasted for 9 time steps with total reward of -17.990386527779833\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6934347809693323\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6900005813536899\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.656975967275351\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.5943381899275523\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5018887061881621\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3792580791100222\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.22591430376913135\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.04117511970778032\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.17577486614171062\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.425861225470907\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 152 lasted for 10 time steps with total reward of -16.818650363311594\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5786188462048922\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5777864366435026\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6026233674566895\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.653122079796481\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6170402365246451\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5023047567036931\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3576425257796144\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.1823909438075223\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.02425303416204294\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.26321289885910926\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.535506195375568\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 153 lasted for 11 time steps with total reward of -16.75144293547968\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5433392065922539\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5447044443866779\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5747397364526226\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6334551343565018\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6655656093835944\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5615130110971702\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4155991783373384\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.2386149194281023\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.03038425229556918\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2100314964473483\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.48366475495125\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 154 lasted for 11 time steps with total reward of -16.485780759068767\n",
            "\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6378171151908052\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.6379322953360839\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6673539595359654\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6452864472173161\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5673476242173556\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.46283373559084806\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.31929356045134594\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.13990774629051678\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.07128812163386128\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -0.31521343075087\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.592875864731724\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 155 lasted for 11 time steps with total reward of -16.90160493328622\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5959608737641275\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.60000200731697\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5771966012207365\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5275676191894514\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4509633377932839\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.347061478139157\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.20235911250271277\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.022013467240118545\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19079248169548146\n",
            "output tensor([[1, 4]]) selected_action: tensor([1]) rew: -20.43694604916615\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 156 lasted for 10 time steps with total reward of -17.304614033695074\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.46653908048312465\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4624643463773068\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.48333268426594134\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5291198103944074\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5999766433434253\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6302600557630954\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.49124785537096105\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3224542040988132\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.12308242302343475\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.10779213226568612\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.37119867995363215\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.60983625123777\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 157 lasted for 12 time steps with total reward of -16.98034996033658\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5107080572779675\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5091179176319036\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5360105935705083\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5913743485675521\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6753741499304591\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6065712451719698\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4691999059567419\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.2966082086934011\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.09307280913355354\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.14233477736997846\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.41064251806618\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 158 lasted for 11 time steps with total reward of -16.264940059502102\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.580174215862105\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5763977448140558\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.545588525816579\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.48753212529463086\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.3921929175443636\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.265971517580301\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.1083391339623404\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.08137970718324161\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3039883540385317\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.560393000066444\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 159 lasted for 10 time steps with total reward of -17.98956488041384\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.47920299251978116\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.48381973650786236\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.457567458609624\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.40047235989688534\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3123860512789114\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.19299014871172626\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.04180490880845822\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14179760697080135\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.358575084837888\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 160 lasted for 9 time steps with total reward of -18.13212903547544\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5177382447693263\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5136356572136481\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5379568420358588\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5873433554859491\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6583372847855048\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6179407255189253\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4784602555499219\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.30897795418277096\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.108701881189055\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.12328671081153875\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.388009389265196\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 161 lasted for 11 time steps with total reward of -16.182203899345776\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4474682271596778\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.44997260907746806\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.48053441259502994\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5391694614197928\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6260696877346884\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6551959530929383\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5137122272403314\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3391788419231734\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.1339724269804875\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.10285921461330416\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -20.37236771307888\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 162 lasted for 11 time steps with total reward of -16.289953080468596\n",
            "\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: 0.4900671315547268\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.4902964286282292\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.5155243275599569\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.5657545476304182\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: 0.6411656848714865\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: 0.6151631592027362\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.47141876051044884\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: 0.2977038899249802\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.09320073346476915\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -0.14303267311189882\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: -20.412037776222707\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 163 lasted for 11 time steps with total reward of -16.374775785986856\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6721423116897589\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6725644183168883\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6610605490013575\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.601308094493518\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.5118599662085193\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.39236459248616573\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.24230654249459171\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.06101823088081659\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.15230371640803142\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -20.398576429926063\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 164 lasted for 10 time steps with total reward of -16.736255440762477\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.46384009551507444\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.46550407101279456\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.49525850711844455\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5531154674329939\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6353244027699191\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6459377482971206\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5019610478516154\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.3281241962954673\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.12360205037372723\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.11255474213890548\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: -20.3813952319928\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 165 lasted for 11 time steps with total reward of -16.28128238746455\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6837212001061038\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6805055999171882\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6699819697587122\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6139088338392958\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.5281544459722904\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.41238948979643675\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.26611959554074394\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.08869652642482945\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.12066608119118494\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -0.3628714413825038\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.580581314719208\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 166 lasted for 11 time steps with total reward of -17.120641175937294\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.45624052120773373\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.4602933957223835\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.48917916567615505\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.542924613735962\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6217309486804504\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6175634203587375\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.47131160297062946\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.2951468240107412\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.08823253962100097\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.15039026737859895\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.4217780449396\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 167 lasted for 11 time steps with total reward of -16.529545280334403\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4893730860811437\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.48888297730009866\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.460826989673931\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.40520362597462967\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.32002695449844987\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.19582888391359154\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.03977718960550841\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.14877826860221455\n",
            "output tensor([[1, 8]]) selected_action: tensor([1]) rew: -20.370614142036484\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 168 lasted for 9 time steps with total reward of -18.119472703591345\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6754649529528182\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6729439517261496\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6439681299963568\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5860728554604558\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4942851501431097\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.37222024590680447\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.2193531222080184\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.03500842414418709\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.18162236653553143\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.431457909687353\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 169 lasted for 10 time steps with total reward of -16.913763443684985\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.525340819125163\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5227509886850099\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5453747569492595\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5931939556719218\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6663656961706069\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6019591497089554\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4611852993343786\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.29032877372421295\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.08859013078051275\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.14495539384648964\n",
            "output tensor([[8, 0]]) selected_action: tensor([0]) rew: -20.41133339587079\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 170 lasted for 11 time steps with total reward of -16.26119921956726\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.6010460260647609\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.601737178681889\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5722777219671861\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5126730231159495\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.42275283037167244\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3021759489173288\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.15043858771901908\n",
            "output tensor([[1, 1]]) selected_action: tensor([0]) rew: -0.03311310156335584\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.24926550565478983\n",
            "output tensor([[11,  0]]) selected_action: tensor([0]) rew: -20.440436636990256\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 171 lasted for 10 time steps with total reward of -17.559713927370595\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6188030470530788\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6162935143186673\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5870623808765284\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5310938127123269\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.44819778517654363\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3380148894687033\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.18493432288971812\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.0012428925442724004\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.21996669013220982\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: -20.472148534100224\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 172 lasted for 10 time steps with total reward of -17.36895836428114\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4916922015960189\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.49189349178255215\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.46135471757222324\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4000787217928803\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.30789387885175\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.18445914564596333\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.029273132175642702\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.15831228100340694\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.379072991747705\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 173 lasted for 9 time steps with total reward of -18.17073998333408\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5429257999133624\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5462467621861492\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5190935144583783\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.46148596516624385\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3732687048883999\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.25411554775499035\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.1035378958789504\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.07910257463820614\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -0.29457613896425855\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.543761199590776\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 174 lasted for 10 time steps with total reward of -18.116765722946766\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.4533080420943659\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.45477608577182893\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.48108268847616875\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5322367107508662\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6084217103378218\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6271456087252886\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.48386450300317896\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.31074958995775825\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.10697796071643129\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.12839796441973406\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.396426083837067\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 175 lasted for 11 time steps with total reward of -16.466261148423094\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5302243421465141\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5342049139276945\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5634311269487533\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6179281482905308\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6812644220509686\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5628479502915\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.41480402331512756\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.23644038242528886\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.026925336329730598\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2146922856620867\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.489456790371918\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 176 lasted for 11 time steps with total reward of -16.536078430307896\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5198852033538794\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5213955328797688\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4954913731784175\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4360677743069541\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3446655341569018\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.22209129968964725\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.06784710660761512\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.11871253279256966\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.3383619837663809\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.533696894622565\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 177 lasted for 10 time steps with total reward of -18.38332758700833\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5880991560713608\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5926447678351962\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6261383737179944\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6886099924024486\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.6109244471828305\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4985170660266448\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.34719713503161953\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.16508110479298393\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.04866083745271721\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.294973512024616\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.57488869193065\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 178 lasted for 11 time steps with total reward of -16.801310998346903\n",
            "\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6006198960117086\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6055976896192279\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5823826045312325\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5263872092395979\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4400863826496876\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.3231615306624167\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.17513018280624382\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.004642034834766218\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.2169245524594841\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.40408175721251\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 179 lasted for 10 time steps with total reward of -17.372282848986643\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4920797768136327\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.48803801272905445\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.45646977372693986\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.39735097372336303\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3104848697818373\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.18634273381574773\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.026251823170180932\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1664079354660557\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -20.392428599434318\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 180 lasted for 9 time steps with total reward of -18.20181857113962\n",
            "\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.48742082977527323\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4901035088832598\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.46194593129739836\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.40296670146908276\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.3130098962129505\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.1917498166357292\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.038699781103499364\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1467745398216907\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -20.365436481419838\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 181 lasted for 9 time steps with total reward of -18.126314555864337\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5061881320693579\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5072093797489075\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5333602755924216\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5846502802242975\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6612637175677342\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5850299549250509\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.43959461209669837\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.2640299604792121\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.05751492223015603\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18089402440383517\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.452237521013316\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 182 lasted for 11 time steps with total reward of -16.494290310483315\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5086110551067495\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5046478930146078\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.47329214472841274\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4145198826424298\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3281342359620043\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.213770764626517\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.056486219091360035\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.13532487221350475\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -20.360368766805127\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 183 lasted for 9 time steps with total reward of -17.99623144384655\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.49096683282238873\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.49210025508493926\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5215799507393252\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5794123602425543\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6657803226178806\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6156712260479265\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4742901218978294\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.29953047015035017\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.09386296605640904\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.14365437233613754\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.414061448186896\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 184 lasted for 11 time steps with total reward of -16.324521314863432\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5505454436935925\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5522052958438277\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5826095787003881\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6417703759126757\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6481416102346196\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5434384332751881\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.40546990757184376\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.22771625805244833\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.018632213660999764\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -0.22272072821736638\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.497373627773275\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 185 lasted for 11 time steps with total reward of -16.54956523904506\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.604730565769884\n",
            "output tensor([[ 0, 17]]) selected_action: tensor([1]) rew: 0.6049447362359777\n",
            "output tensor([[ 0, 18]]) selected_action: tensor([1]) rew: 0.5782794010616293\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5242991768251546\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.43313512771633167\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.3113107142071241\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.158316524856297\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: -0.026506436529549204\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -0.24394906504707758\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.494910475057722\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 186 lasted for 10 time steps with total reward of -17.55034972996195\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6407373009803573\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6422100660473178\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6138317282774621\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5556110436262899\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.46738068981715986\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.3488017195400759\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.19937150689385907\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: 0.018435700052637616\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19479503708688867\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: -20.382636752932232\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 187 lasted for 10 time steps with total reward of -17.09105203478396\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6365222660074231\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6362931917853705\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6093999617625504\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5558436138694618\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4754506293938011\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3568549387474338\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.206060614451269\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.023658692352493516\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1911443296209605\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: -20.439252605197893\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 188 lasted for 10 time steps with total reward of -17.130313026449052\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5140127459168068\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5136186379634196\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5384614355263588\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5885382979530563\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6640214696303525\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5706595724594518\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4271307414172014\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.25342315654141934\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.04872593929549579\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.18789499835490892\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.457471775208642\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 189 lasted for 11 time steps with total reward of -16.526774776859988\n",
            "\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.45778874886843113\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.45807634227149785\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4273511270074897\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3656140587417659\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.2726923190174968\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.14824456372103811\n",
            "output tensor([[1, 1]]) selected_action: tensor([0]) rew: -0.00822964185622943\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19737567182720034\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.3614586087982\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 190 lasted for 9 time steps with total reward of -18.437296762853908\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5806175517768775\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5820977676302413\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6092300128343009\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6620232584542616\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6131972866655176\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.49561643889309914\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3480622232858418\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.16985810868470574\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.03981129631810021\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.281879719169167\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.557372202812065\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 191 lasted for 11 time steps with total reward of -16.818360570074486\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.508030904535876\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5032710788469188\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.46779012383765683\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.40156099980790283\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.30438277404123093\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.1758863273853123\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.015544145879683668\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.1773152269791186\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.403487253571207\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 192 lasted for 9 time steps with total reward of -18.204336126215743\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.684559225912094\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6799184196802865\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6456375254549055\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5816885988428036\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.4878680499514051\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.3638016711099896\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.2089531309262731\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: 0.022636504927643297\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1959663261045934\n",
            "output tensor([[6, 0]]) selected_action: tensor([0]) rew: -20.447782386381068\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 193 lasted for 10 time steps with total reward of -16.968685585680262\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5697308889769689\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5662308916878177\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.591493017824403\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6446063989432721\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6716034009688712\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5594770119976542\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.41762895479824547\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.24540704453117834\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.042016554733385614\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.19346064583872663\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.462040739150964\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 194 lasted for 11 time steps with total reward of -16.347307220527895\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6092371188882527\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.607007788607305\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.577983602664555\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5221512213371249\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.43932316008213834\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.32706282408962195\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.17278899807063325\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.013287958805142108\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.2319689995481682\n",
            "output tensor([[5, 0]]) selected_action: tensor([0]) rew: -20.42569177527216\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 195 lasted for 10 time steps with total reward of -17.415394019885838\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.52533560504751\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.524250292244509\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.5483738370841141\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5976972403605422\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6723866609703388\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5968875295255285\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4547519353014864\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.2824949135268324\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.07931000297489577\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.155733461196206\n",
            "output tensor([[7, 0]]) selected_action: tensor([0]) rew: -20.423665506249154\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 196 lasted for 11 time steps with total reward of -16.2979109504096\n",
            "\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6121920076963705\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6124073666130061\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.638405358868421\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6901897034511869\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6046790757677462\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4869495204666857\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3391358017936956\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.1605665897882514\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.04956811045205051\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.29219625657510995\n",
            "output tensor([[4, 0]]) selected_action: tensor([0]) rew: -20.50987471171016\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 197 lasted for 11 time steps with total reward of -16.70711365429196\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.543367444050943\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.5428639504192984\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5151163274889748\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.46012026094398417\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.3711650639280486\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.24738309370593603\n",
            "output tensor([[0, 4]]) selected_action: tensor([1]) rew: 0.0920557501460148\n",
            "output tensor([[1, 0]]) selected_action: tensor([0]) rew: -0.09547401469156946\n",
            "output tensor([[1, 1]]) selected_action: tensor([0]) rew: -0.3159919820743238\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: -20.512066165201727\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 198 lasted for 10 time steps with total reward of -18.15146027128442\n",
            "\n",
            "output tensor([[ 0, 16]]) selected_action: tensor([1]) rew: 0.5854766779777423\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5855292198985085\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5546780468880207\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.49353634349187614\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.4019277330353004\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.2795054423217007\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.12576095147695648\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.05996305723446038\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.278454655505047\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.530608546747818\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 199 lasted for 10 time steps with total reward of -17.842611844397222\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4686056944794734\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.4709719234787586\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.4982041716216252\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.5503195625843285\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6275098167689641\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6257432919530517\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.480639841248899\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.30563578873795494\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.09990214102438377\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.13751353902668406\n",
            "output tensor([[3, 0]]) selected_action: tensor([0]) rew: -20.40766236445373\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 200 lasted for 11 time steps with total reward of -16.417643671582976\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6411712648687468\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6424356087027548\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6697304850615539\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6432155788449089\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5542622596228421\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4354829310630337\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.28635810828456476\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.10621550298938326\n",
            "output tensor([[0, 2]]) selected_action: tensor([1]) rew: -0.10575402419959296\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.3504749536127116\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: -20.628958916638876\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 201 lasted for 11 time steps with total reward of -17.106316155013392\n",
            "\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6625858015794249\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.6597411462280971\n",
            "output tensor([[ 0, 15]]) selected_action: tensor([1]) rew: 0.6725783472175766\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6168071342105714\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.5314274365004558\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.4161091591838906\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.2703570224185148\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.0935216469046587\n",
            "output tensor([[0, 1]]) selected_action: tensor([1]) rew: -0.11518484585539512\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.35666773466831686\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: -20.63192199618635\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 202 lasted for 11 time steps with total reward of -17.18064688246687\n",
            "\n",
            "output tensor([[ 0, 12]]) selected_action: tensor([1]) rew: 0.6427918013342179\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.6448321813869812\n",
            "output tensor([[ 0, 11]]) selected_action: tensor([1]) rew: 0.617000745285027\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5593083997064541\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.47158996225907057\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.3535085840476202\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.20456367203841463\n",
            "output tensor([[0, 3]]) selected_action: tensor([1]) rew: 0.02410282086008353\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.1886614660685748\n",
            "output tensor([[2, 0]]) selected_action: tensor([0]) rew: -20.434629280399555\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 203 lasted for 10 time steps with total reward of -17.10559257955026\n",
            "\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5628872938690561\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: 0.5627136392387732\n",
            "output tensor([[0, 9]]) selected_action: tensor([1]) rew: 0.5321138733478576\n",
            "output tensor([[0, 8]]) selected_action: tensor([1]) rew: 0.4710866292800798\n",
            "output tensor([[0, 7]]) selected_action: tensor([1]) rew: 0.3794555082934431\n",
            "output tensor([[0, 5]]) selected_action: tensor([1]) rew: 0.2568740456486942\n",
            "output tensor([[0, 6]]) selected_action: tensor([1]) rew: 0.10283452259994946\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.08331884184122823\n",
            "output tensor([[0, 0]]) selected_action: tensor([0]) rew: -0.3023715237590131\n",
            "output tensor([[ 0, 10]]) selected_action: tensor([1]) rew: -20.496854804880652\n",
            "\n",
            "Episode not successful!\n",
            "\n",
            "Episode 204 lasted for 10 time steps with total reward of -18.01457965820304\n",
            "\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6813926854869002\n",
            "output tensor([[ 0, 13]]) selected_action: tensor([1]) rew: 0.6804298146280516\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.653142218984742\n",
            "output tensor([[ 0, 14]]) selected_action: tensor([1]) rew: 0.5995255016089228\n"
          ]
        }
      ],
      "source": [
        "manual_seed(SEED)\n",
        "\n",
        "if WANDB:\n",
        "    wandb.init(project='BioLCNet', entity='singularbrain', config=network_hparams)\n",
        "net = LCNet(time,**network_hparams, reward_fn=RLTasks('CartPole-v0'), wandb_active = WANDB)\n",
        "net.learn(**network_hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gym Playground"
      ],
      "metadata": {
        "id": "9M7NvCJlr0E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "intensity = 400\n",
        "crop_size = 90\n",
        "time = 100+300+50\n",
        "def frame_process(x):\n",
        "        x[x<0.99] = 2.0\n",
        "        x[x<=1.0] = 0.0\n",
        "        x[x==2.0] = 1.0\n",
        "        return x\n",
        "screen = env.render(mode='rgb_array')\n",
        "screen = screen.transpose((2, 0, 1))\n",
        "_, screen_height, screen_width = screen.shape\n",
        "screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "view_width = int(screen_width * 0.6)\n",
        "world_width = env.x_threshold * 2\n",
        "scale = screen_width / world_width\n",
        "cart_location = int(env.state[0] * scale + screen_width / 2.0)\n",
        "if cart_location < view_width // 2:\n",
        "    slice_range = slice(view_width)\n",
        "elif cart_location > (screen_width - view_width // 2):\n",
        "    slice_range = slice(-view_width, None)\n",
        "else:\n",
        "    slice_range = slice(cart_location - view_width // 2,\n",
        "                        cart_location + view_width // 2)\n",
        "# Strip off the edges, so that we have a square image centered on a cart\n",
        "screen = screen[:, :, slice_range]\n",
        "# Convert to float, rescale, convert to torch tensor\n",
        "screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "screen = torch.from_numpy(screen)\n",
        "print(screen.shape)\n",
        "plt.imshow(screen.cpu().permute(\n",
        "        1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "h = screen.shape[1]\n",
        "w = screen.shape[2]\n",
        "print(h*0.75)\n",
        "resize = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize([80, 180]),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 60, 60, 60)),\n",
        "            transforms.Lambda(lambda x: crop(x, 0, 10, 40, 40)),\n",
        "            #transforms.Resize([80, 80]),\n",
        "            #transforms.Lambda(lambda x: crop(x, 0, 0, 80, 80)),\n",
        "            # transforms.Resize([self.crop_size, self.crop_size], interpolation=Image.CUBIC),\n",
        "            #transforms.CenterCrop((crop_size, crop_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Lambda(lambda x: -1.0*x +1.0),\n",
        "            transforms.Lambda(frame_process),\n",
        "            #transforms.Lambda(lambda x: 0*x[x<1.0]),\n",
        "            transforms.Lambda(lambda x: x * intensity),\n",
        "            transforms.Lambda(lambda x: PoissonEncoder(time=time, dt=1)(x))])\n",
        "device = 'cuda'\n",
        "\n",
        "# Resize, and add a batch dimension (BCHW)\n",
        "screen = resize(screen)\n",
        "# print(screen.shape)\n",
        "print(int(h*0.8), int(w*0.8))\n",
        "print(screen.shape)\n",
        "# screen = screen.to(device)\n",
        "# plt.imshow(screen.cpu().permute(\n",
        "#         1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "\n",
        "# print(a.shape)\n",
        "a = screen.sum(axis=0)\n",
        "\n",
        "a = a.to(device)\n",
        "plt.imshow(a.cpu().numpy().squeeze(), cmap='gray')\n",
        "obs, reward, done, info = env.step(0)\n",
        "obs, reward, done, info "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "JDFJB9GeldHl",
        "outputId": "51c07d3d-b5d0-4adb-debc-f2096596bfa9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 160, 360])\n",
            "120.0\n",
            "128 288\n",
            "torch.Size([450, 1, 40, 40])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.00351858, -0.18672182, -0.04909032,  0.24545288]), 1.0, False, {})"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BindsNet Breakout"
      ],
      "metadata": {
        "id": "9jFzOzYIFdGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/BindsNET/bindsnet.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Qi4v29mDF3i",
        "outputId": "e2115a1b-3d7a-403a-e763-57ccd12ddab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/BindsNET/bindsnet.git\n",
            "  Cloning https://github.com/BindsNET/bindsnet.git to /tmp/pip-req-build-mtcdoh5t\n",
            "  Running command git clone -q https://github.com/BindsNET/bindsnet.git /tmp/pip-req-build-mtcdoh5t\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision==0.11.1 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (0.11.1+cu111)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     || 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image<0.19.0,>=0.18.3 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (0.18.3)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (1.10.0+cu111)\n",
            "Collecting gym<0.11.0,>=0.10.4\n",
            "  Downloading gym-0.10.11.tar.gz (1.5 MB)\n",
            "\u001b[K     || 1.5 MB 55.4 MB/s \n",
            "\u001b[?25hCollecting pandas<2.0.0,>=1.3.2\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     || 11.3 MB 64.0 MB/s \n",
            "\u001b[?25hCollecting scipy<2.0.0,>=1.7.1\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     || 38.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (1.19.5)\n",
            "Collecting tensorboardX==2.2\n",
            "  Downloading tensorboardX-2.2-py2.py3-none-any.whl (120 kB)\n",
            "\u001b[K     || 120 kB 79.5 MB/s \n",
            "\u001b[?25hCollecting scikit-build<0.13.0,>=0.12.0\n",
            "  Downloading scikit_build-0.12.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     || 73 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.62.2 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (4.62.3)\n",
            "Collecting opencv-python<5.0.0,>=4.5.3\n",
            "  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n",
            "\u001b[K     || 60.4 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting foolbox<4.0.0,>=3.3.1\n",
            "  Downloading foolbox-3.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     || 1.7 MB 66.4 MB/s \n",
            "\u001b[?25hCollecting matplotlib<4.0.0,>=3.4.3\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     || 11.2 MB 96.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.7/dist-packages (from bindsnet==0.3.0) (0.29.26)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.2->bindsnet==0.3.0) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->bindsnet==0.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1->bindsnet==0.3.0) (7.1.2)\n",
            "Collecting GitPython>=3.0.7\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     || 180 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting eagerpy==0.29.0\n",
            "  Downloading eagerpy-0.29.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (57.4.0)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     || 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     || 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym<0.11.0,>=0.10.4->bindsnet==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.11.0,>=0.10.4->bindsnet==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (3.0.6)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
            "\u001b[K     || 890 kB 91.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.4.3->bindsnet==0.3.0) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.3.2->bindsnet==0.3.0) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym<0.11.0,>=0.10.4->bindsnet==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (2.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox<4.0.0,>=3.3.1->bindsnet==0.3.0) (2.10)\n",
            "Requirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.7/dist-packages (from scikit-build<0.13.0,>=0.12.0->bindsnet==0.3.0) (0.37.1)\n",
            "Collecting distro\n",
            "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19.0,>=0.18.3->bindsnet==0.3.0) (2.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.2->bindsnet==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.2->bindsnet==0.3.0) (3.0.0)\n",
            "Building wheels for collected packages: bindsnet, gym\n",
            "  Building wheel for bindsnet (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bindsnet: filename=bindsnet-0.3.0-py3-none-any.whl size=114807 sha256=028eda52acfd9f56b6fda6aeddbd155258705a105369030f2fff2a96ea21e814\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ankwpzvk/wheels/85/c3/10/2e6bb871a97d7e6fdc9a0c047652b4895c0825d19646be7adf\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-py3-none-any.whl size=1588313 sha256=dbcb356b91d48b050a080f54dc6bd9d5ecf6b87ed287da5013fd2d3b4863ea6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/c9/3a/068c5b80305e89c8de8b0a412e67ef2986cbad74895cfb9551\n",
            "Successfully built bindsnet gym\n",
            "Installing collected packages: smmap, gitdb, fonttools, scipy, requests, matplotlib, GitPython, eagerpy, distro, tensorboardX, scikit-learn, scikit-build, pandas, opencv-python, gym, foolbox, bindsnet\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.26 bindsnet-0.3.0 distro-1.6.0 eagerpy-0.29.0 fonttools-4.28.5 foolbox-3.3.1 gitdb-4.0.9 gym-0.10.11 matplotlib-3.5.1 opencv-python-4.5.5.62 pandas-1.3.5 requests-2.27.1 scikit-build-0.12.0 scikit-learn-0.24.2 scipy-1.7.3 smmap-5.0.0 tensorboardX-2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ROgQ2ZWA-VT",
        "outputId": "98adc726-879e-48d6-8ae4-2cfd47b0b044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-16 14:47:08--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11128004 (11M) [application/x-rar-compressed]\n",
            "Saving to: Roms.rar\n",
            "\n",
            "Roms.rar            100%[===================>]  10.61M   486KB/s    in 23s     \n",
            "\n",
            "2022-01-16 14:47:32 (469 KB/s) - Roms.rar saved [11128004/11128004]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/Roms.rar\n",
            "\n",
            "Extracting  /content/ROM/HC ROMS.zip                                     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  /content/ROM/ROMS.zip                                        \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install matplotlib==2.1.1\n",
        "\n",
        "### Restart your runtime after this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "_K7CuZmqFTny",
        "outputId": "736e4d9f-ec5b-4ea6-cb87-2c444d87957e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==2.1.1\n",
            "  Downloading matplotlib-2.1.1.tar.gz (36.1 MB)\n",
            "\u001b[K     || 36.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (3.0.6)\n",
            "Building wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-2.1.1-cp37-cp37m-linux_x86_64.whl size=10243757 sha256=18b68a6b31696664021b1bb9df2061292918bfcc3217241b860d9366de83d8c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/7a/d6/021782cff10d8257e030d4a766ca5ed9667fd8758606fbbeff\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.1\n",
            "    Uninstalling matplotlib-3.5.1:\n",
            "      Successfully uninstalled matplotlib-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "seaborn 0.11.2 requires matplotlib>=2.2, but you have matplotlib 2.1.1 which is incompatible.\n",
            "plotnine 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.1.1 which is incompatible.\n",
            "mizani 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.1.1 which is incompatible.\n",
            "bindsnet 0.3.0 requires matplotlib<4.0.0,>=3.4.3, but you have matplotlib 2.1.1 which is incompatible.\n",
            "arviz 0.11.4 requires matplotlib>=3.0, but you have matplotlib 2.1.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-2.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### gym and colab compatibility\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "wONJ1L1uEGMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bindsnet.encoding import bernoulli\n",
        "from bindsnet.environment import GymEnvironment\n",
        "from bindsnet.learning import MSTDP\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes\n",
        "from bindsnet.network.topology import Connection\n",
        "from bindsnet.pipeline import EnvironmentPipeline\n",
        "from bindsnet.pipeline.action import select_softmax\n",
        "import time\n",
        "\n",
        "# Build network.\n",
        "network = Network(dt=1.0)\n",
        "\n",
        "# Layers of neurons.\n",
        "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
        "middle = LIFNodes(n=100, traces=True)\n",
        "out = LIFNodes(n=4, refrac=0, traces=True)\n",
        "\n",
        "# Connections between layers.\n",
        "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
        "middle_out = Connection(\n",
        "    source=middle,\n",
        "    target=out,\n",
        "    wmin=0,\n",
        "    wmax=1,\n",
        "    update_rule=MSTDP,\n",
        "    nu=1e-1,\n",
        "    norm=0.5 * middle.n,\n",
        ")\n",
        "\n",
        "# Add all layers and connections to the network.\n",
        "network.add_layer(inpt, name=\"Input Layer\")\n",
        "network.add_layer(middle, name=\"Hidden Layer\")\n",
        "network.add_layer(out, name=\"Output Layer\")\n",
        "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
        "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
        "\n",
        "# Load the Breakout environment.\n",
        "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
        "environment.reset()\n",
        "\n",
        "# Build pipeline from specified components.\n",
        "environment_pipeline = EnvironmentPipeline(\n",
        "    network,\n",
        "    environment,\n",
        "    encoding=bernoulli,\n",
        "    action_function=select_softmax,\n",
        "    output=\"Output Layer\",\n",
        "    time=100,\n",
        "    history_length=1,\n",
        "    delta=1,\n",
        "    plot_interval=None,\n",
        "    render_interval=None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(pipeline, episode_count):\n",
        "    for i in range(episode_count):\n",
        "        total_reward = 0\n",
        "        pipeline.reset_state_variables()\n",
        "        is_done = False\n",
        "        render_counter = 0\n",
        "        while not is_done:\n",
        "            # if render_counter % 10 == 0:\n",
        "            #     # plt.title(\"Game image\")\n",
        "            #     # plt.imshow(pipeline.env.render())\n",
        "            #     # plt.show()\n",
        "            #     a = pipeline.env.render()\n",
        "            #     print(a)\n",
        "            #     time.sleep(0.1)\n",
        "                \n",
        "            render_counter += 1\n",
        "            result = pipeline.env_step()\n",
        "            pipeline.step(result)\n",
        "\n",
        "            reward = result[1]\n",
        "            total_reward += reward\n",
        "            \n",
        "            is_done = result[2]\n",
        "        print(f\"Episode {i} total reward:{total_reward}\")\n",
        "    pipeline.env.close()\n",
        "\n",
        "print(\"Training: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)\n",
        "\n",
        "# stop MSTDP\n",
        "environment_pipeline.network.learning = False\n",
        "\n",
        "print(\"Testing: \")\n",
        "run_pipeline(environment_pipeline, episode_count=100)"
      ],
      "metadata": {
        "id": "y_jx13I5-VTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f32146c8-3f59-47fc-d959-3a66975bf6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: \n",
            "Episode 0 total reward:2.0\n",
            "Episode 1 total reward:0.0\n",
            "Episode 2 total reward:0.0\n",
            "Episode 3 total reward:1.0\n",
            "Episode 4 total reward:1.0\n",
            "Episode 5 total reward:1.0\n",
            "Episode 6 total reward:1.0\n",
            "Episode 7 total reward:3.0\n",
            "Episode 8 total reward:1.0\n",
            "Episode 9 total reward:2.0\n",
            "Episode 10 total reward:2.0\n",
            "Episode 11 total reward:2.0\n",
            "Episode 12 total reward:0.0\n",
            "Episode 13 total reward:1.0\n",
            "Episode 14 total reward:0.0\n",
            "Episode 15 total reward:2.0\n",
            "Episode 16 total reward:0.0\n",
            "Episode 17 total reward:0.0\n",
            "Episode 18 total reward:2.0\n",
            "Episode 19 total reward:0.0\n",
            "Episode 20 total reward:2.0\n",
            "Episode 21 total reward:0.0\n",
            "Episode 22 total reward:0.0\n",
            "Episode 23 total reward:0.0\n",
            "Episode 24 total reward:0.0\n",
            "Episode 25 total reward:1.0\n",
            "Episode 26 total reward:1.0\n",
            "Episode 27 total reward:1.0\n",
            "Episode 28 total reward:1.0\n",
            "Episode 29 total reward:0.0\n",
            "Episode 30 total reward:2.0\n",
            "Episode 31 total reward:3.0\n",
            "Episode 32 total reward:2.0\n",
            "Episode 33 total reward:0.0\n",
            "Episode 34 total reward:2.0\n",
            "Episode 35 total reward:4.0\n",
            "Episode 36 total reward:1.0\n",
            "Episode 37 total reward:2.0\n",
            "Episode 38 total reward:1.0\n",
            "Episode 39 total reward:2.0\n",
            "Episode 40 total reward:2.0\n",
            "Episode 41 total reward:2.0\n",
            "Episode 42 total reward:0.0\n",
            "Episode 43 total reward:2.0\n",
            "Episode 44 total reward:1.0\n",
            "Episode 45 total reward:0.0\n",
            "Episode 46 total reward:1.0\n",
            "Episode 47 total reward:2.0\n",
            "Episode 48 total reward:1.0\n",
            "Episode 49 total reward:0.0\n",
            "Episode 50 total reward:3.0\n",
            "Episode 51 total reward:1.0\n",
            "Episode 52 total reward:0.0\n",
            "Episode 53 total reward:2.0\n",
            "Episode 54 total reward:1.0\n",
            "Episode 55 total reward:3.0\n",
            "Episode 56 total reward:0.0\n",
            "Episode 57 total reward:2.0\n",
            "Episode 58 total reward:0.0\n",
            "Episode 59 total reward:3.0\n",
            "Episode 60 total reward:1.0\n",
            "Episode 61 total reward:3.0\n",
            "Episode 62 total reward:0.0\n",
            "Episode 63 total reward:0.0\n",
            "Episode 64 total reward:0.0\n",
            "Episode 65 total reward:1.0\n",
            "Episode 66 total reward:2.0\n",
            "Episode 67 total reward:2.0\n",
            "Episode 68 total reward:0.0\n",
            "Episode 69 total reward:0.0\n",
            "Episode 70 total reward:1.0\n",
            "Episode 71 total reward:2.0\n",
            "Episode 72 total reward:2.0\n",
            "Episode 73 total reward:0.0\n",
            "Episode 74 total reward:0.0\n",
            "Episode 75 total reward:0.0\n",
            "Episode 76 total reward:0.0\n",
            "Episode 77 total reward:1.0\n",
            "Episode 78 total reward:2.0\n",
            "Episode 79 total reward:3.0\n",
            "Episode 80 total reward:2.0\n",
            "Episode 81 total reward:0.0\n",
            "Episode 82 total reward:2.0\n",
            "Episode 83 total reward:1.0\n",
            "Episode 84 total reward:1.0\n",
            "Episode 85 total reward:1.0\n",
            "Episode 86 total reward:3.0\n",
            "Episode 87 total reward:2.0\n",
            "Episode 88 total reward:2.0\n",
            "Episode 89 total reward:0.0\n",
            "Episode 90 total reward:0.0\n",
            "Episode 91 total reward:5.0\n",
            "Episode 92 total reward:2.0\n",
            "Episode 93 total reward:2.0\n",
            "Episode 94 total reward:2.0\n",
            "Episode 95 total reward:0.0\n",
            "Episode 96 total reward:1.0\n",
            "Episode 97 total reward:0.0\n",
            "Episode 98 total reward:0.0\n",
            "Episode 99 total reward:1.0\n",
            "Testing: \n",
            "Episode 0 total reward:2.0\n",
            "Episode 1 total reward:2.0\n",
            "Episode 2 total reward:0.0\n",
            "Episode 3 total reward:3.0\n",
            "Episode 4 total reward:3.0\n",
            "Episode 5 total reward:2.0\n",
            "Episode 6 total reward:1.0\n",
            "Episode 7 total reward:1.0\n",
            "Episode 8 total reward:0.0\n",
            "Episode 9 total reward:0.0\n",
            "Episode 10 total reward:0.0\n",
            "Episode 11 total reward:2.0\n",
            "Episode 12 total reward:0.0\n",
            "Episode 13 total reward:2.0\n",
            "Episode 14 total reward:1.0\n",
            "Episode 15 total reward:1.0\n",
            "Episode 16 total reward:0.0\n",
            "Episode 17 total reward:2.0\n",
            "Episode 18 total reward:1.0\n",
            "Episode 19 total reward:0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2feff05f12ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-2feff05f12ee>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(pipeline, episode_count)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mrender_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/pipeline/base_pipeline.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecursive_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mstep_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/pipeline/environment_pipeline.py\u001b[0m in \u001b[0;36mstep_\u001b[0;34m(self, gym_batch, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Run the network on the spike train-encoded inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/network.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, time, one_step, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mcurrent_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mone_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mcurrent_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/network.py\u001b[0m in \u001b[0;36m_get_inputs\u001b[0;34m(self, layers)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PLVXCK0Djg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7fTSvrK3T_GA",
        "ULGGHW43UksI",
        "MBKedMpIleMr",
        "8clxN_npa1WY",
        "sCXSLZZoGS4z"
      ],
      "machine_shape": "hm",
      "name": "BioLCNet_CartPole.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "87ae7d1e75b14a98f2d7b99b6b39b40721989d38e6517f9dbec64ca4d8e3011b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f03e9dec1a8a4f4b8e78b35eefadd80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6549daa3fd7b4faebac94fe86774cd10",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dd1292e0c6f142f78421e85a772086f4",
              "IPY_MODEL_6c5f35e306bc419a9a3eb618afdbee31",
              "IPY_MODEL_8d7be4c0e9634af99e65b9eb046688fa"
            ]
          }
        },
        "6549daa3fd7b4faebac94fe86774cd10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd1292e0c6f142f78421e85a772086f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3abda04055e8418c9d28a3f3ffd75faa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Episode: 204Number of steps: 10, Episode Total Reward: -18.01:  20%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b38071ca750840b2bdc034d9e7ad32d9"
          }
        },
        "6c5f35e306bc419a9a3eb618afdbee31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_67e5c7681d7946bdad74b62eb1f03977",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 204,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf857a0b26f64096884d245b33114f44"
          }
        },
        "8d7be4c0e9634af99e65b9eb046688fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57b505ce9a4749b5928802d63663edaa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 204/1000 [23:48&lt;1:33:33,  7.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b63bb2c290047b38b401ee45062853f"
          }
        },
        "3abda04055e8418c9d28a3f3ffd75faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b38071ca750840b2bdc034d9e7ad32d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67e5c7681d7946bdad74b62eb1f03977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf857a0b26f64096884d245b33114f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57b505ce9a4749b5928802d63663edaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b63bb2c290047b38b401ee45062853f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}